{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from network import *\n",
    "from train import *\n",
    "from dataset import *\n",
    "from loss import *\n",
    "from flax.training.train_state import TrainState\n",
    "import torch.utils.data as data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = Hyperparam()\n",
    "hp.dims = [2, 10, 10, 1]\n",
    "hp.lr = 0.001\n",
    "hp.batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "df = pd.read_csv(\"training_data/circle.csv\")\n",
    "dataset = NumpyDataset(df[[\"x\", \"y\"]].to_numpy(), df[\"d\"].to_numpy())\n",
    "train_dataset, val_dataset = train_test_split(dataset, train_size=0.9, shuffle=True)\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=numpy_collate)\n",
    "val_loader = data.DataLoader(\n",
    "    val_dataset, batch_size=hp.batch_size, collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mlp(hp)\n",
    "\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x = random.normal(key1, (2,)) # Dummy input data\n",
    "params = model.init(key2, x) # Initialization call\n",
    "tx = optax.adam(learning_rate=hp.lr)\n",
    "state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: EPOCH 1/100 | BATCH 0/71 | LOSS: 0.38530755043029785\n",
      "TRAIN: EPOCH 1/100 | BATCH 1/71 | LOSS: 0.4003516882658005\n",
      "TRAIN: EPOCH 1/100 | BATCH 2/71 | LOSS: 0.39103670914967853\n",
      "TRAIN: EPOCH 1/100 | BATCH 3/71 | LOSS: 0.3876916915178299\n",
      "TRAIN: EPOCH 1/100 | BATCH 4/71 | LOSS: 0.3773049354553223\n",
      "TRAIN: EPOCH 1/100 | BATCH 5/71 | LOSS: 0.3731925090154012\n",
      "TRAIN: EPOCH 1/100 | BATCH 6/71 | LOSS: 0.37016725540161133\n",
      "TRAIN: EPOCH 1/100 | BATCH 7/71 | LOSS: 0.3682968057692051\n",
      "TRAIN: EPOCH 1/100 | BATCH 8/71 | LOSS: 0.36428865128093296\n",
      "TRAIN: EPOCH 1/100 | BATCH 9/71 | LOSS: 0.356124347448349\n",
      "TRAIN: EPOCH 1/100 | BATCH 10/71 | LOSS: 0.3581731075590307\n",
      "TRAIN: EPOCH 1/100 | BATCH 11/71 | LOSS: 0.3545665070414543\n",
      "TRAIN: EPOCH 1/100 | BATCH 12/71 | LOSS: 0.3523436693044809\n",
      "TRAIN: EPOCH 1/100 | BATCH 13/71 | LOSS: 0.3465723821095058\n",
      "TRAIN: EPOCH 1/100 | BATCH 14/71 | LOSS: 0.34332483212153114\n",
      "TRAIN: EPOCH 1/100 | BATCH 15/71 | LOSS: 0.33864808082580566\n",
      "TRAIN: EPOCH 1/100 | BATCH 16/71 | LOSS: 0.3355058571871589\n",
      "TRAIN: EPOCH 1/100 | BATCH 17/71 | LOSS: 0.33128275142775643\n",
      "TRAIN: EPOCH 1/100 | BATCH 18/71 | LOSS: 0.32809636467381526\n",
      "TRAIN: EPOCH 1/100 | BATCH 19/71 | LOSS: 0.3265534847974777\n",
      "TRAIN: EPOCH 1/100 | BATCH 20/71 | LOSS: 0.32220728340603055\n",
      "TRAIN: EPOCH 1/100 | BATCH 21/71 | LOSS: 0.3168059844862331\n",
      "TRAIN: EPOCH 1/100 | BATCH 22/71 | LOSS: 0.3144986357377923\n",
      "TRAIN: EPOCH 1/100 | BATCH 23/71 | LOSS: 0.3119656170407931\n",
      "TRAIN: EPOCH 1/100 | BATCH 24/71 | LOSS: 0.30891849637031554\n",
      "TRAIN: EPOCH 1/100 | BATCH 25/71 | LOSS: 0.3067325147298666\n",
      "TRAIN: EPOCH 1/100 | BATCH 26/71 | LOSS: 0.3042298908586855\n",
      "TRAIN: EPOCH 1/100 | BATCH 27/71 | LOSS: 0.30105956005198614\n",
      "TRAIN: EPOCH 1/100 | BATCH 28/71 | LOSS: 0.29752402367263003\n",
      "TRAIN: EPOCH 1/100 | BATCH 29/71 | LOSS: 0.2942670837044716\n",
      "TRAIN: EPOCH 1/100 | BATCH 30/71 | LOSS: 0.29164041626837944\n",
      "TRAIN: EPOCH 1/100 | BATCH 31/71 | LOSS: 0.28860454680398107\n",
      "TRAIN: EPOCH 1/100 | BATCH 32/71 | LOSS: 0.2839800223256602\n",
      "TRAIN: EPOCH 1/100 | BATCH 33/71 | LOSS: 0.2818516120314598\n",
      "TRAIN: EPOCH 1/100 | BATCH 34/71 | LOSS: 0.2802295629467283\n",
      "TRAIN: EPOCH 1/100 | BATCH 35/71 | LOSS: 0.27566176859868896\n",
      "TRAIN: EPOCH 1/100 | BATCH 36/71 | LOSS: 0.2723615785708299\n",
      "TRAIN: EPOCH 1/100 | BATCH 37/71 | LOSS: 0.2699077090150432\n",
      "TRAIN: EPOCH 1/100 | BATCH 38/71 | LOSS: 0.2672025435245954\n",
      "TRAIN: EPOCH 1/100 | BATCH 39/71 | LOSS: 0.2655474107712507\n",
      "TRAIN: EPOCH 1/100 | BATCH 40/71 | LOSS: 0.26271102813685815\n",
      "TRAIN: EPOCH 1/100 | BATCH 41/71 | LOSS: 0.26107983681417646\n",
      "TRAIN: EPOCH 1/100 | BATCH 42/71 | LOSS: 0.2590185053819834\n",
      "TRAIN: EPOCH 1/100 | BATCH 43/71 | LOSS: 0.25653333792632277\n",
      "TRAIN: EPOCH 1/100 | BATCH 44/71 | LOSS: 0.2532452356484201\n",
      "TRAIN: EPOCH 1/100 | BATCH 45/71 | LOSS: 0.2513017247876395\n",
      "TRAIN: EPOCH 1/100 | BATCH 46/71 | LOSS: 0.24889774160816314\n",
      "TRAIN: EPOCH 1/100 | BATCH 47/71 | LOSS: 0.24610334060465297\n",
      "TRAIN: EPOCH 1/100 | BATCH 48/71 | LOSS: 0.24379881167290163\n",
      "TRAIN: EPOCH 1/100 | BATCH 49/71 | LOSS: 0.24127403989434243\n",
      "TRAIN: EPOCH 1/100 | BATCH 50/71 | LOSS: 0.23885953908457475\n",
      "TRAIN: EPOCH 1/100 | BATCH 51/71 | LOSS: 0.2369945365935564\n",
      "TRAIN: EPOCH 1/100 | BATCH 52/71 | LOSS: 0.23547413256370797\n",
      "TRAIN: EPOCH 1/100 | BATCH 53/71 | LOSS: 0.2333036470744345\n",
      "TRAIN: EPOCH 1/100 | BATCH 54/71 | LOSS: 0.2317389436743476\n",
      "TRAIN: EPOCH 1/100 | BATCH 55/71 | LOSS: 0.22969489225319453\n",
      "TRAIN: EPOCH 1/100 | BATCH 56/71 | LOSS: 0.22763888237246296\n",
      "TRAIN: EPOCH 1/100 | BATCH 57/71 | LOSS: 0.22586721014873734\n",
      "TRAIN: EPOCH 1/100 | BATCH 58/71 | LOSS: 0.22451594206741302\n",
      "TRAIN: EPOCH 1/100 | BATCH 59/71 | LOSS: 0.22302923488120238\n",
      "TRAIN: EPOCH 1/100 | BATCH 60/71 | LOSS: 0.2214128239477267\n",
      "TRAIN: EPOCH 1/100 | BATCH 61/71 | LOSS: 0.2199186549311684\n",
      "TRAIN: EPOCH 1/100 | BATCH 62/71 | LOSS: 0.21794572531703918\n",
      "TRAIN: EPOCH 1/100 | BATCH 63/71 | LOSS: 0.2163423306774348\n",
      "TRAIN: EPOCH 1/100 | BATCH 64/71 | LOSS: 0.2148215115070343\n",
      "TRAIN: EPOCH 1/100 | BATCH 65/71 | LOSS: 0.21293474485476813\n",
      "TRAIN: EPOCH 1/100 | BATCH 66/71 | LOSS: 0.2115574318971207\n",
      "TRAIN: EPOCH 1/100 | BATCH 67/71 | LOSS: 0.20998957731267986\n",
      "TRAIN: EPOCH 1/100 | BATCH 68/71 | LOSS: 0.2084079866392025\n",
      "TRAIN: EPOCH 1/100 | BATCH 69/71 | LOSS: 0.20686195843986102\n",
      "TRAIN: EPOCH 1/100 | BATCH 70/71 | LOSS: 0.20526705839684312\n",
      "VAL: EPOCH 1/100 | BATCH 0/8 | LOSS: 0.10589617490768433\n",
      "VAL: EPOCH 1/100 | BATCH 1/8 | LOSS: 0.0972425639629364\n",
      "VAL: EPOCH 1/100 | BATCH 2/8 | LOSS: 0.09962645669778188\n",
      "VAL: EPOCH 1/100 | BATCH 3/8 | LOSS: 0.10466684773564339\n",
      "VAL: EPOCH 1/100 | BATCH 4/8 | LOSS: 0.1027742639183998\n",
      "VAL: EPOCH 1/100 | BATCH 5/8 | LOSS: 0.10031011328101158\n",
      "VAL: EPOCH 1/100 | BATCH 6/8 | LOSS: 0.10047581791877747\n",
      "VAL: EPOCH 1/100 | BATCH 7/8 | LOSS: 0.10170887224376202\n",
      "TRAIN: EPOCH 2/100 | BATCH 0/71 | LOSS: 0.10935936123132706\n",
      "TRAIN: EPOCH 2/100 | BATCH 1/71 | LOSS: 0.1009383574128151\n",
      "TRAIN: EPOCH 2/100 | BATCH 2/71 | LOSS: 0.09824199974536896\n",
      "TRAIN: EPOCH 2/100 | BATCH 3/71 | LOSS: 0.0990463774651289\n",
      "TRAIN: EPOCH 2/100 | BATCH 4/71 | LOSS: 0.09814154654741288\n",
      "TRAIN: EPOCH 2/100 | BATCH 5/71 | LOSS: 0.10118809342384338\n",
      "TRAIN: EPOCH 2/100 | BATCH 6/71 | LOSS: 0.10145943931170873\n",
      "TRAIN: EPOCH 2/100 | BATCH 7/71 | LOSS: 0.0986963864415884\n",
      "TRAIN: EPOCH 2/100 | BATCH 8/71 | LOSS: 0.09877137094736099\n",
      "TRAIN: EPOCH 2/100 | BATCH 9/71 | LOSS: 0.09789481982588769\n",
      "TRAIN: EPOCH 2/100 | BATCH 10/71 | LOSS: 0.09680547294291583\n",
      "TRAIN: EPOCH 2/100 | BATCH 11/71 | LOSS: 0.09580930570761363\n",
      "TRAIN: EPOCH 2/100 | BATCH 12/71 | LOSS: 0.09534780394572479\n",
      "TRAIN: EPOCH 2/100 | BATCH 13/71 | LOSS: 0.09600415719406945\n",
      "TRAIN: EPOCH 2/100 | BATCH 14/71 | LOSS: 0.0963188424706459\n",
      "TRAIN: EPOCH 2/100 | BATCH 15/71 | LOSS: 0.09427025867626071\n",
      "TRAIN: EPOCH 2/100 | BATCH 16/71 | LOSS: 0.09291151209789164\n",
      "TRAIN: EPOCH 2/100 | BATCH 17/71 | LOSS: 0.09223390494783719\n",
      "TRAIN: EPOCH 2/100 | BATCH 18/71 | LOSS: 0.09161887317895889\n",
      "TRAIN: EPOCH 2/100 | BATCH 19/71 | LOSS: 0.0918371606618166\n",
      "TRAIN: EPOCH 2/100 | BATCH 20/71 | LOSS: 0.09173523315361568\n",
      "TRAIN: EPOCH 2/100 | BATCH 21/71 | LOSS: 0.09149774367159064\n",
      "TRAIN: EPOCH 2/100 | BATCH 22/71 | LOSS: 0.09068612689557283\n",
      "TRAIN: EPOCH 2/100 | BATCH 23/71 | LOSS: 0.09012961201369762\n",
      "TRAIN: EPOCH 2/100 | BATCH 24/71 | LOSS: 0.09058548420667649\n",
      "TRAIN: EPOCH 2/100 | BATCH 25/71 | LOSS: 0.09003821101326209\n",
      "TRAIN: EPOCH 2/100 | BATCH 26/71 | LOSS: 0.0894472579713221\n",
      "TRAIN: EPOCH 2/100 | BATCH 27/71 | LOSS: 0.0892083365470171\n",
      "TRAIN: EPOCH 2/100 | BATCH 28/71 | LOSS: 0.08837248975860662\n",
      "TRAIN: EPOCH 2/100 | BATCH 29/71 | LOSS: 0.08771510273218155\n",
      "TRAIN: EPOCH 2/100 | BATCH 30/71 | LOSS: 0.08738342936961882\n",
      "TRAIN: EPOCH 2/100 | BATCH 31/71 | LOSS: 0.08696579351089895\n",
      "TRAIN: EPOCH 2/100 | BATCH 32/71 | LOSS: 0.08653408311533206\n",
      "TRAIN: EPOCH 2/100 | BATCH 33/71 | LOSS: 0.08663615387152224\n",
      "TRAIN: EPOCH 2/100 | BATCH 34/71 | LOSS: 0.08642145054680961\n",
      "TRAIN: EPOCH 2/100 | BATCH 35/71 | LOSS: 0.08563083596527576\n",
      "TRAIN: EPOCH 2/100 | BATCH 36/71 | LOSS: 0.08526464249636675\n",
      "TRAIN: EPOCH 2/100 | BATCH 37/71 | LOSS: 0.08467922771447584\n",
      "TRAIN: EPOCH 2/100 | BATCH 38/71 | LOSS: 0.0840483498878968\n",
      "TRAIN: EPOCH 2/100 | BATCH 39/71 | LOSS: 0.08386880718171597\n",
      "TRAIN: EPOCH 2/100 | BATCH 40/71 | LOSS: 0.08379475572487204\n",
      "TRAIN: EPOCH 2/100 | BATCH 41/71 | LOSS: 0.08334722600522496\n",
      "TRAIN: EPOCH 2/100 | BATCH 42/71 | LOSS: 0.08309355395477871\n",
      "TRAIN: EPOCH 2/100 | BATCH 43/71 | LOSS: 0.0825442284853621\n",
      "TRAIN: EPOCH 2/100 | BATCH 44/71 | LOSS: 0.08229961370428403\n",
      "TRAIN: EPOCH 2/100 | BATCH 45/71 | LOSS: 0.08195391240651193\n",
      "TRAIN: EPOCH 2/100 | BATCH 46/71 | LOSS: 0.08151335570406407\n",
      "TRAIN: EPOCH 2/100 | BATCH 47/71 | LOSS: 0.0810794069742163\n",
      "TRAIN: EPOCH 2/100 | BATCH 48/71 | LOSS: 0.08112669371220531\n",
      "TRAIN: EPOCH 2/100 | BATCH 49/71 | LOSS: 0.08053833313286304\n",
      "TRAIN: EPOCH 2/100 | BATCH 50/71 | LOSS: 0.08023559031825439\n",
      "TRAIN: EPOCH 2/100 | BATCH 51/71 | LOSS: 0.07981880446179555\n",
      "TRAIN: EPOCH 2/100 | BATCH 52/71 | LOSS: 0.07948688031086382\n",
      "TRAIN: EPOCH 2/100 | BATCH 53/71 | LOSS: 0.07918581387235059\n",
      "TRAIN: EPOCH 2/100 | BATCH 54/71 | LOSS: 0.07876657064665447\n",
      "TRAIN: EPOCH 2/100 | BATCH 55/71 | LOSS: 0.07844190093289528\n",
      "TRAIN: EPOCH 2/100 | BATCH 56/71 | LOSS: 0.07822310721926522\n",
      "TRAIN: EPOCH 2/100 | BATCH 57/71 | LOSS: 0.07790021212964222\n",
      "TRAIN: EPOCH 2/100 | BATCH 58/71 | LOSS: 0.07762907527513423\n",
      "TRAIN: EPOCH 2/100 | BATCH 59/71 | LOSS: 0.07748469629635414\n",
      "TRAIN: EPOCH 2/100 | BATCH 60/71 | LOSS: 0.07729034437263599\n",
      "TRAIN: EPOCH 2/100 | BATCH 61/71 | LOSS: 0.07701407493122163\n",
      "TRAIN: EPOCH 2/100 | BATCH 62/71 | LOSS: 0.07676674632562532\n",
      "TRAIN: EPOCH 2/100 | BATCH 63/71 | LOSS: 0.07652856787899509\n",
      "TRAIN: EPOCH 2/100 | BATCH 64/71 | LOSS: 0.07630253308094465\n",
      "TRAIN: EPOCH 2/100 | BATCH 65/71 | LOSS: 0.0760230767455968\n",
      "TRAIN: EPOCH 2/100 | BATCH 66/71 | LOSS: 0.07570556567898437\n",
      "TRAIN: EPOCH 2/100 | BATCH 67/71 | LOSS: 0.07565343275885372\n",
      "TRAIN: EPOCH 2/100 | BATCH 68/71 | LOSS: 0.07546394939223926\n",
      "TRAIN: EPOCH 2/100 | BATCH 69/71 | LOSS: 0.07508480437099933\n",
      "TRAIN: EPOCH 2/100 | BATCH 70/71 | LOSS: 0.07479490192843155\n",
      "VAL: EPOCH 2/100 | BATCH 0/8 | LOSS: 0.04926477372646332\n",
      "VAL: EPOCH 2/100 | BATCH 1/8 | LOSS: 0.05433507449924946\n",
      "VAL: EPOCH 2/100 | BATCH 2/8 | LOSS: 0.05329446370402972\n",
      "VAL: EPOCH 2/100 | BATCH 3/8 | LOSS: 0.05566865485161543\n",
      "VAL: EPOCH 2/100 | BATCH 4/8 | LOSS: 0.05518651232123375\n",
      "VAL: EPOCH 2/100 | BATCH 5/8 | LOSS: 0.05580161387721697\n",
      "VAL: EPOCH 2/100 | BATCH 6/8 | LOSS: 0.056647825453962596\n",
      "VAL: EPOCH 2/100 | BATCH 7/8 | LOSS: 0.05837398860603571\n",
      "TRAIN: EPOCH 3/100 | BATCH 0/71 | LOSS: 0.06649109721183777\n",
      "TRAIN: EPOCH 3/100 | BATCH 1/71 | LOSS: 0.06460471078753471\n",
      "TRAIN: EPOCH 3/100 | BATCH 2/71 | LOSS: 0.06444336225589116\n",
      "TRAIN: EPOCH 3/100 | BATCH 3/71 | LOSS: 0.06455408595502377\n",
      "TRAIN: EPOCH 3/100 | BATCH 4/71 | LOSS: 0.06214560866355896\n",
      "TRAIN: EPOCH 3/100 | BATCH 5/71 | LOSS: 0.06109429523348808\n",
      "TRAIN: EPOCH 3/100 | BATCH 6/71 | LOSS: 0.06120585969516209\n",
      "TRAIN: EPOCH 3/100 | BATCH 7/71 | LOSS: 0.06069981958717108\n",
      "TRAIN: EPOCH 3/100 | BATCH 8/71 | LOSS: 0.06047078222036362\n",
      "TRAIN: EPOCH 3/100 | BATCH 9/71 | LOSS: 0.06097337380051613\n",
      "TRAIN: EPOCH 3/100 | BATCH 10/71 | LOSS: 0.06157857789234682\n",
      "TRAIN: EPOCH 3/100 | BATCH 11/71 | LOSS: 0.060554879096647106\n",
      "TRAIN: EPOCH 3/100 | BATCH 12/71 | LOSS: 0.060908347654801145\n",
      "TRAIN: EPOCH 3/100 | BATCH 13/71 | LOSS: 0.06113105613206114\n",
      "TRAIN: EPOCH 3/100 | BATCH 14/71 | LOSS: 0.0603845976293087\n",
      "TRAIN: EPOCH 3/100 | BATCH 15/71 | LOSS: 0.06033415114507079\n",
      "TRAIN: EPOCH 3/100 | BATCH 16/71 | LOSS: 0.06000962020719752\n",
      "TRAIN: EPOCH 3/100 | BATCH 17/71 | LOSS: 0.059375874077280365\n",
      "TRAIN: EPOCH 3/100 | BATCH 18/71 | LOSS: 0.05939049528617608\n",
      "TRAIN: EPOCH 3/100 | BATCH 19/71 | LOSS: 0.05849157031625509\n",
      "TRAIN: EPOCH 3/100 | BATCH 20/71 | LOSS: 0.05811041770946412\n",
      "TRAIN: EPOCH 3/100 | BATCH 21/71 | LOSS: 0.05776832185008309\n",
      "TRAIN: EPOCH 3/100 | BATCH 22/71 | LOSS: 0.057788894552251564\n",
      "TRAIN: EPOCH 3/100 | BATCH 23/71 | LOSS: 0.05763076152652502\n",
      "TRAIN: EPOCH 3/100 | BATCH 24/71 | LOSS: 0.05704773932695389\n",
      "TRAIN: EPOCH 3/100 | BATCH 25/71 | LOSS: 0.05699639122646589\n",
      "TRAIN: EPOCH 3/100 | BATCH 26/71 | LOSS: 0.05659950290013243\n",
      "TRAIN: EPOCH 3/100 | BATCH 27/71 | LOSS: 0.05641161956425224\n",
      "TRAIN: EPOCH 3/100 | BATCH 28/71 | LOSS: 0.05643404146720623\n",
      "TRAIN: EPOCH 3/100 | BATCH 29/71 | LOSS: 0.056248399118582405\n",
      "TRAIN: EPOCH 3/100 | BATCH 30/71 | LOSS: 0.056395728982264\n",
      "TRAIN: EPOCH 3/100 | BATCH 31/71 | LOSS: 0.05587491253390908\n",
      "TRAIN: EPOCH 3/100 | BATCH 32/71 | LOSS: 0.05563249867973906\n",
      "TRAIN: EPOCH 3/100 | BATCH 33/71 | LOSS: 0.05511445968466647\n",
      "TRAIN: EPOCH 3/100 | BATCH 34/71 | LOSS: 0.05508137026003429\n",
      "TRAIN: EPOCH 3/100 | BATCH 35/71 | LOSS: 0.054754605102870196\n",
      "TRAIN: EPOCH 3/100 | BATCH 36/71 | LOSS: 0.05481908289161888\n",
      "TRAIN: EPOCH 3/100 | BATCH 37/71 | LOSS: 0.05478517868016895\n",
      "TRAIN: EPOCH 3/100 | BATCH 38/71 | LOSS: 0.0545953114827474\n",
      "TRAIN: EPOCH 3/100 | BATCH 39/71 | LOSS: 0.05446401964873075\n",
      "TRAIN: EPOCH 3/100 | BATCH 40/71 | LOSS: 0.05448776883323018\n",
      "TRAIN: EPOCH 3/100 | BATCH 41/71 | LOSS: 0.05433282327084314\n",
      "TRAIN: EPOCH 3/100 | BATCH 42/71 | LOSS: 0.0541957501581935\n",
      "TRAIN: EPOCH 3/100 | BATCH 43/71 | LOSS: 0.05407673450694843\n",
      "TRAIN: EPOCH 3/100 | BATCH 44/71 | LOSS: 0.05367999060286416\n",
      "TRAIN: EPOCH 3/100 | BATCH 45/71 | LOSS: 0.053436990827322006\n",
      "TRAIN: EPOCH 3/100 | BATCH 46/71 | LOSS: 0.0534236083956475\n",
      "TRAIN: EPOCH 3/100 | BATCH 47/71 | LOSS: 0.05330966917487482\n",
      "TRAIN: EPOCH 3/100 | BATCH 48/71 | LOSS: 0.05316459151859186\n",
      "TRAIN: EPOCH 3/100 | BATCH 49/71 | LOSS: 0.05303582258522511\n",
      "TRAIN: EPOCH 3/100 | BATCH 50/71 | LOSS: 0.052799506441635245\n",
      "TRAIN: EPOCH 3/100 | BATCH 51/71 | LOSS: 0.05259445825448403\n",
      "TRAIN: EPOCH 3/100 | BATCH 52/71 | LOSS: 0.0525007834130863\n",
      "TRAIN: EPOCH 3/100 | BATCH 53/71 | LOSS: 0.05239140159553952\n",
      "TRAIN: EPOCH 3/100 | BATCH 54/71 | LOSS: 0.05227122015573762\n",
      "TRAIN: EPOCH 3/100 | BATCH 55/71 | LOSS: 0.05220542096399835\n",
      "TRAIN: EPOCH 3/100 | BATCH 56/71 | LOSS: 0.052022084201637064\n",
      "TRAIN: EPOCH 3/100 | BATCH 57/71 | LOSS: 0.05190085247159004\n",
      "TRAIN: EPOCH 3/100 | BATCH 58/71 | LOSS: 0.05170331825896845\n",
      "TRAIN: EPOCH 3/100 | BATCH 59/71 | LOSS: 0.05167909041047096\n",
      "TRAIN: EPOCH 3/100 | BATCH 60/71 | LOSS: 0.051682008094474916\n",
      "TRAIN: EPOCH 3/100 | BATCH 61/71 | LOSS: 0.05152507366672639\n",
      "TRAIN: EPOCH 3/100 | BATCH 62/71 | LOSS: 0.05140774779849582\n",
      "TRAIN: EPOCH 3/100 | BATCH 63/71 | LOSS: 0.05127884820103645\n",
      "TRAIN: EPOCH 3/100 | BATCH 64/71 | LOSS: 0.051139150387965716\n",
      "TRAIN: EPOCH 3/100 | BATCH 65/71 | LOSS: 0.05110183809742783\n",
      "TRAIN: EPOCH 3/100 | BATCH 66/71 | LOSS: 0.05097581023600564\n",
      "TRAIN: EPOCH 3/100 | BATCH 67/71 | LOSS: 0.05086812349584173\n",
      "TRAIN: EPOCH 3/100 | BATCH 68/71 | LOSS: 0.05072601161141326\n",
      "TRAIN: EPOCH 3/100 | BATCH 69/71 | LOSS: 0.050613794582230705\n",
      "TRAIN: EPOCH 3/100 | BATCH 70/71 | LOSS: 0.05063207227159554\n",
      "VAL: EPOCH 3/100 | BATCH 0/8 | LOSS: 0.03526254743337631\n",
      "VAL: EPOCH 3/100 | BATCH 1/8 | LOSS: 0.04068687930703163\n",
      "VAL: EPOCH 3/100 | BATCH 2/8 | LOSS: 0.03926412761211395\n",
      "VAL: EPOCH 3/100 | BATCH 3/8 | LOSS: 0.0409840727224946\n",
      "VAL: EPOCH 3/100 | BATCH 4/8 | LOSS: 0.04082458540797233\n",
      "VAL: EPOCH 3/100 | BATCH 5/8 | LOSS: 0.04169116417566935\n",
      "VAL: EPOCH 3/100 | BATCH 6/8 | LOSS: 0.042415164943252294\n",
      "VAL: EPOCH 3/100 | BATCH 7/8 | LOSS: 0.04369576694443822\n",
      "TRAIN: EPOCH 4/100 | BATCH 0/71 | LOSS: 0.04559767618775368\n",
      "TRAIN: EPOCH 4/100 | BATCH 1/71 | LOSS: 0.04580163396894932\n",
      "TRAIN: EPOCH 4/100 | BATCH 2/71 | LOSS: 0.04552163556218147\n",
      "TRAIN: EPOCH 4/100 | BATCH 3/71 | LOSS: 0.04539303295314312\n",
      "TRAIN: EPOCH 4/100 | BATCH 4/71 | LOSS: 0.045025753974914554\n",
      "TRAIN: EPOCH 4/100 | BATCH 5/71 | LOSS: 0.044141728430986404\n",
      "TRAIN: EPOCH 4/100 | BATCH 6/71 | LOSS: 0.04578517270939691\n",
      "TRAIN: EPOCH 4/100 | BATCH 7/71 | LOSS: 0.04570646397769451\n",
      "TRAIN: EPOCH 4/100 | BATCH 8/71 | LOSS: 0.045673405958546534\n",
      "TRAIN: EPOCH 4/100 | BATCH 9/71 | LOSS: 0.04522645026445389\n",
      "TRAIN: EPOCH 4/100 | BATCH 10/71 | LOSS: 0.04477386549115181\n",
      "TRAIN: EPOCH 4/100 | BATCH 11/71 | LOSS: 0.044730923448999725\n",
      "TRAIN: EPOCH 4/100 | BATCH 12/71 | LOSS: 0.044409084205444045\n",
      "TRAIN: EPOCH 4/100 | BATCH 13/71 | LOSS: 0.04423358131732259\n",
      "TRAIN: EPOCH 4/100 | BATCH 14/71 | LOSS: 0.043801914403835934\n",
      "TRAIN: EPOCH 4/100 | BATCH 15/71 | LOSS: 0.043341964250430465\n",
      "TRAIN: EPOCH 4/100 | BATCH 16/71 | LOSS: 0.0434459450052065\n",
      "TRAIN: EPOCH 4/100 | BATCH 17/71 | LOSS: 0.043390066673358284\n",
      "TRAIN: EPOCH 4/100 | BATCH 18/71 | LOSS: 0.043169952536884104\n",
      "TRAIN: EPOCH 4/100 | BATCH 19/71 | LOSS: 0.042810484394431116\n",
      "TRAIN: EPOCH 4/100 | BATCH 20/71 | LOSS: 0.042633721161456334\n",
      "TRAIN: EPOCH 4/100 | BATCH 21/71 | LOSS: 0.04241859066215428\n",
      "TRAIN: EPOCH 4/100 | BATCH 22/71 | LOSS: 0.04209464140560316\n",
      "TRAIN: EPOCH 4/100 | BATCH 23/71 | LOSS: 0.04212518703813354\n",
      "TRAIN: EPOCH 4/100 | BATCH 24/71 | LOSS: 0.04228692203760147\n",
      "TRAIN: EPOCH 4/100 | BATCH 25/71 | LOSS: 0.04228390509692522\n",
      "TRAIN: EPOCH 4/100 | BATCH 26/71 | LOSS: 0.04223461545727871\n",
      "TRAIN: EPOCH 4/100 | BATCH 27/71 | LOSS: 0.04224921456937279\n",
      "TRAIN: EPOCH 4/100 | BATCH 28/71 | LOSS: 0.0420853805182309\n",
      "TRAIN: EPOCH 4/100 | BATCH 29/71 | LOSS: 0.041810777659217514\n",
      "TRAIN: EPOCH 4/100 | BATCH 30/71 | LOSS: 0.04189735014111765\n",
      "TRAIN: EPOCH 4/100 | BATCH 31/71 | LOSS: 0.04132132511585951\n",
      "TRAIN: EPOCH 4/100 | BATCH 32/71 | LOSS: 0.04125866689013712\n",
      "TRAIN: EPOCH 4/100 | BATCH 33/71 | LOSS: 0.041115972158663416\n",
      "TRAIN: EPOCH 4/100 | BATCH 34/71 | LOSS: 0.04087416423218591\n",
      "TRAIN: EPOCH 4/100 | BATCH 35/71 | LOSS: 0.04068667938311895\n",
      "TRAIN: EPOCH 4/100 | BATCH 36/71 | LOSS: 0.04060514472626351\n",
      "TRAIN: EPOCH 4/100 | BATCH 37/71 | LOSS: 0.04022153613990859\n",
      "TRAIN: EPOCH 4/100 | BATCH 38/71 | LOSS: 0.04011825347940127\n",
      "TRAIN: EPOCH 4/100 | BATCH 39/71 | LOSS: 0.04005833407863975\n",
      "TRAIN: EPOCH 4/100 | BATCH 40/71 | LOSS: 0.03997901272846431\n",
      "TRAIN: EPOCH 4/100 | BATCH 41/71 | LOSS: 0.03999939605238892\n",
      "TRAIN: EPOCH 4/100 | BATCH 42/71 | LOSS: 0.04011030919676603\n",
      "TRAIN: EPOCH 4/100 | BATCH 43/71 | LOSS: 0.040013342164456844\n",
      "TRAIN: EPOCH 4/100 | BATCH 44/71 | LOSS: 0.0398264203634527\n",
      "TRAIN: EPOCH 4/100 | BATCH 45/71 | LOSS: 0.0398539726662895\n",
      "TRAIN: EPOCH 4/100 | BATCH 46/71 | LOSS: 0.039740209170478456\n",
      "TRAIN: EPOCH 4/100 | BATCH 47/71 | LOSS: 0.03969571801523367\n",
      "TRAIN: EPOCH 4/100 | BATCH 48/71 | LOSS: 0.03952342591115406\n",
      "TRAIN: EPOCH 4/100 | BATCH 49/71 | LOSS: 0.039440760165452955\n",
      "TRAIN: EPOCH 4/100 | BATCH 50/71 | LOSS: 0.0393661808441667\n",
      "TRAIN: EPOCH 4/100 | BATCH 51/71 | LOSS: 0.039063417531836495\n",
      "TRAIN: EPOCH 4/100 | BATCH 52/71 | LOSS: 0.03894646251398438\n",
      "TRAIN: EPOCH 4/100 | BATCH 53/71 | LOSS: 0.03883417023138867\n",
      "TRAIN: EPOCH 4/100 | BATCH 54/71 | LOSS: 0.03893169770863923\n",
      "TRAIN: EPOCH 4/100 | BATCH 55/71 | LOSS: 0.039018818543159535\n",
      "TRAIN: EPOCH 4/100 | BATCH 56/71 | LOSS: 0.03890908016055299\n",
      "TRAIN: EPOCH 4/100 | BATCH 57/71 | LOSS: 0.038814213030554094\n",
      "TRAIN: EPOCH 4/100 | BATCH 58/71 | LOSS: 0.03867398540221029\n",
      "TRAIN: EPOCH 4/100 | BATCH 59/71 | LOSS: 0.03855054921781023\n",
      "TRAIN: EPOCH 4/100 | BATCH 60/71 | LOSS: 0.03844100557511947\n",
      "TRAIN: EPOCH 4/100 | BATCH 61/71 | LOSS: 0.03824291855938012\n",
      "TRAIN: EPOCH 4/100 | BATCH 62/71 | LOSS: 0.038091870645682015\n",
      "TRAIN: EPOCH 4/100 | BATCH 63/71 | LOSS: 0.03806085861288011\n",
      "TRAIN: EPOCH 4/100 | BATCH 64/71 | LOSS: 0.037927224114537236\n",
      "TRAIN: EPOCH 4/100 | BATCH 65/71 | LOSS: 0.03797926034101031\n",
      "TRAIN: EPOCH 4/100 | BATCH 66/71 | LOSS: 0.03790719426278748\n",
      "TRAIN: EPOCH 4/100 | BATCH 67/71 | LOSS: 0.037852955954697204\n",
      "TRAIN: EPOCH 4/100 | BATCH 68/71 | LOSS: 0.037793672846063324\n",
      "TRAIN: EPOCH 4/100 | BATCH 69/71 | LOSS: 0.03773714918643236\n",
      "TRAIN: EPOCH 4/100 | BATCH 70/71 | LOSS: 0.03765012253023369\n",
      "VAL: EPOCH 4/100 | BATCH 0/8 | LOSS: 0.026406820863485336\n",
      "VAL: EPOCH 4/100 | BATCH 1/8 | LOSS: 0.03025016002357006\n",
      "VAL: EPOCH 4/100 | BATCH 2/8 | LOSS: 0.028868410736322403\n",
      "VAL: EPOCH 4/100 | BATCH 3/8 | LOSS: 0.029960217885673046\n",
      "VAL: EPOCH 4/100 | BATCH 4/8 | LOSS: 0.029978257045149802\n",
      "VAL: EPOCH 4/100 | BATCH 5/8 | LOSS: 0.030636227689683437\n",
      "VAL: EPOCH 4/100 | BATCH 6/8 | LOSS: 0.031140882255775586\n",
      "VAL: EPOCH 4/100 | BATCH 7/8 | LOSS: 0.031981994630768895\n",
      "TRAIN: EPOCH 5/100 | BATCH 0/71 | LOSS: 0.03274112567305565\n",
      "TRAIN: EPOCH 5/100 | BATCH 1/71 | LOSS: 0.03162821754813194\n",
      "TRAIN: EPOCH 5/100 | BATCH 2/71 | LOSS: 0.03291179363926252\n",
      "TRAIN: EPOCH 5/100 | BATCH 3/71 | LOSS: 0.033583201467990875\n",
      "TRAIN: EPOCH 5/100 | BATCH 4/71 | LOSS: 0.03348483145236969\n",
      "TRAIN: EPOCH 5/100 | BATCH 5/71 | LOSS: 0.03205827654649814\n",
      "TRAIN: EPOCH 5/100 | BATCH 6/71 | LOSS: 0.03225465677678585\n",
      "TRAIN: EPOCH 5/100 | BATCH 7/71 | LOSS: 0.03162017185240984\n",
      "TRAIN: EPOCH 5/100 | BATCH 8/71 | LOSS: 0.03168970305058691\n",
      "TRAIN: EPOCH 5/100 | BATCH 9/71 | LOSS: 0.031386534497141835\n",
      "TRAIN: EPOCH 5/100 | BATCH 10/71 | LOSS: 0.03169490024447441\n",
      "TRAIN: EPOCH 5/100 | BATCH 11/71 | LOSS: 0.03171297659476598\n",
      "TRAIN: EPOCH 5/100 | BATCH 12/71 | LOSS: 0.03154830677578083\n",
      "TRAIN: EPOCH 5/100 | BATCH 13/71 | LOSS: 0.031183794007769654\n",
      "TRAIN: EPOCH 5/100 | BATCH 14/71 | LOSS: 0.031117521598935126\n",
      "TRAIN: EPOCH 5/100 | BATCH 15/71 | LOSS: 0.031110593932680786\n",
      "TRAIN: EPOCH 5/100 | BATCH 16/71 | LOSS: 0.030992631745689055\n",
      "TRAIN: EPOCH 5/100 | BATCH 17/71 | LOSS: 0.030940819221238296\n",
      "TRAIN: EPOCH 5/100 | BATCH 18/71 | LOSS: 0.0307784435388289\n",
      "TRAIN: EPOCH 5/100 | BATCH 19/71 | LOSS: 0.030558325257152318\n",
      "TRAIN: EPOCH 5/100 | BATCH 20/71 | LOSS: 0.030453440954997427\n",
      "TRAIN: EPOCH 5/100 | BATCH 21/71 | LOSS: 0.030546326952224426\n",
      "TRAIN: EPOCH 5/100 | BATCH 22/71 | LOSS: 0.030471958786897037\n",
      "TRAIN: EPOCH 5/100 | BATCH 23/71 | LOSS: 0.0305912543553859\n",
      "TRAIN: EPOCH 5/100 | BATCH 24/71 | LOSS: 0.030584659725427628\n",
      "TRAIN: EPOCH 5/100 | BATCH 25/71 | LOSS: 0.030384781698767956\n",
      "TRAIN: EPOCH 5/100 | BATCH 26/71 | LOSS: 0.03033247017474086\n",
      "TRAIN: EPOCH 5/100 | BATCH 27/71 | LOSS: 0.03023670380935073\n",
      "TRAIN: EPOCH 5/100 | BATCH 28/71 | LOSS: 0.030163166337999803\n",
      "TRAIN: EPOCH 5/100 | BATCH 29/71 | LOSS: 0.030028056104977927\n",
      "TRAIN: EPOCH 5/100 | BATCH 30/71 | LOSS: 0.03004655626512343\n",
      "TRAIN: EPOCH 5/100 | BATCH 31/71 | LOSS: 0.029991206305567175\n",
      "TRAIN: EPOCH 5/100 | BATCH 32/71 | LOSS: 0.02983505092561245\n",
      "TRAIN: EPOCH 5/100 | BATCH 33/71 | LOSS: 0.02968685915145804\n",
      "TRAIN: EPOCH 5/100 | BATCH 34/71 | LOSS: 0.029617212446672574\n",
      "TRAIN: EPOCH 5/100 | BATCH 35/71 | LOSS: 0.029568328729106322\n",
      "TRAIN: EPOCH 5/100 | BATCH 36/71 | LOSS: 0.029441389280396538\n",
      "TRAIN: EPOCH 5/100 | BATCH 37/71 | LOSS: 0.02934752625266188\n",
      "TRAIN: EPOCH 5/100 | BATCH 38/71 | LOSS: 0.02935183143768555\n",
      "TRAIN: EPOCH 5/100 | BATCH 39/71 | LOSS: 0.02919735377654433\n",
      "TRAIN: EPOCH 5/100 | BATCH 40/71 | LOSS: 0.029098496204469262\n",
      "TRAIN: EPOCH 5/100 | BATCH 41/71 | LOSS: 0.02904023966264157\n",
      "TRAIN: EPOCH 5/100 | BATCH 42/71 | LOSS: 0.029122631910235383\n",
      "TRAIN: EPOCH 5/100 | BATCH 43/71 | LOSS: 0.029074163938110523\n",
      "TRAIN: EPOCH 5/100 | BATCH 44/71 | LOSS: 0.02895362451672554\n",
      "TRAIN: EPOCH 5/100 | BATCH 45/71 | LOSS: 0.028920220535086548\n",
      "TRAIN: EPOCH 5/100 | BATCH 46/71 | LOSS: 0.028829001841392924\n",
      "TRAIN: EPOCH 5/100 | BATCH 47/71 | LOSS: 0.02868711296468973\n",
      "TRAIN: EPOCH 5/100 | BATCH 48/71 | LOSS: 0.028533413626101553\n",
      "TRAIN: EPOCH 5/100 | BATCH 49/71 | LOSS: 0.028445959985256196\n",
      "TRAIN: EPOCH 5/100 | BATCH 50/71 | LOSS: 0.028362931318435015\n",
      "TRAIN: EPOCH 5/100 | BATCH 51/71 | LOSS: 0.02824192691164521\n",
      "TRAIN: EPOCH 5/100 | BATCH 52/71 | LOSS: 0.028099347643975942\n",
      "TRAIN: EPOCH 5/100 | BATCH 53/71 | LOSS: 0.028029360946405818\n",
      "TRAIN: EPOCH 5/100 | BATCH 54/71 | LOSS: 0.027950766411694614\n",
      "TRAIN: EPOCH 5/100 | BATCH 55/71 | LOSS: 0.027963966696656177\n",
      "TRAIN: EPOCH 5/100 | BATCH 56/71 | LOSS: 0.027843692094871874\n",
      "TRAIN: EPOCH 5/100 | BATCH 57/71 | LOSS: 0.027743485820447576\n",
      "TRAIN: EPOCH 5/100 | BATCH 58/71 | LOSS: 0.02764508203934815\n",
      "TRAIN: EPOCH 5/100 | BATCH 59/71 | LOSS: 0.027587649940202633\n",
      "TRAIN: EPOCH 5/100 | BATCH 60/71 | LOSS: 0.02759735658764839\n",
      "TRAIN: EPOCH 5/100 | BATCH 61/71 | LOSS: 0.027514064233870275\n",
      "TRAIN: EPOCH 5/100 | BATCH 62/71 | LOSS: 0.02746325610057702\n",
      "TRAIN: EPOCH 5/100 | BATCH 63/71 | LOSS: 0.027352920296834782\n",
      "TRAIN: EPOCH 5/100 | BATCH 64/71 | LOSS: 0.027217700561651816\n",
      "TRAIN: EPOCH 5/100 | BATCH 65/71 | LOSS: 0.0271171410426949\n",
      "TRAIN: EPOCH 5/100 | BATCH 66/71 | LOSS: 0.027078601740189452\n",
      "TRAIN: EPOCH 5/100 | BATCH 67/71 | LOSS: 0.026981909293681383\n",
      "TRAIN: EPOCH 5/100 | BATCH 68/71 | LOSS: 0.026858332364455513\n",
      "TRAIN: EPOCH 5/100 | BATCH 69/71 | LOSS: 0.026822038500436716\n",
      "TRAIN: EPOCH 5/100 | BATCH 70/71 | LOSS: 0.026785302618649642\n",
      "VAL: EPOCH 5/100 | BATCH 0/8 | LOSS: 0.018127668648958206\n",
      "VAL: EPOCH 5/100 | BATCH 1/8 | LOSS: 0.020315350964665413\n",
      "VAL: EPOCH 5/100 | BATCH 2/8 | LOSS: 0.019378043711185455\n",
      "VAL: EPOCH 5/100 | BATCH 3/8 | LOSS: 0.019969839602708817\n",
      "VAL: EPOCH 5/100 | BATCH 4/8 | LOSS: 0.019956465065479278\n",
      "VAL: EPOCH 5/100 | BATCH 5/8 | LOSS: 0.02036699466407299\n",
      "VAL: EPOCH 5/100 | BATCH 6/8 | LOSS: 0.020646173506975174\n",
      "VAL: EPOCH 5/100 | BATCH 7/8 | LOSS: 0.02110576117411256\n",
      "TRAIN: EPOCH 6/100 | BATCH 0/71 | LOSS: 0.02075798064470291\n",
      "TRAIN: EPOCH 6/100 | BATCH 1/71 | LOSS: 0.01996157504618168\n",
      "TRAIN: EPOCH 6/100 | BATCH 2/71 | LOSS: 0.020315278321504593\n",
      "TRAIN: EPOCH 6/100 | BATCH 3/71 | LOSS: 0.020198575221002102\n",
      "TRAIN: EPOCH 6/100 | BATCH 4/71 | LOSS: 0.020598606020212174\n",
      "TRAIN: EPOCH 6/100 | BATCH 5/71 | LOSS: 0.02115616699059804\n",
      "TRAIN: EPOCH 6/100 | BATCH 6/71 | LOSS: 0.021033494600227902\n",
      "TRAIN: EPOCH 6/100 | BATCH 7/71 | LOSS: 0.021145525388419628\n",
      "TRAIN: EPOCH 6/100 | BATCH 8/71 | LOSS: 0.020527108158502314\n",
      "TRAIN: EPOCH 6/100 | BATCH 9/71 | LOSS: 0.020596882607787848\n",
      "TRAIN: EPOCH 6/100 | BATCH 10/71 | LOSS: 0.02045312887904319\n",
      "TRAIN: EPOCH 6/100 | BATCH 11/71 | LOSS: 0.02042577532120049\n",
      "TRAIN: EPOCH 6/100 | BATCH 12/71 | LOSS: 0.02041618375537487\n",
      "TRAIN: EPOCH 6/100 | BATCH 13/71 | LOSS: 0.02060808554025633\n",
      "TRAIN: EPOCH 6/100 | BATCH 14/71 | LOSS: 0.020702836724619072\n",
      "TRAIN: EPOCH 6/100 | BATCH 15/71 | LOSS: 0.020548094355035573\n",
      "TRAIN: EPOCH 6/100 | BATCH 16/71 | LOSS: 0.020528935389045405\n",
      "TRAIN: EPOCH 6/100 | BATCH 17/71 | LOSS: 0.020413371061699256\n",
      "TRAIN: EPOCH 6/100 | BATCH 18/71 | LOSS: 0.020362580342120247\n",
      "TRAIN: EPOCH 6/100 | BATCH 19/71 | LOSS: 0.02006910159252584\n",
      "TRAIN: EPOCH 6/100 | BATCH 20/71 | LOSS: 0.019805427418933028\n",
      "TRAIN: EPOCH 6/100 | BATCH 21/71 | LOSS: 0.01984653634611856\n",
      "TRAIN: EPOCH 6/100 | BATCH 22/71 | LOSS: 0.01973588420483081\n",
      "TRAIN: EPOCH 6/100 | BATCH 23/71 | LOSS: 0.01969697035383433\n",
      "TRAIN: EPOCH 6/100 | BATCH 24/71 | LOSS: 0.019416112527251242\n",
      "TRAIN: EPOCH 6/100 | BATCH 25/71 | LOSS: 0.01929882297722193\n",
      "TRAIN: EPOCH 6/100 | BATCH 26/71 | LOSS: 0.0192395543196687\n",
      "TRAIN: EPOCH 6/100 | BATCH 27/71 | LOSS: 0.01908142545393535\n",
      "TRAIN: EPOCH 6/100 | BATCH 28/71 | LOSS: 0.018926118211499577\n",
      "TRAIN: EPOCH 6/100 | BATCH 29/71 | LOSS: 0.01877851616591215\n",
      "TRAIN: EPOCH 6/100 | BATCH 30/71 | LOSS: 0.018697507138694485\n",
      "TRAIN: EPOCH 6/100 | BATCH 31/71 | LOSS: 0.018698611936997622\n",
      "TRAIN: EPOCH 6/100 | BATCH 32/71 | LOSS: 0.01873906357496074\n",
      "TRAIN: EPOCH 6/100 | BATCH 33/71 | LOSS: 0.018664306358379477\n",
      "TRAIN: EPOCH 6/100 | BATCH 34/71 | LOSS: 0.018508519019399372\n",
      "TRAIN: EPOCH 6/100 | BATCH 35/71 | LOSS: 0.018427138009832963\n",
      "TRAIN: EPOCH 6/100 | BATCH 36/71 | LOSS: 0.01825661733243111\n",
      "TRAIN: EPOCH 6/100 | BATCH 37/71 | LOSS: 0.01811371760834989\n",
      "TRAIN: EPOCH 6/100 | BATCH 38/71 | LOSS: 0.018057488454267\n",
      "TRAIN: EPOCH 6/100 | BATCH 39/71 | LOSS: 0.017933974624611437\n",
      "TRAIN: EPOCH 6/100 | BATCH 40/71 | LOSS: 0.017892239183732648\n",
      "TRAIN: EPOCH 6/100 | BATCH 41/71 | LOSS: 0.017777526728986276\n",
      "TRAIN: EPOCH 6/100 | BATCH 42/71 | LOSS: 0.017715071003104366\n",
      "TRAIN: EPOCH 6/100 | BATCH 43/71 | LOSS: 0.017623503269119697\n",
      "TRAIN: EPOCH 6/100 | BATCH 44/71 | LOSS: 0.017497763865523867\n",
      "TRAIN: EPOCH 6/100 | BATCH 45/71 | LOSS: 0.017406765007130478\n",
      "TRAIN: EPOCH 6/100 | BATCH 46/71 | LOSS: 0.017385717243590254\n",
      "TRAIN: EPOCH 6/100 | BATCH 47/71 | LOSS: 0.0172626130321684\n",
      "TRAIN: EPOCH 6/100 | BATCH 48/71 | LOSS: 0.01714490403478243\n",
      "TRAIN: EPOCH 6/100 | BATCH 49/71 | LOSS: 0.01704626839607954\n",
      "TRAIN: EPOCH 6/100 | BATCH 50/71 | LOSS: 0.01697107828130909\n",
      "TRAIN: EPOCH 6/100 | BATCH 51/71 | LOSS: 0.016891566230557285\n",
      "TRAIN: EPOCH 6/100 | BATCH 52/71 | LOSS: 0.016808580150300602\n",
      "TRAIN: EPOCH 6/100 | BATCH 53/71 | LOSS: 0.01672164430082948\n",
      "TRAIN: EPOCH 6/100 | BATCH 54/71 | LOSS: 0.016604336283423685\n",
      "TRAIN: EPOCH 6/100 | BATCH 55/71 | LOSS: 0.0165202382320006\n",
      "TRAIN: EPOCH 6/100 | BATCH 56/71 | LOSS: 0.016408864506765417\n",
      "TRAIN: EPOCH 6/100 | BATCH 57/71 | LOSS: 0.016368703226205605\n",
      "TRAIN: EPOCH 6/100 | BATCH 58/71 | LOSS: 0.016302892117429586\n",
      "TRAIN: EPOCH 6/100 | BATCH 59/71 | LOSS: 0.016208162143205602\n",
      "TRAIN: EPOCH 6/100 | BATCH 60/71 | LOSS: 0.01617266882027759\n",
      "TRAIN: EPOCH 6/100 | BATCH 61/71 | LOSS: 0.01611228042372292\n",
      "TRAIN: EPOCH 6/100 | BATCH 62/71 | LOSS: 0.016015769366825385\n",
      "TRAIN: EPOCH 6/100 | BATCH 63/71 | LOSS: 0.015902818893664517\n",
      "TRAIN: EPOCH 6/100 | BATCH 64/71 | LOSS: 0.01584544896792907\n",
      "TRAIN: EPOCH 6/100 | BATCH 65/71 | LOSS: 0.015759054768943424\n",
      "TRAIN: EPOCH 6/100 | BATCH 66/71 | LOSS: 0.01566633497323118\n",
      "TRAIN: EPOCH 6/100 | BATCH 67/71 | LOSS: 0.015586042927358957\n",
      "TRAIN: EPOCH 6/100 | BATCH 68/71 | LOSS: 0.015494510234482046\n",
      "TRAIN: EPOCH 6/100 | BATCH 69/71 | LOSS: 0.015457260076488767\n",
      "TRAIN: EPOCH 6/100 | BATCH 70/71 | LOSS: 0.015344035700225914\n",
      "VAL: EPOCH 6/100 | BATCH 0/8 | LOSS: 0.009734982624650002\n",
      "VAL: EPOCH 6/100 | BATCH 1/8 | LOSS: 0.010318788699805737\n",
      "VAL: EPOCH 6/100 | BATCH 2/8 | LOSS: 0.009731625517209372\n",
      "VAL: EPOCH 6/100 | BATCH 3/8 | LOSS: 0.009956798050552607\n",
      "VAL: EPOCH 6/100 | BATCH 4/8 | LOSS: 0.009956737607717514\n",
      "VAL: EPOCH 6/100 | BATCH 5/8 | LOSS: 0.009895243216305971\n",
      "VAL: EPOCH 6/100 | BATCH 6/8 | LOSS: 0.009934131721300738\n",
      "VAL: EPOCH 6/100 | BATCH 7/8 | LOSS: 0.009854037663899362\n",
      "TRAIN: EPOCH 7/100 | BATCH 0/71 | LOSS: 0.008830782026052475\n",
      "TRAIN: EPOCH 7/100 | BATCH 1/71 | LOSS: 0.009323477745056152\n",
      "TRAIN: EPOCH 7/100 | BATCH 2/71 | LOSS: 0.010161886302133402\n",
      "TRAIN: EPOCH 7/100 | BATCH 3/71 | LOSS: 0.009734300430864096\n",
      "TRAIN: EPOCH 7/100 | BATCH 4/71 | LOSS: 0.009571436233818531\n",
      "TRAIN: EPOCH 7/100 | BATCH 5/71 | LOSS: 0.009460559114813805\n",
      "TRAIN: EPOCH 7/100 | BATCH 6/71 | LOSS: 0.00966975066278662\n",
      "TRAIN: EPOCH 7/100 | BATCH 7/71 | LOSS: 0.009415836015250534\n",
      "TRAIN: EPOCH 7/100 | BATCH 8/71 | LOSS: 0.009113822701490588\n",
      "TRAIN: EPOCH 7/100 | BATCH 9/71 | LOSS: 0.00903756874613464\n",
      "TRAIN: EPOCH 7/100 | BATCH 10/71 | LOSS: 0.008872037965127012\n",
      "TRAIN: EPOCH 7/100 | BATCH 11/71 | LOSS: 0.008749937522225082\n",
      "TRAIN: EPOCH 7/100 | BATCH 12/71 | LOSS: 0.008614092098119168\n",
      "TRAIN: EPOCH 7/100 | BATCH 13/71 | LOSS: 0.008652200756062354\n",
      "TRAIN: EPOCH 7/100 | BATCH 14/71 | LOSS: 0.008654601654658716\n",
      "TRAIN: EPOCH 7/100 | BATCH 15/71 | LOSS: 0.008620838372735307\n",
      "TRAIN: EPOCH 7/100 | BATCH 16/71 | LOSS: 0.008512554169796845\n",
      "TRAIN: EPOCH 7/100 | BATCH 17/71 | LOSS: 0.008349410051272975\n",
      "TRAIN: EPOCH 7/100 | BATCH 18/71 | LOSS: 0.008337904356027903\n",
      "TRAIN: EPOCH 7/100 | BATCH 19/71 | LOSS: 0.008231440605595709\n",
      "TRAIN: EPOCH 7/100 | BATCH 20/71 | LOSS: 0.008280773114945208\n",
      "TRAIN: EPOCH 7/100 | BATCH 21/71 | LOSS: 0.008223166985606606\n",
      "TRAIN: EPOCH 7/100 | BATCH 22/71 | LOSS: 0.008157841901740303\n",
      "TRAIN: EPOCH 7/100 | BATCH 23/71 | LOSS: 0.008107090155438831\n",
      "TRAIN: EPOCH 7/100 | BATCH 24/71 | LOSS: 0.008156250547617674\n",
      "TRAIN: EPOCH 7/100 | BATCH 25/71 | LOSS: 0.008099910856869359\n",
      "TRAIN: EPOCH 7/100 | BATCH 26/71 | LOSS: 0.00806892068228788\n",
      "TRAIN: EPOCH 7/100 | BATCH 27/71 | LOSS: 0.008006280936699892\n",
      "TRAIN: EPOCH 7/100 | BATCH 28/71 | LOSS: 0.007965652256047931\n",
      "TRAIN: EPOCH 7/100 | BATCH 29/71 | LOSS: 0.007896570287023981\n",
      "TRAIN: EPOCH 7/100 | BATCH 30/71 | LOSS: 0.007907517119160584\n",
      "TRAIN: EPOCH 7/100 | BATCH 31/71 | LOSS: 0.007912159824627452\n",
      "TRAIN: EPOCH 7/100 | BATCH 32/71 | LOSS: 0.0078607895411551\n",
      "TRAIN: EPOCH 7/100 | BATCH 33/71 | LOSS: 0.007826540276736897\n",
      "TRAIN: EPOCH 7/100 | BATCH 34/71 | LOSS: 0.0077959122535373486\n",
      "TRAIN: EPOCH 7/100 | BATCH 35/71 | LOSS: 0.00778836026115136\n",
      "TRAIN: EPOCH 7/100 | BATCH 36/71 | LOSS: 0.007749544771237148\n",
      "TRAIN: EPOCH 7/100 | BATCH 37/71 | LOSS: 0.007697419716829532\n",
      "TRAIN: EPOCH 7/100 | BATCH 38/71 | LOSS: 0.007659652150976352\n",
      "TRAIN: EPOCH 7/100 | BATCH 39/71 | LOSS: 0.007607806520536542\n",
      "TRAIN: EPOCH 7/100 | BATCH 40/71 | LOSS: 0.007574182923701479\n",
      "TRAIN: EPOCH 7/100 | BATCH 41/71 | LOSS: 0.007527777415123724\n",
      "TRAIN: EPOCH 7/100 | BATCH 42/71 | LOSS: 0.007457287776348896\n",
      "TRAIN: EPOCH 7/100 | BATCH 43/71 | LOSS: 0.0073924413776363836\n",
      "TRAIN: EPOCH 7/100 | BATCH 44/71 | LOSS: 0.007335611857059929\n",
      "TRAIN: EPOCH 7/100 | BATCH 45/71 | LOSS: 0.007292253511917332\n",
      "TRAIN: EPOCH 7/100 | BATCH 46/71 | LOSS: 0.007256397640609995\n",
      "TRAIN: EPOCH 7/100 | BATCH 47/71 | LOSS: 0.007230173330754042\n",
      "TRAIN: EPOCH 7/100 | BATCH 48/71 | LOSS: 0.007190230663637726\n",
      "TRAIN: EPOCH 7/100 | BATCH 49/71 | LOSS: 0.007171569084748626\n",
      "TRAIN: EPOCH 7/100 | BATCH 50/71 | LOSS: 0.007140972149357491\n",
      "TRAIN: EPOCH 7/100 | BATCH 51/71 | LOSS: 0.007097253029664548\n",
      "TRAIN: EPOCH 7/100 | BATCH 52/71 | LOSS: 0.007043121716464465\n",
      "TRAIN: EPOCH 7/100 | BATCH 53/71 | LOSS: 0.006980060228939961\n",
      "TRAIN: EPOCH 7/100 | BATCH 54/71 | LOSS: 0.00696047411554239\n",
      "TRAIN: EPOCH 7/100 | BATCH 55/71 | LOSS: 0.006915160063986799\n",
      "TRAIN: EPOCH 7/100 | BATCH 56/71 | LOSS: 0.006869304598423473\n",
      "TRAIN: EPOCH 7/100 | BATCH 57/71 | LOSS: 0.006811986922222222\n",
      "TRAIN: EPOCH 7/100 | BATCH 58/71 | LOSS: 0.006774097740239763\n",
      "TRAIN: EPOCH 7/100 | BATCH 59/71 | LOSS: 0.006730415994146218\n",
      "TRAIN: EPOCH 7/100 | BATCH 60/71 | LOSS: 0.0067010255354899365\n",
      "TRAIN: EPOCH 7/100 | BATCH 61/71 | LOSS: 0.006673189049075929\n",
      "TRAIN: EPOCH 7/100 | BATCH 62/71 | LOSS: 0.006648966957563682\n",
      "TRAIN: EPOCH 7/100 | BATCH 63/71 | LOSS: 0.006613373978325399\n",
      "TRAIN: EPOCH 7/100 | BATCH 64/71 | LOSS: 0.0065906162194621105\n",
      "TRAIN: EPOCH 7/100 | BATCH 65/71 | LOSS: 0.006542871106472431\n",
      "TRAIN: EPOCH 7/100 | BATCH 66/71 | LOSS: 0.006496339664558199\n",
      "TRAIN: EPOCH 7/100 | BATCH 67/71 | LOSS: 0.006463549746994805\n",
      "TRAIN: EPOCH 7/100 | BATCH 68/71 | LOSS: 0.006416745008090916\n",
      "TRAIN: EPOCH 7/100 | BATCH 69/71 | LOSS: 0.006385148978526039\n",
      "TRAIN: EPOCH 7/100 | BATCH 70/71 | LOSS: 0.006332330884490634\n",
      "VAL: EPOCH 7/100 | BATCH 0/8 | LOSS: 0.0036109494976699352\n",
      "VAL: EPOCH 7/100 | BATCH 1/8 | LOSS: 0.003916129469871521\n",
      "VAL: EPOCH 7/100 | BATCH 2/8 | LOSS: 0.003802780993282795\n",
      "VAL: EPOCH 7/100 | BATCH 3/8 | LOSS: 0.003896835958585143\n",
      "VAL: EPOCH 7/100 | BATCH 4/8 | LOSS: 0.003958286810666322\n",
      "VAL: EPOCH 7/100 | BATCH 5/8 | LOSS: 0.003903656421850125\n",
      "VAL: EPOCH 7/100 | BATCH 6/8 | LOSS: 0.0039419300322021756\n",
      "VAL: EPOCH 7/100 | BATCH 7/8 | LOSS: 0.0038569518947042525\n",
      "TRAIN: EPOCH 8/100 | BATCH 0/71 | LOSS: 0.004338567145168781\n",
      "TRAIN: EPOCH 8/100 | BATCH 1/71 | LOSS: 0.0038441343931481242\n",
      "TRAIN: EPOCH 8/100 | BATCH 2/71 | LOSS: 0.0037238480678449073\n",
      "TRAIN: EPOCH 8/100 | BATCH 3/71 | LOSS: 0.00374774681404233\n",
      "TRAIN: EPOCH 8/100 | BATCH 4/71 | LOSS: 0.0036302931606769563\n",
      "TRAIN: EPOCH 8/100 | BATCH 5/71 | LOSS: 0.003566107867906491\n",
      "TRAIN: EPOCH 8/100 | BATCH 6/71 | LOSS: 0.003539605299010873\n",
      "TRAIN: EPOCH 8/100 | BATCH 7/71 | LOSS: 0.00349045533221215\n",
      "TRAIN: EPOCH 8/100 | BATCH 8/71 | LOSS: 0.0034512056348224482\n",
      "TRAIN: EPOCH 8/100 | BATCH 9/71 | LOSS: 0.003495288826525211\n",
      "TRAIN: EPOCH 8/100 | BATCH 10/71 | LOSS: 0.0035260612653060393\n",
      "TRAIN: EPOCH 8/100 | BATCH 11/71 | LOSS: 0.003446484605471293\n",
      "TRAIN: EPOCH 8/100 | BATCH 12/71 | LOSS: 0.0034173758378109108\n",
      "TRAIN: EPOCH 8/100 | BATCH 13/71 | LOSS: 0.0033784264898193733\n",
      "TRAIN: EPOCH 8/100 | BATCH 14/71 | LOSS: 0.00339103772615393\n",
      "TRAIN: EPOCH 8/100 | BATCH 15/71 | LOSS: 0.003402839574846439\n",
      "TRAIN: EPOCH 8/100 | BATCH 16/71 | LOSS: 0.0033724927480387338\n",
      "TRAIN: EPOCH 8/100 | BATCH 17/71 | LOSS: 0.003368726220085389\n",
      "TRAIN: EPOCH 8/100 | BATCH 18/71 | LOSS: 0.0033213064736245493\n",
      "TRAIN: EPOCH 8/100 | BATCH 19/71 | LOSS: 0.003323012264445424\n",
      "TRAIN: EPOCH 8/100 | BATCH 20/71 | LOSS: 0.0033066324512695984\n",
      "TRAIN: EPOCH 8/100 | BATCH 21/71 | LOSS: 0.0033009555719962173\n",
      "TRAIN: EPOCH 8/100 | BATCH 22/71 | LOSS: 0.0032947656556801953\n",
      "TRAIN: EPOCH 8/100 | BATCH 23/71 | LOSS: 0.003303153488862639\n",
      "TRAIN: EPOCH 8/100 | BATCH 24/71 | LOSS: 0.0033033630438148976\n",
      "TRAIN: EPOCH 8/100 | BATCH 25/71 | LOSS: 0.0032645631402444383\n",
      "TRAIN: EPOCH 8/100 | BATCH 26/71 | LOSS: 0.003239415758461864\n",
      "TRAIN: EPOCH 8/100 | BATCH 27/71 | LOSS: 0.0032148681431343512\n",
      "TRAIN: EPOCH 8/100 | BATCH 28/71 | LOSS: 0.003189101502106621\n",
      "TRAIN: EPOCH 8/100 | BATCH 29/71 | LOSS: 0.003159384080208838\n",
      "TRAIN: EPOCH 8/100 | BATCH 30/71 | LOSS: 0.003131024284108031\n",
      "TRAIN: EPOCH 8/100 | BATCH 31/71 | LOSS: 0.0031279470276786014\n",
      "TRAIN: EPOCH 8/100 | BATCH 32/71 | LOSS: 0.0030936915004117923\n",
      "TRAIN: EPOCH 8/100 | BATCH 33/71 | LOSS: 0.0030827242401702437\n",
      "TRAIN: EPOCH 8/100 | BATCH 34/71 | LOSS: 0.003070147887670568\n",
      "TRAIN: EPOCH 8/100 | BATCH 35/71 | LOSS: 0.003056058084540483\n",
      "TRAIN: EPOCH 8/100 | BATCH 36/71 | LOSS: 0.0030369358955303558\n",
      "TRAIN: EPOCH 8/100 | BATCH 37/71 | LOSS: 0.00302174673293178\n",
      "TRAIN: EPOCH 8/100 | BATCH 38/71 | LOSS: 0.0029980433471978474\n",
      "TRAIN: EPOCH 8/100 | BATCH 39/71 | LOSS: 0.002988810179522261\n",
      "TRAIN: EPOCH 8/100 | BATCH 40/71 | LOSS: 0.0029682875667675964\n",
      "TRAIN: EPOCH 8/100 | BATCH 41/71 | LOSS: 0.0029494615903656396\n",
      "TRAIN: EPOCH 8/100 | BATCH 42/71 | LOSS: 0.002945679548564692\n",
      "TRAIN: EPOCH 8/100 | BATCH 43/71 | LOSS: 0.0029200474223629994\n",
      "TRAIN: EPOCH 8/100 | BATCH 44/71 | LOSS: 0.002920199144217703\n",
      "TRAIN: EPOCH 8/100 | BATCH 45/71 | LOSS: 0.0028902113629990945\n",
      "TRAIN: EPOCH 8/100 | BATCH 46/71 | LOSS: 0.0028891474196806235\n",
      "TRAIN: EPOCH 8/100 | BATCH 47/71 | LOSS: 0.002880975899946255\n",
      "TRAIN: EPOCH 8/100 | BATCH 48/71 | LOSS: 0.002861136325863095\n",
      "TRAIN: EPOCH 8/100 | BATCH 49/71 | LOSS: 0.002841800816822797\n",
      "TRAIN: EPOCH 8/100 | BATCH 50/71 | LOSS: 0.0028347614718893287\n",
      "TRAIN: EPOCH 8/100 | BATCH 51/71 | LOSS: 0.0028272311521085123\n",
      "TRAIN: EPOCH 8/100 | BATCH 52/71 | LOSS: 0.002812132448010709\n",
      "TRAIN: EPOCH 8/100 | BATCH 53/71 | LOSS: 0.0028124767316815755\n",
      "TRAIN: EPOCH 8/100 | BATCH 54/71 | LOSS: 0.0028047388821670956\n",
      "TRAIN: EPOCH 8/100 | BATCH 55/71 | LOSS: 0.002801812081348284\n",
      "TRAIN: EPOCH 8/100 | BATCH 56/71 | LOSS: 0.002798832530204795\n",
      "TRAIN: EPOCH 8/100 | BATCH 57/71 | LOSS: 0.0027792371231420286\n",
      "TRAIN: EPOCH 8/100 | BATCH 58/71 | LOSS: 0.0027806055179591905\n",
      "TRAIN: EPOCH 8/100 | BATCH 59/71 | LOSS: 0.0027628771087620406\n",
      "TRAIN: EPOCH 8/100 | BATCH 60/71 | LOSS: 0.002762752690077683\n",
      "TRAIN: EPOCH 8/100 | BATCH 61/71 | LOSS: 0.0027432893233884487\n",
      "TRAIN: EPOCH 8/100 | BATCH 62/71 | LOSS: 0.002728910427466626\n",
      "TRAIN: EPOCH 8/100 | BATCH 63/71 | LOSS: 0.0027103669872303726\n",
      "TRAIN: EPOCH 8/100 | BATCH 64/71 | LOSS: 0.0026937057425339634\n",
      "TRAIN: EPOCH 8/100 | BATCH 65/71 | LOSS: 0.0026797277722366607\n",
      "TRAIN: EPOCH 8/100 | BATCH 66/71 | LOSS: 0.0026740274833404083\n",
      "TRAIN: EPOCH 8/100 | BATCH 67/71 | LOSS: 0.0026612447177553\n",
      "TRAIN: EPOCH 8/100 | BATCH 68/71 | LOSS: 0.0026430158025544624\n",
      "TRAIN: EPOCH 8/100 | BATCH 69/71 | LOSS: 0.002632413585005062\n",
      "TRAIN: EPOCH 8/100 | BATCH 70/71 | LOSS: 0.0026139458805375117\n",
      "VAL: EPOCH 8/100 | BATCH 0/8 | LOSS: 0.0013632004847750068\n",
      "VAL: EPOCH 8/100 | BATCH 1/8 | LOSS: 0.0015182768111117184\n",
      "VAL: EPOCH 8/100 | BATCH 2/8 | LOSS: 0.0015558881762747963\n",
      "VAL: EPOCH 8/100 | BATCH 3/8 | LOSS: 0.0016058647306635976\n",
      "VAL: EPOCH 8/100 | BATCH 4/8 | LOSS: 0.0016397190745919942\n",
      "VAL: EPOCH 8/100 | BATCH 5/8 | LOSS: 0.0016328787702756624\n",
      "VAL: EPOCH 8/100 | BATCH 6/8 | LOSS: 0.001678234980707722\n",
      "VAL: EPOCH 8/100 | BATCH 7/8 | LOSS: 0.001615835470147431\n",
      "TRAIN: EPOCH 9/100 | BATCH 0/71 | LOSS: 0.001482913619838655\n",
      "TRAIN: EPOCH 9/100 | BATCH 1/71 | LOSS: 0.0017380162025801837\n",
      "TRAIN: EPOCH 9/100 | BATCH 2/71 | LOSS: 0.0017299429358293612\n",
      "TRAIN: EPOCH 9/100 | BATCH 3/71 | LOSS: 0.0015810951008461416\n",
      "TRAIN: EPOCH 9/100 | BATCH 4/71 | LOSS: 0.0014461762271821498\n",
      "TRAIN: EPOCH 9/100 | BATCH 5/71 | LOSS: 0.0014506096680027742\n",
      "TRAIN: EPOCH 9/100 | BATCH 6/71 | LOSS: 0.0014248485011713846\n",
      "TRAIN: EPOCH 9/100 | BATCH 7/71 | LOSS: 0.0014601322181988508\n",
      "TRAIN: EPOCH 9/100 | BATCH 8/71 | LOSS: 0.001422763633955684\n",
      "TRAIN: EPOCH 9/100 | BATCH 9/71 | LOSS: 0.0014532088651321828\n",
      "TRAIN: EPOCH 9/100 | BATCH 10/71 | LOSS: 0.001450638403184712\n",
      "TRAIN: EPOCH 9/100 | BATCH 11/71 | LOSS: 0.0014891715545672923\n",
      "TRAIN: EPOCH 9/100 | BATCH 12/71 | LOSS: 0.0015131201272687088\n",
      "TRAIN: EPOCH 9/100 | BATCH 13/71 | LOSS: 0.0015518864882843836\n",
      "TRAIN: EPOCH 9/100 | BATCH 14/71 | LOSS: 0.0015306937197844188\n",
      "TRAIN: EPOCH 9/100 | BATCH 15/71 | LOSS: 0.0015343976265285164\n",
      "TRAIN: EPOCH 9/100 | BATCH 16/71 | LOSS: 0.0015268730794024818\n",
      "TRAIN: EPOCH 9/100 | BATCH 17/71 | LOSS: 0.0015133161020154755\n",
      "TRAIN: EPOCH 9/100 | BATCH 18/71 | LOSS: 0.00148378128774072\n",
      "TRAIN: EPOCH 9/100 | BATCH 19/71 | LOSS: 0.0014937655243556947\n",
      "TRAIN: EPOCH 9/100 | BATCH 20/71 | LOSS: 0.0014897309953258151\n",
      "TRAIN: EPOCH 9/100 | BATCH 21/71 | LOSS: 0.0014918956545774233\n",
      "TRAIN: EPOCH 9/100 | BATCH 22/71 | LOSS: 0.0014896663665042622\n",
      "TRAIN: EPOCH 9/100 | BATCH 23/71 | LOSS: 0.001477152676670812\n",
      "TRAIN: EPOCH 9/100 | BATCH 24/71 | LOSS: 0.0014810942439362406\n",
      "TRAIN: EPOCH 9/100 | BATCH 25/71 | LOSS: 0.0014622096249905343\n",
      "TRAIN: EPOCH 9/100 | BATCH 26/71 | LOSS: 0.0014472119122122724\n",
      "TRAIN: EPOCH 9/100 | BATCH 27/71 | LOSS: 0.0014451129162417991\n",
      "TRAIN: EPOCH 9/100 | BATCH 28/71 | LOSS: 0.0014422026639483098\n",
      "TRAIN: EPOCH 9/100 | BATCH 29/71 | LOSS: 0.0014446798401574294\n",
      "TRAIN: EPOCH 9/100 | BATCH 30/71 | LOSS: 0.0014348219133793346\n",
      "TRAIN: EPOCH 9/100 | BATCH 31/71 | LOSS: 0.0014217412644939031\n",
      "TRAIN: EPOCH 9/100 | BATCH 32/71 | LOSS: 0.001415772307099718\n",
      "TRAIN: EPOCH 9/100 | BATCH 33/71 | LOSS: 0.0014005311754504766\n",
      "TRAIN: EPOCH 9/100 | BATCH 34/71 | LOSS: 0.0013826782266343279\n",
      "TRAIN: EPOCH 9/100 | BATCH 35/71 | LOSS: 0.001375768642497456\n",
      "TRAIN: EPOCH 9/100 | BATCH 36/71 | LOSS: 0.0013603719967618785\n",
      "TRAIN: EPOCH 9/100 | BATCH 37/71 | LOSS: 0.0013538113884064124\n",
      "TRAIN: EPOCH 9/100 | BATCH 38/71 | LOSS: 0.001347909688961525\n",
      "TRAIN: EPOCH 9/100 | BATCH 39/71 | LOSS: 0.0013396219597780145\n",
      "TRAIN: EPOCH 9/100 | BATCH 40/71 | LOSS: 0.0013326080303025863\n",
      "TRAIN: EPOCH 9/100 | BATCH 41/71 | LOSS: 0.001327569143281185\n",
      "TRAIN: EPOCH 9/100 | BATCH 42/71 | LOSS: 0.0013192640441513166\n",
      "TRAIN: EPOCH 9/100 | BATCH 43/71 | LOSS: 0.001306686349297789\n",
      "TRAIN: EPOCH 9/100 | BATCH 44/71 | LOSS: 0.001297523391743501\n",
      "TRAIN: EPOCH 9/100 | BATCH 45/71 | LOSS: 0.0012922323050746775\n",
      "TRAIN: EPOCH 9/100 | BATCH 46/71 | LOSS: 0.0012792231604219118\n",
      "TRAIN: EPOCH 9/100 | BATCH 47/71 | LOSS: 0.0012659064450417645\n",
      "TRAIN: EPOCH 9/100 | BATCH 48/71 | LOSS: 0.0012575710258845771\n",
      "TRAIN: EPOCH 9/100 | BATCH 49/71 | LOSS: 0.0012529281456954778\n",
      "TRAIN: EPOCH 9/100 | BATCH 50/71 | LOSS: 0.0012522692034276678\n",
      "TRAIN: EPOCH 9/100 | BATCH 51/71 | LOSS: 0.0012460610641238208\n",
      "TRAIN: EPOCH 9/100 | BATCH 52/71 | LOSS: 0.0012390386359326822\n",
      "TRAIN: EPOCH 9/100 | BATCH 53/71 | LOSS: 0.0012317746069230554\n",
      "TRAIN: EPOCH 9/100 | BATCH 54/71 | LOSS: 0.0012257658484899861\n",
      "TRAIN: EPOCH 9/100 | BATCH 55/71 | LOSS: 0.0012159656169907457\n",
      "TRAIN: EPOCH 9/100 | BATCH 56/71 | LOSS: 0.0012122130750580446\n",
      "TRAIN: EPOCH 9/100 | BATCH 57/71 | LOSS: 0.00121034309970504\n",
      "TRAIN: EPOCH 9/100 | BATCH 58/71 | LOSS: 0.0012005460399234574\n",
      "TRAIN: EPOCH 9/100 | BATCH 59/71 | LOSS: 0.0011948073069409778\n",
      "TRAIN: EPOCH 9/100 | BATCH 60/71 | LOSS: 0.0011848720748618734\n",
      "TRAIN: EPOCH 9/100 | BATCH 61/71 | LOSS: 0.0011825497401127172\n",
      "TRAIN: EPOCH 9/100 | BATCH 62/71 | LOSS: 0.001175276269870145\n",
      "TRAIN: EPOCH 9/100 | BATCH 63/71 | LOSS: 0.001167478001661948\n",
      "TRAIN: EPOCH 9/100 | BATCH 64/71 | LOSS: 0.0011628314440783399\n",
      "TRAIN: EPOCH 9/100 | BATCH 65/71 | LOSS: 0.0011628091628098805\n",
      "TRAIN: EPOCH 9/100 | BATCH 66/71 | LOSS: 0.0011585556516257017\n",
      "TRAIN: EPOCH 9/100 | BATCH 67/71 | LOSS: 0.0011549869899788652\n",
      "TRAIN: EPOCH 9/100 | BATCH 68/71 | LOSS: 0.0011507681211578133\n",
      "TRAIN: EPOCH 9/100 | BATCH 69/71 | LOSS: 0.001146909164630675\n",
      "TRAIN: EPOCH 9/100 | BATCH 70/71 | LOSS: 0.001139138138223387\n",
      "VAL: EPOCH 9/100 | BATCH 0/8 | LOSS: 0.000637784949503839\n",
      "VAL: EPOCH 9/100 | BATCH 1/8 | LOSS: 0.0007400387257803231\n",
      "VAL: EPOCH 9/100 | BATCH 2/8 | LOSS: 0.0007581010965319971\n",
      "VAL: EPOCH 9/100 | BATCH 3/8 | LOSS: 0.0007760981243336573\n",
      "VAL: EPOCH 9/100 | BATCH 4/8 | LOSS: 0.0007910489104688168\n",
      "VAL: EPOCH 9/100 | BATCH 5/8 | LOSS: 0.0007944434570769469\n",
      "VAL: EPOCH 9/100 | BATCH 6/8 | LOSS: 0.0008282000697883112\n",
      "VAL: EPOCH 9/100 | BATCH 7/8 | LOSS: 0.0007924462188384496\n",
      "TRAIN: EPOCH 10/100 | BATCH 0/71 | LOSS: 0.0005623135366477072\n",
      "TRAIN: EPOCH 10/100 | BATCH 1/71 | LOSS: 0.0007816997531335801\n",
      "TRAIN: EPOCH 10/100 | BATCH 2/71 | LOSS: 0.0008087596118760606\n",
      "TRAIN: EPOCH 10/100 | BATCH 3/71 | LOSS: 0.0008284038922283798\n",
      "TRAIN: EPOCH 10/100 | BATCH 4/71 | LOSS: 0.0008951833238825202\n",
      "TRAIN: EPOCH 10/100 | BATCH 5/71 | LOSS: 0.0008599710029860338\n",
      "TRAIN: EPOCH 10/100 | BATCH 6/71 | LOSS: 0.0008287135528267495\n",
      "TRAIN: EPOCH 10/100 | BATCH 7/71 | LOSS: 0.0008186620325432159\n",
      "TRAIN: EPOCH 10/100 | BATCH 8/71 | LOSS: 0.0008224373127126859\n",
      "TRAIN: EPOCH 10/100 | BATCH 9/71 | LOSS: 0.0008903548994567245\n",
      "TRAIN: EPOCH 10/100 | BATCH 10/71 | LOSS: 0.0008884460072625767\n",
      "TRAIN: EPOCH 10/100 | BATCH 11/71 | LOSS: 0.0008980429265648127\n",
      "TRAIN: EPOCH 10/100 | BATCH 12/71 | LOSS: 0.0008807782567321108\n",
      "TRAIN: EPOCH 10/100 | BATCH 13/71 | LOSS: 0.000882405076741374\n",
      "TRAIN: EPOCH 10/100 | BATCH 14/71 | LOSS: 0.0008806943272550901\n",
      "TRAIN: EPOCH 10/100 | BATCH 15/71 | LOSS: 0.0008538120509911096\n",
      "TRAIN: EPOCH 10/100 | BATCH 16/71 | LOSS: 0.0008698664246099617\n",
      "TRAIN: EPOCH 10/100 | BATCH 17/71 | LOSS: 0.0008539315490957557\n",
      "TRAIN: EPOCH 10/100 | BATCH 18/71 | LOSS: 0.000871976302924419\n",
      "TRAIN: EPOCH 10/100 | BATCH 19/71 | LOSS: 0.0008646854650578462\n",
      "TRAIN: EPOCH 10/100 | BATCH 20/71 | LOSS: 0.0008649294794027117\n",
      "TRAIN: EPOCH 10/100 | BATCH 21/71 | LOSS: 0.0008593677074796605\n",
      "TRAIN: EPOCH 10/100 | BATCH 22/71 | LOSS: 0.0008519227026820021\n",
      "TRAIN: EPOCH 10/100 | BATCH 23/71 | LOSS: 0.0008560301442533577\n",
      "TRAIN: EPOCH 10/100 | BATCH 24/71 | LOSS: 0.0008498535666149109\n",
      "TRAIN: EPOCH 10/100 | BATCH 25/71 | LOSS: 0.0008565960082565793\n",
      "TRAIN: EPOCH 10/100 | BATCH 26/71 | LOSS: 0.0008411929507187948\n",
      "TRAIN: EPOCH 10/100 | BATCH 27/71 | LOSS: 0.0008328828116646037\n",
      "TRAIN: EPOCH 10/100 | BATCH 28/71 | LOSS: 0.0008326074758801481\n",
      "TRAIN: EPOCH 10/100 | BATCH 29/71 | LOSS: 0.0008292688270254682\n",
      "TRAIN: EPOCH 10/100 | BATCH 30/71 | LOSS: 0.0008241412034558673\n",
      "TRAIN: EPOCH 10/100 | BATCH 31/71 | LOSS: 0.000816716219560476\n",
      "TRAIN: EPOCH 10/100 | BATCH 32/71 | LOSS: 0.0008074703245339068\n",
      "TRAIN: EPOCH 10/100 | BATCH 33/71 | LOSS: 0.0008022321186110596\n",
      "TRAIN: EPOCH 10/100 | BATCH 34/71 | LOSS: 0.000793442248167204\n",
      "TRAIN: EPOCH 10/100 | BATCH 35/71 | LOSS: 0.0007831301263649948\n",
      "TRAIN: EPOCH 10/100 | BATCH 36/71 | LOSS: 0.0007768828709723075\n",
      "TRAIN: EPOCH 10/100 | BATCH 37/71 | LOSS: 0.000776319236856101\n",
      "TRAIN: EPOCH 10/100 | BATCH 38/71 | LOSS: 0.0007696934633900244\n",
      "TRAIN: EPOCH 10/100 | BATCH 39/71 | LOSS: 0.0007806655827153009\n",
      "TRAIN: EPOCH 10/100 | BATCH 40/71 | LOSS: 0.0007771472340143035\n",
      "TRAIN: EPOCH 10/100 | BATCH 41/71 | LOSS: 0.0007742224184940348\n",
      "TRAIN: EPOCH 10/100 | BATCH 42/71 | LOSS: 0.0007702057827859684\n",
      "TRAIN: EPOCH 10/100 | BATCH 43/71 | LOSS: 0.0007728714071245948\n",
      "TRAIN: EPOCH 10/100 | BATCH 44/71 | LOSS: 0.0007645998525226282\n",
      "TRAIN: EPOCH 10/100 | BATCH 45/71 | LOSS: 0.0007596207230918757\n",
      "TRAIN: EPOCH 10/100 | BATCH 46/71 | LOSS: 0.0007600093438745813\n",
      "TRAIN: EPOCH 10/100 | BATCH 47/71 | LOSS: 0.000754036411914664\n",
      "TRAIN: EPOCH 10/100 | BATCH 48/71 | LOSS: 0.0007506464655530088\n",
      "TRAIN: EPOCH 10/100 | BATCH 49/71 | LOSS: 0.0007497864519245923\n",
      "TRAIN: EPOCH 10/100 | BATCH 50/71 | LOSS: 0.0007446359765405456\n",
      "TRAIN: EPOCH 10/100 | BATCH 51/71 | LOSS: 0.0007419097371614323\n",
      "TRAIN: EPOCH 10/100 | BATCH 52/71 | LOSS: 0.0007400727778110864\n",
      "TRAIN: EPOCH 10/100 | BATCH 53/71 | LOSS: 0.0007384093715464351\n",
      "TRAIN: EPOCH 10/100 | BATCH 54/71 | LOSS: 0.00074001264504411\n",
      "TRAIN: EPOCH 10/100 | BATCH 55/71 | LOSS: 0.0007368744793763783\n",
      "TRAIN: EPOCH 10/100 | BATCH 56/71 | LOSS: 0.0007376085360240387\n",
      "TRAIN: EPOCH 10/100 | BATCH 57/71 | LOSS: 0.0007336722929753235\n",
      "TRAIN: EPOCH 10/100 | BATCH 58/71 | LOSS: 0.0007287231984279924\n",
      "TRAIN: EPOCH 10/100 | BATCH 59/71 | LOSS: 0.0007285964859571929\n",
      "TRAIN: EPOCH 10/100 | BATCH 60/71 | LOSS: 0.0007277032557004666\n",
      "TRAIN: EPOCH 10/100 | BATCH 61/71 | LOSS: 0.0007226140014902358\n",
      "TRAIN: EPOCH 10/100 | BATCH 62/71 | LOSS: 0.0007191157071954674\n",
      "TRAIN: EPOCH 10/100 | BATCH 63/71 | LOSS: 0.0007165776078181807\n",
      "TRAIN: EPOCH 10/100 | BATCH 64/71 | LOSS: 0.0007158863680580488\n",
      "TRAIN: EPOCH 10/100 | BATCH 65/71 | LOSS: 0.0007120400850632877\n",
      "TRAIN: EPOCH 10/100 | BATCH 66/71 | LOSS: 0.0007128700676527041\n",
      "TRAIN: EPOCH 10/100 | BATCH 67/71 | LOSS: 0.0007115596390041687\n",
      "TRAIN: EPOCH 10/100 | BATCH 68/71 | LOSS: 0.0007159067562484331\n",
      "TRAIN: EPOCH 10/100 | BATCH 69/71 | LOSS: 0.0007146452952708517\n",
      "TRAIN: EPOCH 10/100 | BATCH 70/71 | LOSS: 0.0007131121213167487\n",
      "VAL: EPOCH 10/100 | BATCH 0/8 | LOSS: 0.00047276250552386045\n",
      "VAL: EPOCH 10/100 | BATCH 1/8 | LOSS: 0.0005326514365151525\n",
      "VAL: EPOCH 10/100 | BATCH 2/8 | LOSS: 0.0005410877250445386\n",
      "VAL: EPOCH 10/100 | BATCH 3/8 | LOSS: 0.0005569945060415193\n",
      "VAL: EPOCH 10/100 | BATCH 4/8 | LOSS: 0.0005709654767997562\n",
      "VAL: EPOCH 10/100 | BATCH 5/8 | LOSS: 0.0005733183061238378\n",
      "VAL: EPOCH 10/100 | BATCH 6/8 | LOSS: 0.0005954099386664373\n",
      "VAL: EPOCH 10/100 | BATCH 7/8 | LOSS: 0.0005685676442226395\n",
      "TRAIN: EPOCH 11/100 | BATCH 0/71 | LOSS: 0.0008593496168032289\n",
      "TRAIN: EPOCH 11/100 | BATCH 1/71 | LOSS: 0.0007695565582253039\n",
      "TRAIN: EPOCH 11/100 | BATCH 2/71 | LOSS: 0.0007477068963150183\n",
      "TRAIN: EPOCH 11/100 | BATCH 3/71 | LOSS: 0.0007071167492540553\n",
      "TRAIN: EPOCH 11/100 | BATCH 4/71 | LOSS: 0.0007244683220051229\n",
      "TRAIN: EPOCH 11/100 | BATCH 5/71 | LOSS: 0.0006727732252329588\n",
      "TRAIN: EPOCH 11/100 | BATCH 6/71 | LOSS: 0.0006649451762703913\n",
      "TRAIN: EPOCH 11/100 | BATCH 7/71 | LOSS: 0.0006679688667645678\n",
      "TRAIN: EPOCH 11/100 | BATCH 8/71 | LOSS: 0.0006637039542612103\n",
      "TRAIN: EPOCH 11/100 | BATCH 9/71 | LOSS: 0.0006824966752901673\n",
      "TRAIN: EPOCH 11/100 | BATCH 10/71 | LOSS: 0.0006618542459116063\n",
      "TRAIN: EPOCH 11/100 | BATCH 11/71 | LOSS: 0.0006522049710232144\n",
      "TRAIN: EPOCH 11/100 | BATCH 12/71 | LOSS: 0.000636964658042416\n",
      "TRAIN: EPOCH 11/100 | BATCH 13/71 | LOSS: 0.0006251375120232946\n",
      "TRAIN: EPOCH 11/100 | BATCH 14/71 | LOSS: 0.00061603017966263\n",
      "TRAIN: EPOCH 11/100 | BATCH 15/71 | LOSS: 0.0006264151979848975\n",
      "TRAIN: EPOCH 11/100 | BATCH 16/71 | LOSS: 0.0006240767526084228\n",
      "TRAIN: EPOCH 11/100 | BATCH 17/71 | LOSS: 0.0006194081264159953\n",
      "TRAIN: EPOCH 11/100 | BATCH 18/71 | LOSS: 0.0006107543350050324\n",
      "TRAIN: EPOCH 11/100 | BATCH 19/71 | LOSS: 0.0006055896170437336\n",
      "TRAIN: EPOCH 11/100 | BATCH 20/71 | LOSS: 0.0006003862529593919\n",
      "TRAIN: EPOCH 11/100 | BATCH 21/71 | LOSS: 0.0005977878145958213\n",
      "TRAIN: EPOCH 11/100 | BATCH 22/71 | LOSS: 0.0005975161127381674\n",
      "TRAIN: EPOCH 11/100 | BATCH 23/71 | LOSS: 0.0005918718585841513\n",
      "TRAIN: EPOCH 11/100 | BATCH 24/71 | LOSS: 0.0005891776329372078\n",
      "TRAIN: EPOCH 11/100 | BATCH 25/71 | LOSS: 0.0005881449783023876\n",
      "TRAIN: EPOCH 11/100 | BATCH 26/71 | LOSS: 0.000589645132465564\n",
      "TRAIN: EPOCH 11/100 | BATCH 27/71 | LOSS: 0.0005926900952804967\n",
      "TRAIN: EPOCH 11/100 | BATCH 28/71 | LOSS: 0.0005982472964739106\n",
      "TRAIN: EPOCH 11/100 | BATCH 29/71 | LOSS: 0.0005982176876083638\n",
      "TRAIN: EPOCH 11/100 | BATCH 30/71 | LOSS: 0.0005907691262226792\n",
      "TRAIN: EPOCH 11/100 | BATCH 31/71 | LOSS: 0.0005892501931157312\n",
      "TRAIN: EPOCH 11/100 | BATCH 32/71 | LOSS: 0.0005896356138471289\n",
      "TRAIN: EPOCH 11/100 | BATCH 33/71 | LOSS: 0.0005825772857540013\n",
      "TRAIN: EPOCH 11/100 | BATCH 34/71 | LOSS: 0.0005769149666385991\n",
      "TRAIN: EPOCH 11/100 | BATCH 35/71 | LOSS: 0.0005785929469210613\n",
      "TRAIN: EPOCH 11/100 | BATCH 36/71 | LOSS: 0.0005727841682430055\n",
      "TRAIN: EPOCH 11/100 | BATCH 37/71 | LOSS: 0.0005644343787265059\n",
      "TRAIN: EPOCH 11/100 | BATCH 38/71 | LOSS: 0.000570160680068418\n",
      "TRAIN: EPOCH 11/100 | BATCH 39/71 | LOSS: 0.0005715342427720316\n",
      "TRAIN: EPOCH 11/100 | BATCH 40/71 | LOSS: 0.0005775090994131638\n",
      "TRAIN: EPOCH 11/100 | BATCH 41/71 | LOSS: 0.0005712684256390535\n",
      "TRAIN: EPOCH 11/100 | BATCH 42/71 | LOSS: 0.0005695128706446307\n",
      "TRAIN: EPOCH 11/100 | BATCH 43/71 | LOSS: 0.0005714899962185882\n",
      "TRAIN: EPOCH 11/100 | BATCH 44/71 | LOSS: 0.0005668769241310656\n",
      "TRAIN: EPOCH 11/100 | BATCH 45/71 | LOSS: 0.000566822252712091\n",
      "TRAIN: EPOCH 11/100 | BATCH 46/71 | LOSS: 0.0005678386697446571\n",
      "TRAIN: EPOCH 11/100 | BATCH 47/71 | LOSS: 0.0005729593819220705\n",
      "TRAIN: EPOCH 11/100 | BATCH 48/71 | LOSS: 0.0005713480641134083\n",
      "TRAIN: EPOCH 11/100 | BATCH 49/71 | LOSS: 0.000570540571352467\n",
      "TRAIN: EPOCH 11/100 | BATCH 50/71 | LOSS: 0.000573816453563232\n",
      "TRAIN: EPOCH 11/100 | BATCH 51/71 | LOSS: 0.0005719489776171171\n",
      "TRAIN: EPOCH 11/100 | BATCH 52/71 | LOSS: 0.0005686397484134672\n",
      "TRAIN: EPOCH 11/100 | BATCH 53/71 | LOSS: 0.0005726347272111862\n",
      "TRAIN: EPOCH 11/100 | BATCH 54/71 | LOSS: 0.0005705225424290719\n",
      "TRAIN: EPOCH 11/100 | BATCH 55/71 | LOSS: 0.0005774382459224268\n",
      "TRAIN: EPOCH 11/100 | BATCH 56/71 | LOSS: 0.0005753149590945165\n",
      "TRAIN: EPOCH 11/100 | BATCH 57/71 | LOSS: 0.0005780390457747953\n",
      "TRAIN: EPOCH 11/100 | BATCH 58/71 | LOSS: 0.0005804829080431265\n",
      "TRAIN: EPOCH 11/100 | BATCH 59/71 | LOSS: 0.0005816900840727613\n",
      "TRAIN: EPOCH 11/100 | BATCH 60/71 | LOSS: 0.0005782492555196962\n",
      "TRAIN: EPOCH 11/100 | BATCH 61/71 | LOSS: 0.000574397198731951\n",
      "TRAIN: EPOCH 11/100 | BATCH 62/71 | LOSS: 0.0005725969413539307\n",
      "TRAIN: EPOCH 11/100 | BATCH 63/71 | LOSS: 0.0005721872316826193\n",
      "TRAIN: EPOCH 11/100 | BATCH 64/71 | LOSS: 0.0005694668363923064\n",
      "TRAIN: EPOCH 11/100 | BATCH 65/71 | LOSS: 0.0005679280390419686\n",
      "TRAIN: EPOCH 11/100 | BATCH 66/71 | LOSS: 0.0005665396851897517\n",
      "TRAIN: EPOCH 11/100 | BATCH 67/71 | LOSS: 0.0005655434039962368\n",
      "TRAIN: EPOCH 11/100 | BATCH 68/71 | LOSS: 0.0005614529245708516\n",
      "TRAIN: EPOCH 11/100 | BATCH 69/71 | LOSS: 0.0005631567744006004\n",
      "TRAIN: EPOCH 11/100 | BATCH 70/71 | LOSS: 0.0005613820242833241\n",
      "VAL: EPOCH 11/100 | BATCH 0/8 | LOSS: 0.0003716791106853634\n",
      "VAL: EPOCH 11/100 | BATCH 1/8 | LOSS: 0.0004207780148135498\n",
      "VAL: EPOCH 11/100 | BATCH 2/8 | LOSS: 0.0004281867974592994\n",
      "VAL: EPOCH 11/100 | BATCH 3/8 | LOSS: 0.00044296435225987807\n",
      "VAL: EPOCH 11/100 | BATCH 4/8 | LOSS: 0.00045399042428471146\n",
      "VAL: EPOCH 11/100 | BATCH 5/8 | LOSS: 0.000453723783721216\n",
      "VAL: EPOCH 11/100 | BATCH 6/8 | LOSS: 0.00047271309969281513\n",
      "VAL: EPOCH 11/100 | BATCH 7/8 | LOSS: 0.00045275019147084095\n",
      "TRAIN: EPOCH 12/100 | BATCH 0/71 | LOSS: 0.0004168835002928972\n",
      "TRAIN: EPOCH 12/100 | BATCH 1/71 | LOSS: 0.0004580773529596627\n",
      "TRAIN: EPOCH 12/100 | BATCH 2/71 | LOSS: 0.0004152453232867022\n",
      "TRAIN: EPOCH 12/100 | BATCH 3/71 | LOSS: 0.0004461970383999869\n",
      "TRAIN: EPOCH 12/100 | BATCH 4/71 | LOSS: 0.0004326808266341686\n",
      "TRAIN: EPOCH 12/100 | BATCH 5/71 | LOSS: 0.0004618201540627827\n",
      "TRAIN: EPOCH 12/100 | BATCH 6/71 | LOSS: 0.0005451501097663172\n",
      "TRAIN: EPOCH 12/100 | BATCH 7/71 | LOSS: 0.000568105788261164\n",
      "TRAIN: EPOCH 12/100 | BATCH 8/71 | LOSS: 0.0005352106818463653\n",
      "TRAIN: EPOCH 12/100 | BATCH 9/71 | LOSS: 0.0005298489646520466\n",
      "TRAIN: EPOCH 12/100 | BATCH 10/71 | LOSS: 0.0005043419818817215\n",
      "TRAIN: EPOCH 12/100 | BATCH 11/71 | LOSS: 0.0005204551707720384\n",
      "TRAIN: EPOCH 12/100 | BATCH 12/71 | LOSS: 0.0005217385164891871\n",
      "TRAIN: EPOCH 12/100 | BATCH 13/71 | LOSS: 0.0005295644930031683\n",
      "TRAIN: EPOCH 12/100 | BATCH 14/71 | LOSS: 0.000519913660051922\n",
      "TRAIN: EPOCH 12/100 | BATCH 15/71 | LOSS: 0.000513278653670568\n",
      "TRAIN: EPOCH 12/100 | BATCH 16/71 | LOSS: 0.0005055568557616104\n",
      "TRAIN: EPOCH 12/100 | BATCH 17/71 | LOSS: 0.0005028104610068516\n",
      "TRAIN: EPOCH 12/100 | BATCH 18/71 | LOSS: 0.0005050010115251338\n",
      "TRAIN: EPOCH 12/100 | BATCH 19/71 | LOSS: 0.0005033968496718444\n",
      "TRAIN: EPOCH 12/100 | BATCH 20/71 | LOSS: 0.0004945544739409039\n",
      "TRAIN: EPOCH 12/100 | BATCH 21/71 | LOSS: 0.0004885239865292202\n",
      "TRAIN: EPOCH 12/100 | BATCH 22/71 | LOSS: 0.0004884961476225568\n",
      "TRAIN: EPOCH 12/100 | BATCH 23/71 | LOSS: 0.0004798170472592271\n",
      "TRAIN: EPOCH 12/100 | BATCH 24/71 | LOSS: 0.00047535644145682457\n",
      "TRAIN: EPOCH 12/100 | BATCH 25/71 | LOSS: 0.0004768793721898244\n",
      "TRAIN: EPOCH 12/100 | BATCH 26/71 | LOSS: 0.00047744798077339374\n",
      "TRAIN: EPOCH 12/100 | BATCH 27/71 | LOSS: 0.0004828140476352668\n",
      "TRAIN: EPOCH 12/100 | BATCH 28/71 | LOSS: 0.0004821884393659902\n",
      "TRAIN: EPOCH 12/100 | BATCH 29/71 | LOSS: 0.00048605880971687535\n",
      "TRAIN: EPOCH 12/100 | BATCH 30/71 | LOSS: 0.00048676985991938463\n",
      "TRAIN: EPOCH 12/100 | BATCH 31/71 | LOSS: 0.0004832941649510758\n",
      "TRAIN: EPOCH 12/100 | BATCH 32/71 | LOSS: 0.0004809370377298557\n",
      "TRAIN: EPOCH 12/100 | BATCH 33/71 | LOSS: 0.00047467967413146705\n",
      "TRAIN: EPOCH 12/100 | BATCH 34/71 | LOSS: 0.000493820384144783\n",
      "TRAIN: EPOCH 12/100 | BATCH 35/71 | LOSS: 0.0004938778357528564\n",
      "TRAIN: EPOCH 12/100 | BATCH 36/71 | LOSS: 0.0004902121381255219\n",
      "TRAIN: EPOCH 12/100 | BATCH 37/71 | LOSS: 0.00048724727055646087\n",
      "TRAIN: EPOCH 12/100 | BATCH 38/71 | LOSS: 0.0004855090295537733\n",
      "TRAIN: EPOCH 12/100 | BATCH 39/71 | LOSS: 0.0004820936250325758\n",
      "TRAIN: EPOCH 12/100 | BATCH 40/71 | LOSS: 0.0004852232828497796\n",
      "TRAIN: EPOCH 12/100 | BATCH 41/71 | LOSS: 0.0004840251986890854\n",
      "TRAIN: EPOCH 12/100 | BATCH 42/71 | LOSS: 0.00048258861183015587\n",
      "TRAIN: EPOCH 12/100 | BATCH 43/71 | LOSS: 0.00048262786995937034\n",
      "TRAIN: EPOCH 12/100 | BATCH 44/71 | LOSS: 0.0004813977911706186\n",
      "TRAIN: EPOCH 12/100 | BATCH 45/71 | LOSS: 0.0004775896820280215\n",
      "TRAIN: EPOCH 12/100 | BATCH 46/71 | LOSS: 0.0004750197261641238\n",
      "TRAIN: EPOCH 12/100 | BATCH 47/71 | LOSS: 0.0004757553585174416\n",
      "TRAIN: EPOCH 12/100 | BATCH 48/71 | LOSS: 0.0004726153458658682\n",
      "TRAIN: EPOCH 12/100 | BATCH 49/71 | LOSS: 0.00047520703985355796\n",
      "TRAIN: EPOCH 12/100 | BATCH 50/71 | LOSS: 0.00047470860904557445\n",
      "TRAIN: EPOCH 12/100 | BATCH 51/71 | LOSS: 0.0004785930460247283\n",
      "TRAIN: EPOCH 12/100 | BATCH 52/71 | LOSS: 0.000484826374922018\n",
      "TRAIN: EPOCH 12/100 | BATCH 53/71 | LOSS: 0.0004867671738827118\n",
      "TRAIN: EPOCH 12/100 | BATCH 54/71 | LOSS: 0.0004835598953914913\n",
      "TRAIN: EPOCH 12/100 | BATCH 55/71 | LOSS: 0.0004809066984827431\n",
      "TRAIN: EPOCH 12/100 | BATCH 56/71 | LOSS: 0.00047887003063551035\n",
      "TRAIN: EPOCH 12/100 | BATCH 57/71 | LOSS: 0.00047856813912874023\n",
      "TRAIN: EPOCH 12/100 | BATCH 58/71 | LOSS: 0.000480800610079067\n",
      "TRAIN: EPOCH 12/100 | BATCH 59/71 | LOSS: 0.00048132713466960314\n",
      "TRAIN: EPOCH 12/100 | BATCH 60/71 | LOSS: 0.0004872814566087833\n",
      "TRAIN: EPOCH 12/100 | BATCH 61/71 | LOSS: 0.0004840618306432941\n",
      "TRAIN: EPOCH 12/100 | BATCH 62/71 | LOSS: 0.0004826298889903618\n",
      "TRAIN: EPOCH 12/100 | BATCH 63/71 | LOSS: 0.0004812450752069708\n",
      "TRAIN: EPOCH 12/100 | BATCH 64/71 | LOSS: 0.00048079696806290975\n",
      "TRAIN: EPOCH 12/100 | BATCH 65/71 | LOSS: 0.0004790253984868865\n",
      "TRAIN: EPOCH 12/100 | BATCH 66/71 | LOSS: 0.0004762757583451805\n",
      "TRAIN: EPOCH 12/100 | BATCH 67/71 | LOSS: 0.0004768570326953469\n",
      "TRAIN: EPOCH 12/100 | BATCH 68/71 | LOSS: 0.000475762671355725\n",
      "TRAIN: EPOCH 12/100 | BATCH 69/71 | LOSS: 0.00047723491235436604\n",
      "TRAIN: EPOCH 12/100 | BATCH 70/71 | LOSS: 0.00048210310239777705\n",
      "VAL: EPOCH 12/100 | BATCH 0/8 | LOSS: 0.0003232655581086874\n",
      "VAL: EPOCH 12/100 | BATCH 1/8 | LOSS: 0.0003604338417062536\n",
      "VAL: EPOCH 12/100 | BATCH 2/8 | LOSS: 0.0003695893780483554\n",
      "VAL: EPOCH 12/100 | BATCH 3/8 | LOSS: 0.0003874925969284959\n",
      "VAL: EPOCH 12/100 | BATCH 4/8 | LOSS: 0.00039547571213915944\n",
      "VAL: EPOCH 12/100 | BATCH 5/8 | LOSS: 0.00039686382418343175\n",
      "VAL: EPOCH 12/100 | BATCH 6/8 | LOSS: 0.0004120620557971831\n",
      "VAL: EPOCH 12/100 | BATCH 7/8 | LOSS: 0.00039408760858350433\n",
      "TRAIN: EPOCH 13/100 | BATCH 0/71 | LOSS: 0.0003288171428721398\n",
      "TRAIN: EPOCH 13/100 | BATCH 1/71 | LOSS: 0.00039831902540754527\n",
      "TRAIN: EPOCH 13/100 | BATCH 2/71 | LOSS: 0.0004235671561521788\n",
      "TRAIN: EPOCH 13/100 | BATCH 3/71 | LOSS: 0.00040388014167547226\n",
      "TRAIN: EPOCH 13/100 | BATCH 4/71 | LOSS: 0.0004029077186714858\n",
      "TRAIN: EPOCH 13/100 | BATCH 5/71 | LOSS: 0.0004035003318373735\n",
      "TRAIN: EPOCH 13/100 | BATCH 6/71 | LOSS: 0.0003997057002769517\n",
      "TRAIN: EPOCH 13/100 | BATCH 7/71 | LOSS: 0.00040131097557605244\n",
      "TRAIN: EPOCH 13/100 | BATCH 8/71 | LOSS: 0.0003959089614606152\n",
      "TRAIN: EPOCH 13/100 | BATCH 9/71 | LOSS: 0.00039555853290949015\n",
      "TRAIN: EPOCH 13/100 | BATCH 10/71 | LOSS: 0.00040960729132745075\n",
      "TRAIN: EPOCH 13/100 | BATCH 11/71 | LOSS: 0.00041996894530408707\n",
      "TRAIN: EPOCH 13/100 | BATCH 12/71 | LOSS: 0.0004293973391983085\n",
      "TRAIN: EPOCH 13/100 | BATCH 13/71 | LOSS: 0.0004220200706705717\n",
      "TRAIN: EPOCH 13/100 | BATCH 14/71 | LOSS: 0.00043463981128297744\n",
      "TRAIN: EPOCH 13/100 | BATCH 15/71 | LOSS: 0.0004270960362191545\n",
      "TRAIN: EPOCH 13/100 | BATCH 16/71 | LOSS: 0.00041974057710510404\n",
      "TRAIN: EPOCH 13/100 | BATCH 17/71 | LOSS: 0.00041627216948351514\n",
      "TRAIN: EPOCH 13/100 | BATCH 18/71 | LOSS: 0.00041189255027443563\n",
      "TRAIN: EPOCH 13/100 | BATCH 19/71 | LOSS: 0.0004126803847611882\n",
      "TRAIN: EPOCH 13/100 | BATCH 20/71 | LOSS: 0.000410605033504821\n",
      "TRAIN: EPOCH 13/100 | BATCH 21/71 | LOSS: 0.00040777317190077156\n",
      "TRAIN: EPOCH 13/100 | BATCH 22/71 | LOSS: 0.00040871651625544155\n",
      "TRAIN: EPOCH 13/100 | BATCH 23/71 | LOSS: 0.00040951949631562456\n",
      "TRAIN: EPOCH 13/100 | BATCH 24/71 | LOSS: 0.0004121064196806401\n",
      "TRAIN: EPOCH 13/100 | BATCH 25/71 | LOSS: 0.0004106297934553228\n",
      "TRAIN: EPOCH 13/100 | BATCH 26/71 | LOSS: 0.0004052946050823839\n",
      "TRAIN: EPOCH 13/100 | BATCH 27/71 | LOSS: 0.00041071942541748285\n",
      "TRAIN: EPOCH 13/100 | BATCH 28/71 | LOSS: 0.0004125658727796941\n",
      "TRAIN: EPOCH 13/100 | BATCH 29/71 | LOSS: 0.00040764642568926016\n",
      "TRAIN: EPOCH 13/100 | BATCH 30/71 | LOSS: 0.00040976221357742624\n",
      "TRAIN: EPOCH 13/100 | BATCH 31/71 | LOSS: 0.0004147186300542671\n",
      "TRAIN: EPOCH 13/100 | BATCH 32/71 | LOSS: 0.00040965334681624716\n",
      "TRAIN: EPOCH 13/100 | BATCH 33/71 | LOSS: 0.00040964605132638314\n",
      "TRAIN: EPOCH 13/100 | BATCH 34/71 | LOSS: 0.0004103429216359343\n",
      "TRAIN: EPOCH 13/100 | BATCH 35/71 | LOSS: 0.0004134231518643598\n",
      "TRAIN: EPOCH 13/100 | BATCH 36/71 | LOSS: 0.00041114497690688113\n",
      "TRAIN: EPOCH 13/100 | BATCH 37/71 | LOSS: 0.00040917723412061796\n",
      "TRAIN: EPOCH 13/100 | BATCH 38/71 | LOSS: 0.00040442114569043787\n",
      "TRAIN: EPOCH 13/100 | BATCH 39/71 | LOSS: 0.00041112034923571625\n",
      "TRAIN: EPOCH 13/100 | BATCH 40/71 | LOSS: 0.00041260575221158655\n",
      "TRAIN: EPOCH 13/100 | BATCH 41/71 | LOSS: 0.0004098790533524672\n",
      "TRAIN: EPOCH 13/100 | BATCH 42/71 | LOSS: 0.00041299118344986075\n",
      "TRAIN: EPOCH 13/100 | BATCH 43/71 | LOSS: 0.00041153377473661254\n",
      "TRAIN: EPOCH 13/100 | BATCH 44/71 | LOSS: 0.0004101113755799209\n",
      "TRAIN: EPOCH 13/100 | BATCH 45/71 | LOSS: 0.00041558104598864344\n",
      "TRAIN: EPOCH 13/100 | BATCH 46/71 | LOSS: 0.00042249681211344183\n",
      "TRAIN: EPOCH 13/100 | BATCH 47/71 | LOSS: 0.0004221738020836104\n",
      "TRAIN: EPOCH 13/100 | BATCH 48/71 | LOSS: 0.00041931961355398277\n",
      "TRAIN: EPOCH 13/100 | BATCH 49/71 | LOSS: 0.0004159384858212434\n",
      "TRAIN: EPOCH 13/100 | BATCH 50/71 | LOSS: 0.0004167270486576337\n",
      "TRAIN: EPOCH 13/100 | BATCH 51/71 | LOSS: 0.0004216473174053065\n",
      "TRAIN: EPOCH 13/100 | BATCH 52/71 | LOSS: 0.000420378867859671\n",
      "TRAIN: EPOCH 13/100 | BATCH 53/71 | LOSS: 0.0004165162269297677\n",
      "TRAIN: EPOCH 13/100 | BATCH 54/71 | LOSS: 0.0004183837696804072\n",
      "TRAIN: EPOCH 13/100 | BATCH 55/71 | LOSS: 0.0004200650725709108\n",
      "TRAIN: EPOCH 13/100 | BATCH 56/71 | LOSS: 0.00042272721628633966\n",
      "TRAIN: EPOCH 13/100 | BATCH 57/71 | LOSS: 0.0004202975218629882\n",
      "TRAIN: EPOCH 13/100 | BATCH 58/71 | LOSS: 0.0004196327692193721\n",
      "TRAIN: EPOCH 13/100 | BATCH 59/71 | LOSS: 0.0004188945179824562\n",
      "TRAIN: EPOCH 13/100 | BATCH 60/71 | LOSS: 0.0004174899418177999\n",
      "TRAIN: EPOCH 13/100 | BATCH 61/71 | LOSS: 0.0004161206788041689\n",
      "TRAIN: EPOCH 13/100 | BATCH 62/71 | LOSS: 0.00041449112315950474\n",
      "TRAIN: EPOCH 13/100 | BATCH 63/71 | LOSS: 0.0004158648100656137\n",
      "TRAIN: EPOCH 13/100 | BATCH 64/71 | LOSS: 0.0004151663279536968\n",
      "TRAIN: EPOCH 13/100 | BATCH 65/71 | LOSS: 0.0004153985366539211\n",
      "TRAIN: EPOCH 13/100 | BATCH 66/71 | LOSS: 0.0004138469675628802\n",
      "TRAIN: EPOCH 13/100 | BATCH 67/71 | LOSS: 0.00041318847048815396\n",
      "TRAIN: EPOCH 13/100 | BATCH 68/71 | LOSS: 0.00041459084668136\n",
      "TRAIN: EPOCH 13/100 | BATCH 69/71 | LOSS: 0.00041373696731170637\n",
      "TRAIN: EPOCH 13/100 | BATCH 70/71 | LOSS: 0.0004120220327307142\n",
      "VAL: EPOCH 13/100 | BATCH 0/8 | LOSS: 0.00030127164791338146\n",
      "VAL: EPOCH 13/100 | BATCH 1/8 | LOSS: 0.0003260015801060945\n",
      "VAL: EPOCH 13/100 | BATCH 2/8 | LOSS: 0.00032935390481725335\n",
      "VAL: EPOCH 13/100 | BATCH 3/8 | LOSS: 0.0003473133110674098\n",
      "VAL: EPOCH 13/100 | BATCH 4/8 | LOSS: 0.000349957263097167\n",
      "VAL: EPOCH 13/100 | BATCH 5/8 | LOSS: 0.00034838420591161895\n",
      "VAL: EPOCH 13/100 | BATCH 6/8 | LOSS: 0.0003617271390144846\n",
      "VAL: EPOCH 13/100 | BATCH 7/8 | LOSS: 0.0003443440145929344\n",
      "TRAIN: EPOCH 14/100 | BATCH 0/71 | LOSS: 0.00035186985041946173\n",
      "TRAIN: EPOCH 14/100 | BATCH 1/71 | LOSS: 0.0003767957678064704\n",
      "TRAIN: EPOCH 14/100 | BATCH 2/71 | LOSS: 0.00033014436000181985\n",
      "TRAIN: EPOCH 14/100 | BATCH 3/71 | LOSS: 0.00035293996552354656\n",
      "TRAIN: EPOCH 14/100 | BATCH 4/71 | LOSS: 0.00045232681732159107\n",
      "TRAIN: EPOCH 14/100 | BATCH 5/71 | LOSS: 0.0004236285870623154\n",
      "TRAIN: EPOCH 14/100 | BATCH 6/71 | LOSS: 0.00041234443571218956\n",
      "TRAIN: EPOCH 14/100 | BATCH 7/71 | LOSS: 0.0004048672581120627\n",
      "TRAIN: EPOCH 14/100 | BATCH 8/71 | LOSS: 0.0003990367549704388\n",
      "TRAIN: EPOCH 14/100 | BATCH 9/71 | LOSS: 0.000408638508815784\n",
      "TRAIN: EPOCH 14/100 | BATCH 10/71 | LOSS: 0.0004071210899961774\n",
      "TRAIN: EPOCH 14/100 | BATCH 11/71 | LOSS: 0.0004063475074265928\n",
      "TRAIN: EPOCH 14/100 | BATCH 12/71 | LOSS: 0.000394735155098785\n",
      "TRAIN: EPOCH 14/100 | BATCH 13/71 | LOSS: 0.0003878635959283981\n",
      "TRAIN: EPOCH 14/100 | BATCH 14/71 | LOSS: 0.0004062849242473021\n",
      "TRAIN: EPOCH 14/100 | BATCH 15/71 | LOSS: 0.0003993803247794858\n",
      "TRAIN: EPOCH 14/100 | BATCH 16/71 | LOSS: 0.0003967431899475153\n",
      "TRAIN: EPOCH 14/100 | BATCH 17/71 | LOSS: 0.0003939112122882054\n",
      "TRAIN: EPOCH 14/100 | BATCH 18/71 | LOSS: 0.0003887601127835775\n",
      "TRAIN: EPOCH 14/100 | BATCH 19/71 | LOSS: 0.00038413690417655745\n",
      "TRAIN: EPOCH 14/100 | BATCH 20/71 | LOSS: 0.00037813935376193727\n",
      "TRAIN: EPOCH 14/100 | BATCH 21/71 | LOSS: 0.00038184774497164074\n",
      "TRAIN: EPOCH 14/100 | BATCH 22/71 | LOSS: 0.000387662945127965\n",
      "TRAIN: EPOCH 14/100 | BATCH 23/71 | LOSS: 0.00038291494577909663\n",
      "TRAIN: EPOCH 14/100 | BATCH 24/71 | LOSS: 0.000379484502482228\n",
      "TRAIN: EPOCH 14/100 | BATCH 25/71 | LOSS: 0.0003874407506932934\n",
      "TRAIN: EPOCH 14/100 | BATCH 26/71 | LOSS: 0.0003861792237903164\n",
      "TRAIN: EPOCH 14/100 | BATCH 27/71 | LOSS: 0.0003863086681771425\n",
      "TRAIN: EPOCH 14/100 | BATCH 28/71 | LOSS: 0.0003855880334820791\n",
      "TRAIN: EPOCH 14/100 | BATCH 29/71 | LOSS: 0.0003866840837872587\n",
      "TRAIN: EPOCH 14/100 | BATCH 30/71 | LOSS: 0.00038300984165435956\n",
      "TRAIN: EPOCH 14/100 | BATCH 31/71 | LOSS: 0.0003848452201964392\n",
      "TRAIN: EPOCH 14/100 | BATCH 32/71 | LOSS: 0.00038762353263640154\n",
      "TRAIN: EPOCH 14/100 | BATCH 33/71 | LOSS: 0.00038519164210062143\n",
      "TRAIN: EPOCH 14/100 | BATCH 34/71 | LOSS: 0.0003818270322101723\n",
      "TRAIN: EPOCH 14/100 | BATCH 35/71 | LOSS: 0.000384015553006773\n",
      "TRAIN: EPOCH 14/100 | BATCH 36/71 | LOSS: 0.0003833855613216607\n",
      "TRAIN: EPOCH 14/100 | BATCH 37/71 | LOSS: 0.00038201829815271164\n",
      "TRAIN: EPOCH 14/100 | BATCH 38/71 | LOSS: 0.0003854928904613002\n",
      "TRAIN: EPOCH 14/100 | BATCH 39/71 | LOSS: 0.00038214675259951036\n",
      "TRAIN: EPOCH 14/100 | BATCH 40/71 | LOSS: 0.00038047245009404767\n",
      "TRAIN: EPOCH 14/100 | BATCH 41/71 | LOSS: 0.000378707062433629\n",
      "TRAIN: EPOCH 14/100 | BATCH 42/71 | LOSS: 0.00037612829412885966\n",
      "TRAIN: EPOCH 14/100 | BATCH 43/71 | LOSS: 0.0003802409036407798\n",
      "TRAIN: EPOCH 14/100 | BATCH 44/71 | LOSS: 0.00037855908449273555\n",
      "TRAIN: EPOCH 14/100 | BATCH 45/71 | LOSS: 0.0003770404130235598\n",
      "TRAIN: EPOCH 14/100 | BATCH 46/71 | LOSS: 0.00037290782641688795\n",
      "TRAIN: EPOCH 14/100 | BATCH 47/71 | LOSS: 0.0003808559931712807\n",
      "TRAIN: EPOCH 14/100 | BATCH 48/71 | LOSS: 0.0003792204349468063\n",
      "TRAIN: EPOCH 14/100 | BATCH 49/71 | LOSS: 0.0003797994399792515\n",
      "TRAIN: EPOCH 14/100 | BATCH 50/71 | LOSS: 0.0003784318296489872\n",
      "TRAIN: EPOCH 14/100 | BATCH 51/71 | LOSS: 0.0003818716781657153\n",
      "TRAIN: EPOCH 14/100 | BATCH 52/71 | LOSS: 0.00037890440512026817\n",
      "TRAIN: EPOCH 14/100 | BATCH 53/71 | LOSS: 0.00037983497626286136\n",
      "TRAIN: EPOCH 14/100 | BATCH 54/71 | LOSS: 0.00037729118802500044\n",
      "TRAIN: EPOCH 14/100 | BATCH 55/71 | LOSS: 0.00038165175543066913\n",
      "TRAIN: EPOCH 14/100 | BATCH 56/71 | LOSS: 0.0003796388520736639\n",
      "TRAIN: EPOCH 14/100 | BATCH 57/71 | LOSS: 0.00037732366763520986\n",
      "TRAIN: EPOCH 14/100 | BATCH 58/71 | LOSS: 0.0003764565231996762\n",
      "TRAIN: EPOCH 14/100 | BATCH 59/71 | LOSS: 0.0003736526314848258\n",
      "TRAIN: EPOCH 14/100 | BATCH 60/71 | LOSS: 0.00037076284838878534\n",
      "TRAIN: EPOCH 14/100 | BATCH 61/71 | LOSS: 0.00036897207352129985\n",
      "TRAIN: EPOCH 14/100 | BATCH 62/71 | LOSS: 0.0003669399496478339\n",
      "TRAIN: EPOCH 14/100 | BATCH 63/71 | LOSS: 0.000365481640528742\n",
      "TRAIN: EPOCH 14/100 | BATCH 64/71 | LOSS: 0.0003656758062648945\n",
      "TRAIN: EPOCH 14/100 | BATCH 65/71 | LOSS: 0.00036616844899430305\n",
      "TRAIN: EPOCH 14/100 | BATCH 66/71 | LOSS: 0.00036688682096732306\n",
      "TRAIN: EPOCH 14/100 | BATCH 67/71 | LOSS: 0.0003659006067798199\n",
      "TRAIN: EPOCH 14/100 | BATCH 68/71 | LOSS: 0.00036491509535403895\n",
      "TRAIN: EPOCH 14/100 | BATCH 69/71 | LOSS: 0.0003655067566017221\n",
      "TRAIN: EPOCH 14/100 | BATCH 70/71 | LOSS: 0.00036463373124232174\n",
      "VAL: EPOCH 14/100 | BATCH 0/8 | LOSS: 0.00027256784960627556\n",
      "VAL: EPOCH 14/100 | BATCH 1/8 | LOSS: 0.0002902820851886645\n",
      "VAL: EPOCH 14/100 | BATCH 2/8 | LOSS: 0.00029326788110968965\n",
      "VAL: EPOCH 14/100 | BATCH 3/8 | LOSS: 0.0003123872520518489\n",
      "VAL: EPOCH 14/100 | BATCH 4/8 | LOSS: 0.0003114191989880055\n",
      "VAL: EPOCH 14/100 | BATCH 5/8 | LOSS: 0.00030931717871377867\n",
      "VAL: EPOCH 14/100 | BATCH 6/8 | LOSS: 0.0003210681357554027\n",
      "VAL: EPOCH 14/100 | BATCH 7/8 | LOSS: 0.0003048272974410793\n",
      "TRAIN: EPOCH 15/100 | BATCH 0/71 | LOSS: 0.00023969337053131312\n",
      "TRAIN: EPOCH 15/100 | BATCH 1/71 | LOSS: 0.0002527555261622183\n",
      "TRAIN: EPOCH 15/100 | BATCH 2/71 | LOSS: 0.00028480447266095627\n",
      "TRAIN: EPOCH 15/100 | BATCH 3/71 | LOSS: 0.00029444025494740345\n",
      "TRAIN: EPOCH 15/100 | BATCH 4/71 | LOSS: 0.0002887339011067525\n",
      "TRAIN: EPOCH 15/100 | BATCH 5/71 | LOSS: 0.00028069379429022473\n",
      "TRAIN: EPOCH 15/100 | BATCH 6/71 | LOSS: 0.0002761153570775475\n",
      "TRAIN: EPOCH 15/100 | BATCH 7/71 | LOSS: 0.000277310267847497\n",
      "TRAIN: EPOCH 15/100 | BATCH 8/71 | LOSS: 0.00028152000676426623\n",
      "TRAIN: EPOCH 15/100 | BATCH 9/71 | LOSS: 0.00028678327798843385\n",
      "TRAIN: EPOCH 15/100 | BATCH 10/71 | LOSS: 0.00028929818654432893\n",
      "TRAIN: EPOCH 15/100 | BATCH 11/71 | LOSS: 0.0002864521908729027\n",
      "TRAIN: EPOCH 15/100 | BATCH 12/71 | LOSS: 0.00031357081248783146\n",
      "TRAIN: EPOCH 15/100 | BATCH 13/71 | LOSS: 0.0003072711842833087\n",
      "TRAIN: EPOCH 15/100 | BATCH 14/71 | LOSS: 0.0003074009184880803\n",
      "TRAIN: EPOCH 15/100 | BATCH 15/71 | LOSS: 0.0003149586100335\n",
      "TRAIN: EPOCH 15/100 | BATCH 16/71 | LOSS: 0.0003211584048318293\n",
      "TRAIN: EPOCH 15/100 | BATCH 17/71 | LOSS: 0.00031882130133453757\n",
      "TRAIN: EPOCH 15/100 | BATCH 18/71 | LOSS: 0.0003145447852531154\n",
      "TRAIN: EPOCH 15/100 | BATCH 19/71 | LOSS: 0.00031088308314792813\n",
      "TRAIN: EPOCH 15/100 | BATCH 20/71 | LOSS: 0.0003089207013891566\n",
      "TRAIN: EPOCH 15/100 | BATCH 21/71 | LOSS: 0.0003245494153816253\n",
      "TRAIN: EPOCH 15/100 | BATCH 22/71 | LOSS: 0.00032207603394256336\n",
      "TRAIN: EPOCH 15/100 | BATCH 23/71 | LOSS: 0.00031860197001757723\n",
      "TRAIN: EPOCH 15/100 | BATCH 24/71 | LOSS: 0.00032169747166335585\n",
      "TRAIN: EPOCH 15/100 | BATCH 25/71 | LOSS: 0.00031955029743795213\n",
      "TRAIN: EPOCH 15/100 | BATCH 26/71 | LOSS: 0.00031947485425647484\n",
      "TRAIN: EPOCH 15/100 | BATCH 27/71 | LOSS: 0.0003148137423392784\n",
      "TRAIN: EPOCH 15/100 | BATCH 28/71 | LOSS: 0.0003118136735103125\n",
      "TRAIN: EPOCH 15/100 | BATCH 29/71 | LOSS: 0.00031829847818395744\n",
      "TRAIN: EPOCH 15/100 | BATCH 30/71 | LOSS: 0.0003143068451824928\n",
      "TRAIN: EPOCH 15/100 | BATCH 31/71 | LOSS: 0.00031531234617432347\n",
      "TRAIN: EPOCH 15/100 | BATCH 32/71 | LOSS: 0.00031359635018320245\n",
      "TRAIN: EPOCH 15/100 | BATCH 33/71 | LOSS: 0.0003238859216627829\n",
      "TRAIN: EPOCH 15/100 | BATCH 34/71 | LOSS: 0.0003205084616118776\n",
      "TRAIN: EPOCH 15/100 | BATCH 35/71 | LOSS: 0.0003228758537766731\n",
      "TRAIN: EPOCH 15/100 | BATCH 36/71 | LOSS: 0.0003204264637158328\n",
      "TRAIN: EPOCH 15/100 | BATCH 37/71 | LOSS: 0.00032656801723581\n",
      "TRAIN: EPOCH 15/100 | BATCH 38/71 | LOSS: 0.00032326953451280505\n",
      "TRAIN: EPOCH 15/100 | BATCH 39/71 | LOSS: 0.00032580949227849486\n",
      "TRAIN: EPOCH 15/100 | BATCH 40/71 | LOSS: 0.0003269403032169155\n",
      "TRAIN: EPOCH 15/100 | BATCH 41/71 | LOSS: 0.000326016790668724\n",
      "TRAIN: EPOCH 15/100 | BATCH 42/71 | LOSS: 0.00032346224111808074\n",
      "TRAIN: EPOCH 15/100 | BATCH 43/71 | LOSS: 0.00032205341862705114\n",
      "TRAIN: EPOCH 15/100 | BATCH 44/71 | LOSS: 0.00032341744259206786\n",
      "TRAIN: EPOCH 15/100 | BATCH 45/71 | LOSS: 0.000323922731458088\n",
      "TRAIN: EPOCH 15/100 | BATCH 46/71 | LOSS: 0.0003212774026906118\n",
      "TRAIN: EPOCH 15/100 | BATCH 47/71 | LOSS: 0.0003197774497796975\n",
      "TRAIN: EPOCH 15/100 | BATCH 48/71 | LOSS: 0.00032020062451697506\n",
      "TRAIN: EPOCH 15/100 | BATCH 49/71 | LOSS: 0.0003218692194786854\n",
      "TRAIN: EPOCH 15/100 | BATCH 50/71 | LOSS: 0.0003226522231296472\n",
      "TRAIN: EPOCH 15/100 | BATCH 51/71 | LOSS: 0.0003248701471490606\n",
      "TRAIN: EPOCH 15/100 | BATCH 52/71 | LOSS: 0.0003260319356758253\n",
      "TRAIN: EPOCH 15/100 | BATCH 53/71 | LOSS: 0.00032539768020527573\n",
      "TRAIN: EPOCH 15/100 | BATCH 54/71 | LOSS: 0.0003274150803918019\n",
      "TRAIN: EPOCH 15/100 | BATCH 55/71 | LOSS: 0.0003280435056305058\n",
      "TRAIN: EPOCH 15/100 | BATCH 56/71 | LOSS: 0.00032556744388370006\n",
      "TRAIN: EPOCH 15/100 | BATCH 57/71 | LOSS: 0.00032815842032191697\n",
      "TRAIN: EPOCH 15/100 | BATCH 58/71 | LOSS: 0.00032845196280612655\n",
      "TRAIN: EPOCH 15/100 | BATCH 59/71 | LOSS: 0.00032664145570985664\n",
      "TRAIN: EPOCH 15/100 | BATCH 60/71 | LOSS: 0.0003241252993351826\n",
      "TRAIN: EPOCH 15/100 | BATCH 61/71 | LOSS: 0.00032299198562301876\n",
      "TRAIN: EPOCH 15/100 | BATCH 62/71 | LOSS: 0.0003282157720402918\n",
      "TRAIN: EPOCH 15/100 | BATCH 63/71 | LOSS: 0.00032746367082836514\n",
      "TRAIN: EPOCH 15/100 | BATCH 64/71 | LOSS: 0.00032937117614962447\n",
      "TRAIN: EPOCH 15/100 | BATCH 65/71 | LOSS: 0.0003281214019146071\n",
      "TRAIN: EPOCH 15/100 | BATCH 66/71 | LOSS: 0.0003279884879689302\n",
      "TRAIN: EPOCH 15/100 | BATCH 67/71 | LOSS: 0.000327465603571481\n",
      "TRAIN: EPOCH 15/100 | BATCH 68/71 | LOSS: 0.0003282358054581193\n",
      "TRAIN: EPOCH 15/100 | BATCH 69/71 | LOSS: 0.00032697276640517107\n",
      "TRAIN: EPOCH 15/100 | BATCH 70/71 | LOSS: 0.00032622869594619463\n",
      "VAL: EPOCH 15/100 | BATCH 0/8 | LOSS: 0.00024524526088498533\n",
      "VAL: EPOCH 15/100 | BATCH 1/8 | LOSS: 0.00025911944976542145\n",
      "VAL: EPOCH 15/100 | BATCH 2/8 | LOSS: 0.0002634668004854272\n",
      "VAL: EPOCH 15/100 | BATCH 3/8 | LOSS: 0.00028232363547431305\n",
      "VAL: EPOCH 15/100 | BATCH 4/8 | LOSS: 0.0002788613608572632\n",
      "VAL: EPOCH 15/100 | BATCH 5/8 | LOSS: 0.00027789987507276237\n",
      "VAL: EPOCH 15/100 | BATCH 6/8 | LOSS: 0.0002888715659667339\n",
      "VAL: EPOCH 15/100 | BATCH 7/8 | LOSS: 0.0002740608306339709\n",
      "TRAIN: EPOCH 16/100 | BATCH 0/71 | LOSS: 0.0003079838934354484\n",
      "TRAIN: EPOCH 16/100 | BATCH 1/71 | LOSS: 0.00027780234813690186\n",
      "TRAIN: EPOCH 16/100 | BATCH 2/71 | LOSS: 0.00030196904360006255\n",
      "TRAIN: EPOCH 16/100 | BATCH 3/71 | LOSS: 0.00027945027250098065\n",
      "TRAIN: EPOCH 16/100 | BATCH 4/71 | LOSS: 0.00027770833694376054\n",
      "TRAIN: EPOCH 16/100 | BATCH 5/71 | LOSS: 0.0002739145517504464\n",
      "TRAIN: EPOCH 16/100 | BATCH 6/71 | LOSS: 0.00026847875311172435\n",
      "TRAIN: EPOCH 16/100 | BATCH 7/71 | LOSS: 0.0002832387835951522\n",
      "TRAIN: EPOCH 16/100 | BATCH 8/71 | LOSS: 0.00027614207970651076\n",
      "TRAIN: EPOCH 16/100 | BATCH 9/71 | LOSS: 0.000283648977347184\n",
      "TRAIN: EPOCH 16/100 | BATCH 10/71 | LOSS: 0.0002795972408917309\n",
      "TRAIN: EPOCH 16/100 | BATCH 11/71 | LOSS: 0.000288223414827371\n",
      "TRAIN: EPOCH 16/100 | BATCH 12/71 | LOSS: 0.0002870377155396944\n",
      "TRAIN: EPOCH 16/100 | BATCH 13/71 | LOSS: 0.0002819558555659439\n",
      "TRAIN: EPOCH 16/100 | BATCH 14/71 | LOSS: 0.0002778733770052592\n",
      "TRAIN: EPOCH 16/100 | BATCH 15/71 | LOSS: 0.00027547286117624026\n",
      "TRAIN: EPOCH 16/100 | BATCH 16/71 | LOSS: 0.0002734679891385467\n",
      "TRAIN: EPOCH 16/100 | BATCH 17/71 | LOSS: 0.00027307830088021647\n",
      "TRAIN: EPOCH 16/100 | BATCH 18/71 | LOSS: 0.0002740650264681072\n",
      "TRAIN: EPOCH 16/100 | BATCH 19/71 | LOSS: 0.0002729220832407009\n",
      "TRAIN: EPOCH 16/100 | BATCH 20/71 | LOSS: 0.00027417958301070725\n",
      "TRAIN: EPOCH 16/100 | BATCH 21/71 | LOSS: 0.000273596433875024\n",
      "TRAIN: EPOCH 16/100 | BATCH 22/71 | LOSS: 0.0002795411419356242\n",
      "TRAIN: EPOCH 16/100 | BATCH 23/71 | LOSS: 0.00027788345141743775\n",
      "TRAIN: EPOCH 16/100 | BATCH 24/71 | LOSS: 0.00028274009528104214\n",
      "TRAIN: EPOCH 16/100 | BATCH 25/71 | LOSS: 0.00028835563940587093\n",
      "TRAIN: EPOCH 16/100 | BATCH 26/71 | LOSS: 0.00029431972965270417\n",
      "TRAIN: EPOCH 16/100 | BATCH 27/71 | LOSS: 0.00029309472977599526\n",
      "TRAIN: EPOCH 16/100 | BATCH 28/71 | LOSS: 0.00028947217476249124\n",
      "TRAIN: EPOCH 16/100 | BATCH 29/71 | LOSS: 0.0002935774037420439\n",
      "TRAIN: EPOCH 16/100 | BATCH 30/71 | LOSS: 0.000292823878548769\n",
      "TRAIN: EPOCH 16/100 | BATCH 31/71 | LOSS: 0.00029572567791547044\n",
      "TRAIN: EPOCH 16/100 | BATCH 32/71 | LOSS: 0.00029850586724933237\n",
      "TRAIN: EPOCH 16/100 | BATCH 33/71 | LOSS: 0.00029435892407919336\n",
      "TRAIN: EPOCH 16/100 | BATCH 34/71 | LOSS: 0.00029477833221400423\n",
      "TRAIN: EPOCH 16/100 | BATCH 35/71 | LOSS: 0.00029508792229333066\n",
      "TRAIN: EPOCH 16/100 | BATCH 36/71 | LOSS: 0.0002965035642232947\n",
      "TRAIN: EPOCH 16/100 | BATCH 37/71 | LOSS: 0.0002965792312017201\n",
      "TRAIN: EPOCH 16/100 | BATCH 38/71 | LOSS: 0.000296977835182005\n",
      "TRAIN: EPOCH 16/100 | BATCH 39/71 | LOSS: 0.0003009602922247723\n",
      "TRAIN: EPOCH 16/100 | BATCH 40/71 | LOSS: 0.0002995797563571392\n",
      "TRAIN: EPOCH 16/100 | BATCH 41/71 | LOSS: 0.0002996905102571916\n",
      "TRAIN: EPOCH 16/100 | BATCH 42/71 | LOSS: 0.00029857719701519887\n",
      "TRAIN: EPOCH 16/100 | BATCH 43/71 | LOSS: 0.00030238052369730377\n",
      "TRAIN: EPOCH 16/100 | BATCH 44/71 | LOSS: 0.0003068247952291535\n",
      "TRAIN: EPOCH 16/100 | BATCH 45/71 | LOSS: 0.0003045081656360391\n",
      "TRAIN: EPOCH 16/100 | BATCH 46/71 | LOSS: 0.0003091934755657542\n",
      "TRAIN: EPOCH 16/100 | BATCH 47/71 | LOSS: 0.00030875277631518355\n",
      "TRAIN: EPOCH 16/100 | BATCH 48/71 | LOSS: 0.0003068474972530325\n",
      "TRAIN: EPOCH 16/100 | BATCH 49/71 | LOSS: 0.00030942008394049483\n",
      "TRAIN: EPOCH 16/100 | BATCH 50/71 | LOSS: 0.00030765262422124474\n",
      "TRAIN: EPOCH 16/100 | BATCH 51/71 | LOSS: 0.00030569491774748225\n",
      "TRAIN: EPOCH 16/100 | BATCH 52/71 | LOSS: 0.0003046819268624773\n",
      "TRAIN: EPOCH 16/100 | BATCH 53/71 | LOSS: 0.00030278092647980276\n",
      "TRAIN: EPOCH 16/100 | BATCH 54/71 | LOSS: 0.0003029423800233582\n",
      "TRAIN: EPOCH 16/100 | BATCH 55/71 | LOSS: 0.0003018081016697189\n",
      "TRAIN: EPOCH 16/100 | BATCH 56/71 | LOSS: 0.0003034807818231026\n",
      "TRAIN: EPOCH 16/100 | BATCH 57/71 | LOSS: 0.0003040868057347513\n",
      "TRAIN: EPOCH 16/100 | BATCH 58/71 | LOSS: 0.0003033134611757567\n",
      "TRAIN: EPOCH 16/100 | BATCH 59/71 | LOSS: 0.00030189020229348293\n",
      "TRAIN: EPOCH 16/100 | BATCH 60/71 | LOSS: 0.0003008715243968105\n",
      "TRAIN: EPOCH 16/100 | BATCH 61/71 | LOSS: 0.00029915031078534443\n",
      "TRAIN: EPOCH 16/100 | BATCH 62/71 | LOSS: 0.00029933876390310214\n",
      "TRAIN: EPOCH 16/100 | BATCH 63/71 | LOSS: 0.0002974828737478674\n",
      "TRAIN: EPOCH 16/100 | BATCH 64/71 | LOSS: 0.00029661969769781887\n",
      "TRAIN: EPOCH 16/100 | BATCH 65/71 | LOSS: 0.0002954024642928165\n",
      "TRAIN: EPOCH 16/100 | BATCH 66/71 | LOSS: 0.000296658156708981\n",
      "TRAIN: EPOCH 16/100 | BATCH 67/71 | LOSS: 0.000294950114467236\n",
      "TRAIN: EPOCH 16/100 | BATCH 68/71 | LOSS: 0.0002954577245965492\n",
      "TRAIN: EPOCH 16/100 | BATCH 69/71 | LOSS: 0.000294744035428656\n",
      "TRAIN: EPOCH 16/100 | BATCH 70/71 | LOSS: 0.0002942468280720354\n",
      "VAL: EPOCH 16/100 | BATCH 0/8 | LOSS: 0.0002281788329128176\n",
      "VAL: EPOCH 16/100 | BATCH 1/8 | LOSS: 0.00023736344883218408\n",
      "VAL: EPOCH 16/100 | BATCH 2/8 | LOSS: 0.00023943608781943718\n",
      "VAL: EPOCH 16/100 | BATCH 3/8 | LOSS: 0.0002570542274042964\n",
      "VAL: EPOCH 16/100 | BATCH 4/8 | LOSS: 0.0002521571528632194\n",
      "VAL: EPOCH 16/100 | BATCH 5/8 | LOSS: 0.00024943398117708665\n",
      "VAL: EPOCH 16/100 | BATCH 6/8 | LOSS: 0.00026010050870744247\n",
      "VAL: EPOCH 16/100 | BATCH 7/8 | LOSS: 0.0002464032641000813\n",
      "TRAIN: EPOCH 17/100 | BATCH 0/71 | LOSS: 0.0002993039961438626\n",
      "TRAIN: EPOCH 17/100 | BATCH 1/71 | LOSS: 0.00028749236662406474\n",
      "TRAIN: EPOCH 17/100 | BATCH 2/71 | LOSS: 0.0002591608645161614\n",
      "TRAIN: EPOCH 17/100 | BATCH 3/71 | LOSS: 0.0002655767784744967\n",
      "TRAIN: EPOCH 17/100 | BATCH 4/71 | LOSS: 0.00031285839213524014\n",
      "TRAIN: EPOCH 17/100 | BATCH 5/71 | LOSS: 0.0003183474060885298\n",
      "TRAIN: EPOCH 17/100 | BATCH 6/71 | LOSS: 0.00034310132988529015\n",
      "TRAIN: EPOCH 17/100 | BATCH 7/71 | LOSS: 0.00032702913267712574\n",
      "TRAIN: EPOCH 17/100 | BATCH 8/71 | LOSS: 0.00031666449568648305\n",
      "TRAIN: EPOCH 17/100 | BATCH 9/71 | LOSS: 0.0003099634050158784\n",
      "TRAIN: EPOCH 17/100 | BATCH 10/71 | LOSS: 0.00030379210313020104\n",
      "TRAIN: EPOCH 17/100 | BATCH 11/71 | LOSS: 0.0002951597210388475\n",
      "TRAIN: EPOCH 17/100 | BATCH 12/71 | LOSS: 0.0002872692533016491\n",
      "TRAIN: EPOCH 17/100 | BATCH 13/71 | LOSS: 0.0002783593185345775\n",
      "TRAIN: EPOCH 17/100 | BATCH 14/71 | LOSS: 0.00027494994186175366\n",
      "TRAIN: EPOCH 17/100 | BATCH 15/71 | LOSS: 0.0002743038930930197\n",
      "TRAIN: EPOCH 17/100 | BATCH 16/71 | LOSS: 0.00026725134760935735\n",
      "TRAIN: EPOCH 17/100 | BATCH 17/71 | LOSS: 0.00027000204540349334\n",
      "TRAIN: EPOCH 17/100 | BATCH 18/71 | LOSS: 0.00027112388373720214\n",
      "TRAIN: EPOCH 17/100 | BATCH 19/71 | LOSS: 0.00027111103336210365\n",
      "TRAIN: EPOCH 17/100 | BATCH 20/71 | LOSS: 0.00026877931460538614\n",
      "TRAIN: EPOCH 17/100 | BATCH 21/71 | LOSS: 0.0002682443003074943\n",
      "TRAIN: EPOCH 17/100 | BATCH 22/71 | LOSS: 0.00026526577826386887\n",
      "TRAIN: EPOCH 17/100 | BATCH 23/71 | LOSS: 0.000270739060700483\n",
      "TRAIN: EPOCH 17/100 | BATCH 24/71 | LOSS: 0.0002669163863174617\n",
      "TRAIN: EPOCH 17/100 | BATCH 25/71 | LOSS: 0.000265883009818096\n",
      "TRAIN: EPOCH 17/100 | BATCH 26/71 | LOSS: 0.00026614236735945777\n",
      "TRAIN: EPOCH 17/100 | BATCH 27/71 | LOSS: 0.00026352598173876425\n",
      "TRAIN: EPOCH 17/100 | BATCH 28/71 | LOSS: 0.0002624168440073343\n",
      "TRAIN: EPOCH 17/100 | BATCH 29/71 | LOSS: 0.00026674468244891614\n",
      "TRAIN: EPOCH 17/100 | BATCH 30/71 | LOSS: 0.0002631421441272382\n",
      "TRAIN: EPOCH 17/100 | BATCH 31/71 | LOSS: 0.0002644562009663787\n",
      "TRAIN: EPOCH 17/100 | BATCH 32/71 | LOSS: 0.0002642174844037403\n",
      "TRAIN: EPOCH 17/100 | BATCH 33/71 | LOSS: 0.0002621274821542423\n",
      "TRAIN: EPOCH 17/100 | BATCH 34/71 | LOSS: 0.00026579050463624296\n",
      "TRAIN: EPOCH 17/100 | BATCH 35/71 | LOSS: 0.0002681348438879165\n",
      "TRAIN: EPOCH 17/100 | BATCH 36/71 | LOSS: 0.0002673460403457284\n",
      "TRAIN: EPOCH 17/100 | BATCH 37/71 | LOSS: 0.0002695285503777038\n",
      "TRAIN: EPOCH 17/100 | BATCH 38/71 | LOSS: 0.0002713922429113434\n",
      "TRAIN: EPOCH 17/100 | BATCH 39/71 | LOSS: 0.0002728077379288152\n",
      "TRAIN: EPOCH 17/100 | BATCH 40/71 | LOSS: 0.000270754238732568\n",
      "TRAIN: EPOCH 17/100 | BATCH 41/71 | LOSS: 0.0002702631311313737\n",
      "TRAIN: EPOCH 17/100 | BATCH 42/71 | LOSS: 0.0002686347660803518\n",
      "TRAIN: EPOCH 17/100 | BATCH 43/71 | LOSS: 0.0002677173767799766\n",
      "TRAIN: EPOCH 17/100 | BATCH 44/71 | LOSS: 0.00027128494564547306\n",
      "TRAIN: EPOCH 17/100 | BATCH 45/71 | LOSS: 0.00027261407155057657\n",
      "TRAIN: EPOCH 17/100 | BATCH 46/71 | LOSS: 0.00027250602068577676\n",
      "TRAIN: EPOCH 17/100 | BATCH 47/71 | LOSS: 0.00027280884023639373\n",
      "TRAIN: EPOCH 17/100 | BATCH 48/71 | LOSS: 0.0002713373524246129\n",
      "TRAIN: EPOCH 17/100 | BATCH 49/71 | LOSS: 0.0002710668006329797\n",
      "TRAIN: EPOCH 17/100 | BATCH 50/71 | LOSS: 0.00026994526150006363\n",
      "TRAIN: EPOCH 17/100 | BATCH 51/71 | LOSS: 0.00026975035563317273\n",
      "TRAIN: EPOCH 17/100 | BATCH 52/71 | LOSS: 0.00027066617759322433\n",
      "TRAIN: EPOCH 17/100 | BATCH 53/71 | LOSS: 0.00026910878201575814\n",
      "TRAIN: EPOCH 17/100 | BATCH 54/71 | LOSS: 0.000271698147513565\n",
      "TRAIN: EPOCH 17/100 | BATCH 55/71 | LOSS: 0.0002709661163992548\n",
      "TRAIN: EPOCH 17/100 | BATCH 56/71 | LOSS: 0.00027362325483026276\n",
      "TRAIN: EPOCH 17/100 | BATCH 57/71 | LOSS: 0.00027509172048604375\n",
      "TRAIN: EPOCH 17/100 | BATCH 58/71 | LOSS: 0.0002757452950730004\n",
      "TRAIN: EPOCH 17/100 | BATCH 59/71 | LOSS: 0.00027449624855459357\n",
      "TRAIN: EPOCH 17/100 | BATCH 60/71 | LOSS: 0.00027356484759843253\n",
      "TRAIN: EPOCH 17/100 | BATCH 61/71 | LOSS: 0.00027549191708526303\n",
      "TRAIN: EPOCH 17/100 | BATCH 62/71 | LOSS: 0.00027374722689966715\n",
      "TRAIN: EPOCH 17/100 | BATCH 63/71 | LOSS: 0.0002722477843235538\n",
      "TRAIN: EPOCH 17/100 | BATCH 64/71 | LOSS: 0.0002705911469932359\n",
      "TRAIN: EPOCH 17/100 | BATCH 65/71 | LOSS: 0.0002721999674880256\n",
      "TRAIN: EPOCH 17/100 | BATCH 66/71 | LOSS: 0.0002714441363776186\n",
      "TRAIN: EPOCH 17/100 | BATCH 67/71 | LOSS: 0.0002696750959919026\n",
      "TRAIN: EPOCH 17/100 | BATCH 68/71 | LOSS: 0.00026892630136200643\n",
      "TRAIN: EPOCH 17/100 | BATCH 69/71 | LOSS: 0.00026858334791281127\n",
      "TRAIN: EPOCH 17/100 | BATCH 70/71 | LOSS: 0.00026695047973320673\n",
      "VAL: EPOCH 17/100 | BATCH 0/8 | LOSS: 0.00021237178589217365\n",
      "VAL: EPOCH 17/100 | BATCH 1/8 | LOSS: 0.00021631622075801715\n",
      "VAL: EPOCH 17/100 | BATCH 2/8 | LOSS: 0.00021635258356885365\n",
      "VAL: EPOCH 17/100 | BATCH 3/8 | LOSS: 0.00023334666548180394\n",
      "VAL: EPOCH 17/100 | BATCH 4/8 | LOSS: 0.00022760004794690757\n",
      "VAL: EPOCH 17/100 | BATCH 5/8 | LOSS: 0.00022535855290091908\n",
      "VAL: EPOCH 17/100 | BATCH 6/8 | LOSS: 0.00023458053848506616\n",
      "VAL: EPOCH 17/100 | BATCH 7/8 | LOSS: 0.00022261120830080472\n",
      "TRAIN: EPOCH 18/100 | BATCH 0/71 | LOSS: 0.0002866385621018708\n",
      "TRAIN: EPOCH 18/100 | BATCH 1/71 | LOSS: 0.00021610414842143655\n",
      "TRAIN: EPOCH 18/100 | BATCH 2/71 | LOSS: 0.000267138685255001\n",
      "TRAIN: EPOCH 18/100 | BATCH 3/71 | LOSS: 0.0002610244664538186\n",
      "TRAIN: EPOCH 18/100 | BATCH 4/71 | LOSS: 0.00023935509379953147\n",
      "TRAIN: EPOCH 18/100 | BATCH 5/71 | LOSS: 0.0002262603423635786\n",
      "TRAIN: EPOCH 18/100 | BATCH 6/71 | LOSS: 0.0002786212176683226\n",
      "TRAIN: EPOCH 18/100 | BATCH 7/71 | LOSS: 0.00026989358775608707\n",
      "TRAIN: EPOCH 18/100 | BATCH 8/71 | LOSS: 0.00026978967394421086\n",
      "TRAIN: EPOCH 18/100 | BATCH 9/71 | LOSS: 0.0002548957956605591\n",
      "TRAIN: EPOCH 18/100 | BATCH 10/71 | LOSS: 0.00024864585033025253\n",
      "TRAIN: EPOCH 18/100 | BATCH 11/71 | LOSS: 0.0002463438577251509\n",
      "TRAIN: EPOCH 18/100 | BATCH 12/71 | LOSS: 0.00025159983376327617\n",
      "TRAIN: EPOCH 18/100 | BATCH 13/71 | LOSS: 0.0002562372329910951\n",
      "TRAIN: EPOCH 18/100 | BATCH 14/71 | LOSS: 0.0002519220734635989\n",
      "TRAIN: EPOCH 18/100 | BATCH 15/71 | LOSS: 0.00024465282876917627\n",
      "TRAIN: EPOCH 18/100 | BATCH 16/71 | LOSS: 0.0002475871194345767\n",
      "TRAIN: EPOCH 18/100 | BATCH 17/71 | LOSS: 0.00024843668506946415\n",
      "TRAIN: EPOCH 18/100 | BATCH 18/71 | LOSS: 0.0002468443938261388\n",
      "TRAIN: EPOCH 18/100 | BATCH 19/71 | LOSS: 0.0002506706223357469\n",
      "TRAIN: EPOCH 18/100 | BATCH 20/71 | LOSS: 0.0002476229364263071\n",
      "TRAIN: EPOCH 18/100 | BATCH 21/71 | LOSS: 0.0002476606611956165\n",
      "TRAIN: EPOCH 18/100 | BATCH 22/71 | LOSS: 0.00025486037243947226\n",
      "TRAIN: EPOCH 18/100 | BATCH 23/71 | LOSS: 0.00025272998997631174\n",
      "TRAIN: EPOCH 18/100 | BATCH 24/71 | LOSS: 0.00025929506751708685\n",
      "TRAIN: EPOCH 18/100 | BATCH 25/71 | LOSS: 0.0002564730283875878\n",
      "TRAIN: EPOCH 18/100 | BATCH 26/71 | LOSS: 0.0002543702053285583\n",
      "TRAIN: EPOCH 18/100 | BATCH 27/71 | LOSS: 0.0002551934573732849\n",
      "TRAIN: EPOCH 18/100 | BATCH 28/71 | LOSS: 0.0002521425831846186\n",
      "TRAIN: EPOCH 18/100 | BATCH 29/71 | LOSS: 0.00024956557948219903\n",
      "TRAIN: EPOCH 18/100 | BATCH 30/71 | LOSS: 0.0002500500972194958\n",
      "TRAIN: EPOCH 18/100 | BATCH 31/71 | LOSS: 0.000247223417773057\n",
      "TRAIN: EPOCH 18/100 | BATCH 32/71 | LOSS: 0.00025057810169969207\n",
      "TRAIN: EPOCH 18/100 | BATCH 33/71 | LOSS: 0.00024933903606485245\n",
      "TRAIN: EPOCH 18/100 | BATCH 34/71 | LOSS: 0.0002501630486221984\n",
      "TRAIN: EPOCH 18/100 | BATCH 35/71 | LOSS: 0.0002486108698778682\n",
      "TRAIN: EPOCH 18/100 | BATCH 36/71 | LOSS: 0.0002462251085907573\n",
      "TRAIN: EPOCH 18/100 | BATCH 37/71 | LOSS: 0.00024493389546956966\n",
      "TRAIN: EPOCH 18/100 | BATCH 38/71 | LOSS: 0.00024407832675840324\n",
      "TRAIN: EPOCH 18/100 | BATCH 39/71 | LOSS: 0.00024275798896269407\n",
      "TRAIN: EPOCH 18/100 | BATCH 40/71 | LOSS: 0.00024217050543861327\n",
      "TRAIN: EPOCH 18/100 | BATCH 41/71 | LOSS: 0.0002422036417382991\n",
      "TRAIN: EPOCH 18/100 | BATCH 42/71 | LOSS: 0.0002425513629595814\n",
      "TRAIN: EPOCH 18/100 | BATCH 43/71 | LOSS: 0.00024328757453159514\n",
      "TRAIN: EPOCH 18/100 | BATCH 44/71 | LOSS: 0.00024240504782129492\n",
      "TRAIN: EPOCH 18/100 | BATCH 45/71 | LOSS: 0.00024206345488378048\n",
      "TRAIN: EPOCH 18/100 | BATCH 46/71 | LOSS: 0.00024201965518048073\n",
      "TRAIN: EPOCH 18/100 | BATCH 47/71 | LOSS: 0.0002445203860285498\n",
      "TRAIN: EPOCH 18/100 | BATCH 48/71 | LOSS: 0.0002479774497891301\n",
      "TRAIN: EPOCH 18/100 | BATCH 49/71 | LOSS: 0.0002468471901374869\n",
      "TRAIN: EPOCH 18/100 | BATCH 50/71 | LOSS: 0.0002509757232117266\n",
      "TRAIN: EPOCH 18/100 | BATCH 51/71 | LOSS: 0.00025132649814342865\n",
      "TRAIN: EPOCH 18/100 | BATCH 52/71 | LOSS: 0.000250501000818814\n",
      "TRAIN: EPOCH 18/100 | BATCH 53/71 | LOSS: 0.0002491351204096443\n",
      "TRAIN: EPOCH 18/100 | BATCH 54/71 | LOSS: 0.0002475093192928894\n",
      "TRAIN: EPOCH 18/100 | BATCH 55/71 | LOSS: 0.0002468539109291409\n",
      "TRAIN: EPOCH 18/100 | BATCH 56/71 | LOSS: 0.0002469993086650216\n",
      "TRAIN: EPOCH 18/100 | BATCH 57/71 | LOSS: 0.00024691618128944636\n",
      "TRAIN: EPOCH 18/100 | BATCH 58/71 | LOSS: 0.0002461226010335988\n",
      "TRAIN: EPOCH 18/100 | BATCH 59/71 | LOSS: 0.00025034876331725777\n",
      "TRAIN: EPOCH 18/100 | BATCH 60/71 | LOSS: 0.00024950178387788595\n",
      "TRAIN: EPOCH 18/100 | BATCH 61/71 | LOSS: 0.000247664408356462\n",
      "TRAIN: EPOCH 18/100 | BATCH 62/71 | LOSS: 0.0002467977903997673\n",
      "TRAIN: EPOCH 18/100 | BATCH 63/71 | LOSS: 0.0002469433234182361\n",
      "TRAIN: EPOCH 18/100 | BATCH 64/71 | LOSS: 0.0002471815472325453\n",
      "TRAIN: EPOCH 18/100 | BATCH 65/71 | LOSS: 0.0002464855228247347\n",
      "TRAIN: EPOCH 18/100 | BATCH 66/71 | LOSS: 0.0002461439876038748\n",
      "TRAIN: EPOCH 18/100 | BATCH 67/71 | LOSS: 0.0002454627717858838\n",
      "TRAIN: EPOCH 18/100 | BATCH 68/71 | LOSS: 0.0002453047425766219\n",
      "TRAIN: EPOCH 18/100 | BATCH 69/71 | LOSS: 0.0002447488977590443\n",
      "TRAIN: EPOCH 18/100 | BATCH 70/71 | LOSS: 0.00024318496321990403\n",
      "VAL: EPOCH 18/100 | BATCH 0/8 | LOSS: 0.00019126922416035086\n",
      "VAL: EPOCH 18/100 | BATCH 1/8 | LOSS: 0.00019465442892396823\n",
      "VAL: EPOCH 18/100 | BATCH 2/8 | LOSS: 0.00019556139401781061\n",
      "VAL: EPOCH 18/100 | BATCH 3/8 | LOSS: 0.00021174777066335082\n",
      "VAL: EPOCH 18/100 | BATCH 4/8 | LOSS: 0.00020607349579222501\n",
      "VAL: EPOCH 18/100 | BATCH 5/8 | LOSS: 0.00020390051940921694\n",
      "VAL: EPOCH 18/100 | BATCH 6/8 | LOSS: 0.00021249909020428146\n",
      "VAL: EPOCH 18/100 | BATCH 7/8 | LOSS: 0.00020161484644631855\n",
      "TRAIN: EPOCH 19/100 | BATCH 0/71 | LOSS: 0.0002605448826216161\n",
      "TRAIN: EPOCH 19/100 | BATCH 1/71 | LOSS: 0.0002745541278272867\n",
      "TRAIN: EPOCH 19/100 | BATCH 2/71 | LOSS: 0.00025829039320039254\n",
      "TRAIN: EPOCH 19/100 | BATCH 3/71 | LOSS: 0.0002503092400729656\n",
      "TRAIN: EPOCH 19/100 | BATCH 4/71 | LOSS: 0.00023619559360668063\n",
      "TRAIN: EPOCH 19/100 | BATCH 5/71 | LOSS: 0.00026842677713527036\n",
      "TRAIN: EPOCH 19/100 | BATCH 6/71 | LOSS: 0.0002728382283489087\n",
      "TRAIN: EPOCH 19/100 | BATCH 7/71 | LOSS: 0.0002631266343087191\n",
      "TRAIN: EPOCH 19/100 | BATCH 8/71 | LOSS: 0.00025495502470423363\n",
      "TRAIN: EPOCH 19/100 | BATCH 9/71 | LOSS: 0.0002444331490551122\n",
      "TRAIN: EPOCH 19/100 | BATCH 10/71 | LOSS: 0.00023873952952933243\n",
      "TRAIN: EPOCH 19/100 | BATCH 11/71 | LOSS: 0.00023840855040665096\n",
      "TRAIN: EPOCH 19/100 | BATCH 12/71 | LOSS: 0.00023698414196797574\n",
      "TRAIN: EPOCH 19/100 | BATCH 13/71 | LOSS: 0.00023230105372411863\n",
      "TRAIN: EPOCH 19/100 | BATCH 14/71 | LOSS: 0.0002316223515663296\n",
      "TRAIN: EPOCH 19/100 | BATCH 15/71 | LOSS: 0.00023222661275212886\n",
      "TRAIN: EPOCH 19/100 | BATCH 16/71 | LOSS: 0.00024305331873406162\n",
      "TRAIN: EPOCH 19/100 | BATCH 17/71 | LOSS: 0.00023909244676663852\n",
      "TRAIN: EPOCH 19/100 | BATCH 18/71 | LOSS: 0.00024077475705110518\n",
      "TRAIN: EPOCH 19/100 | BATCH 19/71 | LOSS: 0.00024343804907402954\n",
      "TRAIN: EPOCH 19/100 | BATCH 20/71 | LOSS: 0.0002427112624337453\n",
      "TRAIN: EPOCH 19/100 | BATCH 21/71 | LOSS: 0.0002398968321027827\n",
      "TRAIN: EPOCH 19/100 | BATCH 22/71 | LOSS: 0.00023987429765680724\n",
      "TRAIN: EPOCH 19/100 | BATCH 23/71 | LOSS: 0.00024148780903487932\n",
      "TRAIN: EPOCH 19/100 | BATCH 24/71 | LOSS: 0.0002381212665932253\n",
      "TRAIN: EPOCH 19/100 | BATCH 25/71 | LOSS: 0.00023732165741071536\n",
      "TRAIN: EPOCH 19/100 | BATCH 26/71 | LOSS: 0.00023575820604390983\n",
      "TRAIN: EPOCH 19/100 | BATCH 27/71 | LOSS: 0.00023723912714298682\n",
      "TRAIN: EPOCH 19/100 | BATCH 28/71 | LOSS: 0.00023486584089776693\n",
      "TRAIN: EPOCH 19/100 | BATCH 29/71 | LOSS: 0.00023431116569554432\n",
      "TRAIN: EPOCH 19/100 | BATCH 30/71 | LOSS: 0.00023592142115962963\n",
      "TRAIN: EPOCH 19/100 | BATCH 31/71 | LOSS: 0.0002344363642805547\n",
      "TRAIN: EPOCH 19/100 | BATCH 32/71 | LOSS: 0.00023558978511683756\n",
      "TRAIN: EPOCH 19/100 | BATCH 33/71 | LOSS: 0.0002332590944099459\n",
      "TRAIN: EPOCH 19/100 | BATCH 34/71 | LOSS: 0.0002308977780298197\n",
      "TRAIN: EPOCH 19/100 | BATCH 35/71 | LOSS: 0.00022875755732659146\n",
      "TRAIN: EPOCH 19/100 | BATCH 36/71 | LOSS: 0.00022723538780622689\n",
      "TRAIN: EPOCH 19/100 | BATCH 37/71 | LOSS: 0.00023041492118193818\n",
      "TRAIN: EPOCH 19/100 | BATCH 38/71 | LOSS: 0.00023009082621847017\n",
      "TRAIN: EPOCH 19/100 | BATCH 39/71 | LOSS: 0.000228737809084123\n",
      "TRAIN: EPOCH 19/100 | BATCH 40/71 | LOSS: 0.00022703867402813603\n",
      "TRAIN: EPOCH 19/100 | BATCH 41/71 | LOSS: 0.00022728239994085882\n",
      "TRAIN: EPOCH 19/100 | BATCH 42/71 | LOSS: 0.00022616631285081682\n",
      "TRAIN: EPOCH 19/100 | BATCH 43/71 | LOSS: 0.00022942622606272133\n",
      "TRAIN: EPOCH 19/100 | BATCH 44/71 | LOSS: 0.00023112811516815175\n",
      "TRAIN: EPOCH 19/100 | BATCH 45/71 | LOSS: 0.00022819968644003183\n",
      "TRAIN: EPOCH 19/100 | BATCH 46/71 | LOSS: 0.0002291505683233288\n",
      "TRAIN: EPOCH 19/100 | BATCH 47/71 | LOSS: 0.00022748724404664245\n",
      "TRAIN: EPOCH 19/100 | BATCH 48/71 | LOSS: 0.00022643737403476344\n",
      "TRAIN: EPOCH 19/100 | BATCH 49/71 | LOSS: 0.00022588676132727413\n",
      "TRAIN: EPOCH 19/100 | BATCH 50/71 | LOSS: 0.00022427184711548261\n",
      "TRAIN: EPOCH 19/100 | BATCH 51/71 | LOSS: 0.00022418228064242823\n",
      "TRAIN: EPOCH 19/100 | BATCH 52/71 | LOSS: 0.00022487614862840482\n",
      "TRAIN: EPOCH 19/100 | BATCH 53/71 | LOSS: 0.00022572328641588143\n",
      "TRAIN: EPOCH 19/100 | BATCH 54/71 | LOSS: 0.0002249974088044837\n",
      "TRAIN: EPOCH 19/100 | BATCH 55/71 | LOSS: 0.00022395283122023102\n",
      "TRAIN: EPOCH 19/100 | BATCH 56/71 | LOSS: 0.00022355540912892473\n",
      "TRAIN: EPOCH 19/100 | BATCH 57/71 | LOSS: 0.0002274699082622176\n",
      "TRAIN: EPOCH 19/100 | BATCH 58/71 | LOSS: 0.00022713476012587168\n",
      "TRAIN: EPOCH 19/100 | BATCH 59/71 | LOSS: 0.00022538377185507368\n",
      "TRAIN: EPOCH 19/100 | BATCH 60/71 | LOSS: 0.00022598101373868764\n",
      "TRAIN: EPOCH 19/100 | BATCH 61/71 | LOSS: 0.00022645144406030135\n",
      "TRAIN: EPOCH 19/100 | BATCH 62/71 | LOSS: 0.00022832268529335067\n",
      "TRAIN: EPOCH 19/100 | BATCH 63/71 | LOSS: 0.00022732719276064017\n",
      "TRAIN: EPOCH 19/100 | BATCH 64/71 | LOSS: 0.0002273193215772223\n",
      "TRAIN: EPOCH 19/100 | BATCH 65/71 | LOSS: 0.0002280024182806356\n",
      "TRAIN: EPOCH 19/100 | BATCH 66/71 | LOSS: 0.00022693099401919033\n",
      "TRAIN: EPOCH 19/100 | BATCH 67/71 | LOSS: 0.00022578131350147647\n",
      "TRAIN: EPOCH 19/100 | BATCH 68/71 | LOSS: 0.0002245424147459098\n",
      "TRAIN: EPOCH 19/100 | BATCH 69/71 | LOSS: 0.00022373268937891615\n",
      "TRAIN: EPOCH 19/100 | BATCH 70/71 | LOSS: 0.0002229711099062115\n",
      "VAL: EPOCH 19/100 | BATCH 0/8 | LOSS: 0.00018719042418524623\n",
      "VAL: EPOCH 19/100 | BATCH 1/8 | LOSS: 0.00018418842955725268\n",
      "VAL: EPOCH 19/100 | BATCH 2/8 | LOSS: 0.00018404677878910056\n",
      "VAL: EPOCH 19/100 | BATCH 3/8 | LOSS: 0.00019716777387657203\n",
      "VAL: EPOCH 19/100 | BATCH 4/8 | LOSS: 0.0001913357205921784\n",
      "VAL: EPOCH 19/100 | BATCH 5/8 | LOSS: 0.00018873691684954488\n",
      "VAL: EPOCH 19/100 | BATCH 6/8 | LOSS: 0.00019673160984114344\n",
      "VAL: EPOCH 19/100 | BATCH 7/8 | LOSS: 0.00018680105949897552\n",
      "TRAIN: EPOCH 20/100 | BATCH 0/71 | LOSS: 0.00016619703092146665\n",
      "TRAIN: EPOCH 20/100 | BATCH 1/71 | LOSS: 0.00018263057427247986\n",
      "TRAIN: EPOCH 20/100 | BATCH 2/71 | LOSS: 0.00018919226810491333\n",
      "TRAIN: EPOCH 20/100 | BATCH 3/71 | LOSS: 0.0001876328606158495\n",
      "TRAIN: EPOCH 20/100 | BATCH 4/71 | LOSS: 0.0001776776713086292\n",
      "TRAIN: EPOCH 20/100 | BATCH 5/71 | LOSS: 0.00019576952763600275\n",
      "TRAIN: EPOCH 20/100 | BATCH 6/71 | LOSS: 0.00020046265230381062\n",
      "TRAIN: EPOCH 20/100 | BATCH 7/71 | LOSS: 0.00020130596021772362\n",
      "TRAIN: EPOCH 20/100 | BATCH 8/71 | LOSS: 0.00020020785935533544\n",
      "TRAIN: EPOCH 20/100 | BATCH 9/71 | LOSS: 0.00020108310127398\n",
      "TRAIN: EPOCH 20/100 | BATCH 10/71 | LOSS: 0.00020148237606256524\n",
      "TRAIN: EPOCH 20/100 | BATCH 11/71 | LOSS: 0.00019654963155820346\n",
      "TRAIN: EPOCH 20/100 | BATCH 12/71 | LOSS: 0.00020160567454205683\n",
      "TRAIN: EPOCH 20/100 | BATCH 13/71 | LOSS: 0.00019827919459203258\n",
      "TRAIN: EPOCH 20/100 | BATCH 14/71 | LOSS: 0.0002024261038362359\n",
      "TRAIN: EPOCH 20/100 | BATCH 15/71 | LOSS: 0.0002040882573055569\n",
      "TRAIN: EPOCH 20/100 | BATCH 16/71 | LOSS: 0.00019928531871889443\n",
      "TRAIN: EPOCH 20/100 | BATCH 17/71 | LOSS: 0.00019536026229616255\n",
      "TRAIN: EPOCH 20/100 | BATCH 18/71 | LOSS: 0.00019737196884019986\n",
      "TRAIN: EPOCH 20/100 | BATCH 19/71 | LOSS: 0.0001950466881680768\n",
      "TRAIN: EPOCH 20/100 | BATCH 20/71 | LOSS: 0.00019488405294915928\n",
      "TRAIN: EPOCH 20/100 | BATCH 21/71 | LOSS: 0.00019363916296490723\n",
      "TRAIN: EPOCH 20/100 | BATCH 22/71 | LOSS: 0.00019676448415418196\n",
      "TRAIN: EPOCH 20/100 | BATCH 23/71 | LOSS: 0.00019706114532406596\n",
      "TRAIN: EPOCH 20/100 | BATCH 24/71 | LOSS: 0.00019781974784564228\n",
      "TRAIN: EPOCH 20/100 | BATCH 25/71 | LOSS: 0.00020320852602778288\n",
      "TRAIN: EPOCH 20/100 | BATCH 26/71 | LOSS: 0.00020909554991198497\n",
      "TRAIN: EPOCH 20/100 | BATCH 27/71 | LOSS: 0.0002088645238213108\n",
      "TRAIN: EPOCH 20/100 | BATCH 28/71 | LOSS: 0.0002146935891279758\n",
      "TRAIN: EPOCH 20/100 | BATCH 29/71 | LOSS: 0.00021355830831453203\n",
      "TRAIN: EPOCH 20/100 | BATCH 30/71 | LOSS: 0.0002129574000249587\n",
      "TRAIN: EPOCH 20/100 | BATCH 31/71 | LOSS: 0.0002120429790011258\n",
      "TRAIN: EPOCH 20/100 | BATCH 32/71 | LOSS: 0.00021488963940293726\n",
      "TRAIN: EPOCH 20/100 | BATCH 33/71 | LOSS: 0.00021701920965878184\n",
      "TRAIN: EPOCH 20/100 | BATCH 34/71 | LOSS: 0.0002155131215528984\n",
      "TRAIN: EPOCH 20/100 | BATCH 35/71 | LOSS: 0.0002132962314741841\n",
      "TRAIN: EPOCH 20/100 | BATCH 36/71 | LOSS: 0.0002106078658520395\n",
      "TRAIN: EPOCH 20/100 | BATCH 37/71 | LOSS: 0.00020866572269465235\n",
      "TRAIN: EPOCH 20/100 | BATCH 38/71 | LOSS: 0.00020997279837805158\n",
      "TRAIN: EPOCH 20/100 | BATCH 39/71 | LOSS: 0.00021019163159508025\n",
      "TRAIN: EPOCH 20/100 | BATCH 40/71 | LOSS: 0.0002093493639244471\n",
      "TRAIN: EPOCH 20/100 | BATCH 41/71 | LOSS: 0.0002126065403336681\n",
      "TRAIN: EPOCH 20/100 | BATCH 42/71 | LOSS: 0.00021211126819929802\n",
      "TRAIN: EPOCH 20/100 | BATCH 43/71 | LOSS: 0.00020981257098769262\n",
      "TRAIN: EPOCH 20/100 | BATCH 44/71 | LOSS: 0.0002113148188477175\n",
      "TRAIN: EPOCH 20/100 | BATCH 45/71 | LOSS: 0.00021021930138965178\n",
      "TRAIN: EPOCH 20/100 | BATCH 46/71 | LOSS: 0.0002101498270194304\n",
      "TRAIN: EPOCH 20/100 | BATCH 47/71 | LOSS: 0.00020972081953611146\n",
      "TRAIN: EPOCH 20/100 | BATCH 48/71 | LOSS: 0.00020796338919483657\n",
      "TRAIN: EPOCH 20/100 | BATCH 49/71 | LOSS: 0.00020766125613590702\n",
      "TRAIN: EPOCH 20/100 | BATCH 50/71 | LOSS: 0.00020796304035648777\n",
      "TRAIN: EPOCH 20/100 | BATCH 51/71 | LOSS: 0.00020768280955962837\n",
      "TRAIN: EPOCH 20/100 | BATCH 52/71 | LOSS: 0.0002088776881859269\n",
      "TRAIN: EPOCH 20/100 | BATCH 53/71 | LOSS: 0.00020736101928546473\n",
      "TRAIN: EPOCH 20/100 | BATCH 54/71 | LOSS: 0.00020644645079631696\n",
      "TRAIN: EPOCH 20/100 | BATCH 55/71 | LOSS: 0.00020603564553312026\n",
      "TRAIN: EPOCH 20/100 | BATCH 56/71 | LOSS: 0.00020405838088328508\n",
      "TRAIN: EPOCH 20/100 | BATCH 57/71 | LOSS: 0.00020314843804326616\n",
      "TRAIN: EPOCH 20/100 | BATCH 58/71 | LOSS: 0.00020261348735233295\n",
      "TRAIN: EPOCH 20/100 | BATCH 59/71 | LOSS: 0.00020517222219496034\n",
      "TRAIN: EPOCH 20/100 | BATCH 60/71 | LOSS: 0.00020458495919974367\n",
      "TRAIN: EPOCH 20/100 | BATCH 61/71 | LOSS: 0.0002039857595997502\n",
      "TRAIN: EPOCH 20/100 | BATCH 62/71 | LOSS: 0.00020367113549062715\n",
      "TRAIN: EPOCH 20/100 | BATCH 63/71 | LOSS: 0.00020375121243887406\n",
      "TRAIN: EPOCH 20/100 | BATCH 64/71 | LOSS: 0.0002026803108702342\n",
      "TRAIN: EPOCH 20/100 | BATCH 65/71 | LOSS: 0.00020275716440554595\n",
      "TRAIN: EPOCH 20/100 | BATCH 66/71 | LOSS: 0.00020500009578875084\n",
      "TRAIN: EPOCH 20/100 | BATCH 67/71 | LOSS: 0.00020371774425096672\n",
      "TRAIN: EPOCH 20/100 | BATCH 68/71 | LOSS: 0.0002058810292721213\n",
      "TRAIN: EPOCH 20/100 | BATCH 69/71 | LOSS: 0.000205051225306566\n",
      "TRAIN: EPOCH 20/100 | BATCH 70/71 | LOSS: 0.00020527546460048335\n",
      "VAL: EPOCH 20/100 | BATCH 0/8 | LOSS: 0.00017642034799791873\n",
      "VAL: EPOCH 20/100 | BATCH 1/8 | LOSS: 0.00017067270528059453\n",
      "VAL: EPOCH 20/100 | BATCH 2/8 | LOSS: 0.00016964391882841787\n",
      "VAL: EPOCH 20/100 | BATCH 3/8 | LOSS: 0.0001821947516873479\n",
      "VAL: EPOCH 20/100 | BATCH 4/8 | LOSS: 0.0001764539396390319\n",
      "VAL: EPOCH 20/100 | BATCH 5/8 | LOSS: 0.00017353433213429525\n",
      "VAL: EPOCH 20/100 | BATCH 6/8 | LOSS: 0.00018052385088854602\n",
      "VAL: EPOCH 20/100 | BATCH 7/8 | LOSS: 0.00017166644465760328\n",
      "TRAIN: EPOCH 21/100 | BATCH 0/71 | LOSS: 0.00032760502654127777\n",
      "TRAIN: EPOCH 21/100 | BATCH 1/71 | LOSS: 0.00027225686062593013\n",
      "TRAIN: EPOCH 21/100 | BATCH 2/71 | LOSS: 0.00025642471155151725\n",
      "TRAIN: EPOCH 21/100 | BATCH 3/71 | LOSS: 0.0002553821977926418\n",
      "TRAIN: EPOCH 21/100 | BATCH 4/71 | LOSS: 0.00023143957077991216\n",
      "TRAIN: EPOCH 21/100 | BATCH 5/71 | LOSS: 0.00022668427845928818\n",
      "TRAIN: EPOCH 21/100 | BATCH 6/71 | LOSS: 0.0002195201481559447\n",
      "TRAIN: EPOCH 21/100 | BATCH 7/71 | LOSS: 0.00023701250029262155\n",
      "TRAIN: EPOCH 21/100 | BATCH 8/71 | LOSS: 0.00023479779419075284\n",
      "TRAIN: EPOCH 21/100 | BATCH 9/71 | LOSS: 0.00022760272258892655\n",
      "TRAIN: EPOCH 21/100 | BATCH 10/71 | LOSS: 0.00022327197472226214\n",
      "TRAIN: EPOCH 21/100 | BATCH 11/71 | LOSS: 0.0002209459914107962\n",
      "TRAIN: EPOCH 21/100 | BATCH 12/71 | LOSS: 0.00022879354051278473\n",
      "TRAIN: EPOCH 21/100 | BATCH 13/71 | LOSS: 0.00022427443764172494\n",
      "TRAIN: EPOCH 21/100 | BATCH 14/71 | LOSS: 0.00022044592963842055\n",
      "TRAIN: EPOCH 21/100 | BATCH 15/71 | LOSS: 0.00021490718972927425\n",
      "TRAIN: EPOCH 21/100 | BATCH 16/71 | LOSS: 0.0002173141949563561\n",
      "TRAIN: EPOCH 21/100 | BATCH 17/71 | LOSS: 0.00021872676896034845\n",
      "TRAIN: EPOCH 21/100 | BATCH 18/71 | LOSS: 0.00021460432694048474\n",
      "TRAIN: EPOCH 21/100 | BATCH 19/71 | LOSS: 0.000212245958391577\n",
      "TRAIN: EPOCH 21/100 | BATCH 20/71 | LOSS: 0.0002090748859613779\n",
      "TRAIN: EPOCH 21/100 | BATCH 21/71 | LOSS: 0.00021034632597795942\n",
      "TRAIN: EPOCH 21/100 | BATCH 22/71 | LOSS: 0.00021688532560010967\n",
      "TRAIN: EPOCH 21/100 | BATCH 23/71 | LOSS: 0.0002154057328880299\n",
      "TRAIN: EPOCH 21/100 | BATCH 24/71 | LOSS: 0.0002114647562848404\n",
      "TRAIN: EPOCH 21/100 | BATCH 25/71 | LOSS: 0.00020967045286222576\n",
      "TRAIN: EPOCH 21/100 | BATCH 26/71 | LOSS: 0.00020706961448821757\n",
      "TRAIN: EPOCH 21/100 | BATCH 27/71 | LOSS: 0.00020518368390704772\n",
      "TRAIN: EPOCH 21/100 | BATCH 28/71 | LOSS: 0.00020316563012752812\n",
      "TRAIN: EPOCH 21/100 | BATCH 29/71 | LOSS: 0.00020115203806199134\n",
      "TRAIN: EPOCH 21/100 | BATCH 30/71 | LOSS: 0.00019982594840844433\n",
      "TRAIN: EPOCH 21/100 | BATCH 31/71 | LOSS: 0.00019720144359780534\n",
      "TRAIN: EPOCH 21/100 | BATCH 32/71 | LOSS: 0.00019541920791556495\n",
      "TRAIN: EPOCH 21/100 | BATCH 33/71 | LOSS: 0.00019501931156435816\n",
      "TRAIN: EPOCH 21/100 | BATCH 34/71 | LOSS: 0.000196331632780909\n",
      "TRAIN: EPOCH 21/100 | BATCH 35/71 | LOSS: 0.00019678555670705263\n",
      "TRAIN: EPOCH 21/100 | BATCH 36/71 | LOSS: 0.0001948847106226871\n",
      "TRAIN: EPOCH 21/100 | BATCH 37/71 | LOSS: 0.00019297728029318693\n",
      "TRAIN: EPOCH 21/100 | BATCH 38/71 | LOSS: 0.00019520451967717888\n",
      "TRAIN: EPOCH 21/100 | BATCH 39/71 | LOSS: 0.00019528231841832168\n",
      "TRAIN: EPOCH 21/100 | BATCH 40/71 | LOSS: 0.00019890935037415703\n",
      "TRAIN: EPOCH 21/100 | BATCH 41/71 | LOSS: 0.00019757126106664406\n",
      "TRAIN: EPOCH 21/100 | BATCH 42/71 | LOSS: 0.00019690202062456805\n",
      "TRAIN: EPOCH 21/100 | BATCH 43/71 | LOSS: 0.00019787808047443502\n",
      "TRAIN: EPOCH 21/100 | BATCH 44/71 | LOSS: 0.00019720324085533827\n",
      "TRAIN: EPOCH 21/100 | BATCH 45/71 | LOSS: 0.00019579799545286022\n",
      "TRAIN: EPOCH 21/100 | BATCH 46/71 | LOSS: 0.00019584915323921063\n",
      "TRAIN: EPOCH 21/100 | BATCH 47/71 | LOSS: 0.00019530320438813456\n",
      "TRAIN: EPOCH 21/100 | BATCH 48/71 | LOSS: 0.0001942549675657908\n",
      "TRAIN: EPOCH 21/100 | BATCH 49/71 | LOSS: 0.00019314588935230858\n",
      "TRAIN: EPOCH 21/100 | BATCH 50/71 | LOSS: 0.0001946974857584756\n",
      "TRAIN: EPOCH 21/100 | BATCH 51/71 | LOSS: 0.00019291523550726616\n",
      "TRAIN: EPOCH 21/100 | BATCH 52/71 | LOSS: 0.00019237594865701812\n",
      "TRAIN: EPOCH 21/100 | BATCH 53/71 | LOSS: 0.00019178009555347402\n",
      "TRAIN: EPOCH 21/100 | BATCH 54/71 | LOSS: 0.00019273701049810784\n",
      "TRAIN: EPOCH 21/100 | BATCH 55/71 | LOSS: 0.00019351736253676272\n",
      "TRAIN: EPOCH 21/100 | BATCH 56/71 | LOSS: 0.00019250978591132974\n",
      "TRAIN: EPOCH 21/100 | BATCH 57/71 | LOSS: 0.00019281879438745692\n",
      "TRAIN: EPOCH 21/100 | BATCH 58/71 | LOSS: 0.00019161768136269775\n",
      "TRAIN: EPOCH 21/100 | BATCH 59/71 | LOSS: 0.00019125226802619484\n",
      "TRAIN: EPOCH 21/100 | BATCH 60/71 | LOSS: 0.00019087320738510213\n",
      "TRAIN: EPOCH 21/100 | BATCH 61/71 | LOSS: 0.00019025670401113588\n",
      "TRAIN: EPOCH 21/100 | BATCH 62/71 | LOSS: 0.0001895613954513378\n",
      "TRAIN: EPOCH 21/100 | BATCH 63/71 | LOSS: 0.00019131364479108015\n",
      "TRAIN: EPOCH 21/100 | BATCH 64/71 | LOSS: 0.00019120180871910774\n",
      "TRAIN: EPOCH 21/100 | BATCH 65/71 | LOSS: 0.00019047366698611188\n",
      "TRAIN: EPOCH 21/100 | BATCH 66/71 | LOSS: 0.00019033087930927026\n",
      "TRAIN: EPOCH 21/100 | BATCH 67/71 | LOSS: 0.0001896382041997961\n",
      "TRAIN: EPOCH 21/100 | BATCH 68/71 | LOSS: 0.00019043432384142247\n",
      "TRAIN: EPOCH 21/100 | BATCH 69/71 | LOSS: 0.00018961550730247316\n",
      "TRAIN: EPOCH 21/100 | BATCH 70/71 | LOSS: 0.0001878323254600154\n",
      "VAL: EPOCH 21/100 | BATCH 0/8 | LOSS: 0.00016119865176733583\n",
      "VAL: EPOCH 21/100 | BATCH 1/8 | LOSS: 0.0001565103666507639\n",
      "VAL: EPOCH 21/100 | BATCH 2/8 | LOSS: 0.00015552596596535295\n",
      "VAL: EPOCH 21/100 | BATCH 3/8 | LOSS: 0.00016696431339369155\n",
      "VAL: EPOCH 21/100 | BATCH 4/8 | LOSS: 0.0001615584595128894\n",
      "VAL: EPOCH 21/100 | BATCH 5/8 | LOSS: 0.00015901748944694796\n",
      "VAL: EPOCH 21/100 | BATCH 6/8 | LOSS: 0.00016550470068718174\n",
      "VAL: EPOCH 21/100 | BATCH 7/8 | LOSS: 0.00015754761716380017\n",
      "TRAIN: EPOCH 22/100 | BATCH 0/71 | LOSS: 0.00016156982746906579\n",
      "TRAIN: EPOCH 22/100 | BATCH 1/71 | LOSS: 0.00022546807304024696\n",
      "TRAIN: EPOCH 22/100 | BATCH 2/71 | LOSS: 0.0001902618799552632\n",
      "TRAIN: EPOCH 22/100 | BATCH 3/71 | LOSS: 0.00017928199122252408\n",
      "TRAIN: EPOCH 22/100 | BATCH 4/71 | LOSS: 0.00018303004471817986\n",
      "TRAIN: EPOCH 22/100 | BATCH 5/71 | LOSS: 0.000221783800952835\n",
      "TRAIN: EPOCH 22/100 | BATCH 6/71 | LOSS: 0.00022463974996104037\n",
      "TRAIN: EPOCH 22/100 | BATCH 7/71 | LOSS: 0.00021868481508136028\n",
      "TRAIN: EPOCH 22/100 | BATCH 8/71 | LOSS: 0.00021788976624116508\n",
      "TRAIN: EPOCH 22/100 | BATCH 9/71 | LOSS: 0.00022577626004931516\n",
      "TRAIN: EPOCH 22/100 | BATCH 10/71 | LOSS: 0.00021760583529777995\n",
      "TRAIN: EPOCH 22/100 | BATCH 11/71 | LOSS: 0.00021511221772622471\n",
      "TRAIN: EPOCH 22/100 | BATCH 12/71 | LOSS: 0.00021255659913339725\n",
      "TRAIN: EPOCH 22/100 | BATCH 13/71 | LOSS: 0.00021358557101588564\n",
      "TRAIN: EPOCH 22/100 | BATCH 14/71 | LOSS: 0.0002124200424683901\n",
      "TRAIN: EPOCH 22/100 | BATCH 15/71 | LOSS: 0.00021611701640722458\n",
      "TRAIN: EPOCH 22/100 | BATCH 16/71 | LOSS: 0.00021812798128367458\n",
      "TRAIN: EPOCH 22/100 | BATCH 17/71 | LOSS: 0.0002211345551283254\n",
      "TRAIN: EPOCH 22/100 | BATCH 18/71 | LOSS: 0.00021780360494679036\n",
      "TRAIN: EPOCH 22/100 | BATCH 19/71 | LOSS: 0.0002177267935621785\n",
      "TRAIN: EPOCH 22/100 | BATCH 20/71 | LOSS: 0.00021374636430880942\n",
      "TRAIN: EPOCH 22/100 | BATCH 21/71 | LOSS: 0.00020997745906987058\n",
      "TRAIN: EPOCH 22/100 | BATCH 22/71 | LOSS: 0.00021154296352862096\n",
      "TRAIN: EPOCH 22/100 | BATCH 23/71 | LOSS: 0.00020955325120060783\n",
      "TRAIN: EPOCH 22/100 | BATCH 24/71 | LOSS: 0.00020845197403104976\n",
      "TRAIN: EPOCH 22/100 | BATCH 25/71 | LOSS: 0.0002065188480348577\n",
      "TRAIN: EPOCH 22/100 | BATCH 26/71 | LOSS: 0.0002033509552819413\n",
      "TRAIN: EPOCH 22/100 | BATCH 27/71 | LOSS: 0.00019996351511508692\n",
      "TRAIN: EPOCH 22/100 | BATCH 28/71 | LOSS: 0.0002034350259707245\n",
      "TRAIN: EPOCH 22/100 | BATCH 29/71 | LOSS: 0.00020105840546117784\n",
      "TRAIN: EPOCH 22/100 | BATCH 30/71 | LOSS: 0.00019931444647579242\n",
      "TRAIN: EPOCH 22/100 | BATCH 31/71 | LOSS: 0.00019780610159614298\n",
      "TRAIN: EPOCH 22/100 | BATCH 32/71 | LOSS: 0.0001963059385712823\n",
      "TRAIN: EPOCH 22/100 | BATCH 33/71 | LOSS: 0.0001953920597289278\n",
      "TRAIN: EPOCH 22/100 | BATCH 34/71 | LOSS: 0.0001944926818915909\n",
      "TRAIN: EPOCH 22/100 | BATCH 35/71 | LOSS: 0.00019273653722646285\n",
      "TRAIN: EPOCH 22/100 | BATCH 36/71 | LOSS: 0.00019134911279842208\n",
      "TRAIN: EPOCH 22/100 | BATCH 37/71 | LOSS: 0.00019419069940340705\n",
      "TRAIN: EPOCH 22/100 | BATCH 38/71 | LOSS: 0.0001925681603890366\n",
      "TRAIN: EPOCH 22/100 | BATCH 39/71 | LOSS: 0.00019112296449748102\n",
      "TRAIN: EPOCH 22/100 | BATCH 40/71 | LOSS: 0.0001904785648348923\n",
      "TRAIN: EPOCH 22/100 | BATCH 41/71 | LOSS: 0.00018899457565913465\n",
      "TRAIN: EPOCH 22/100 | BATCH 42/71 | LOSS: 0.00018749640172287864\n",
      "TRAIN: EPOCH 22/100 | BATCH 43/71 | LOSS: 0.0001881454004019774\n",
      "TRAIN: EPOCH 22/100 | BATCH 44/71 | LOSS: 0.00018846755960516423\n",
      "TRAIN: EPOCH 22/100 | BATCH 45/71 | LOSS: 0.00018675662858700178\n",
      "TRAIN: EPOCH 22/100 | BATCH 46/71 | LOSS: 0.00018541711680417723\n",
      "TRAIN: EPOCH 22/100 | BATCH 47/71 | LOSS: 0.0001869816395204301\n",
      "TRAIN: EPOCH 22/100 | BATCH 48/71 | LOSS: 0.00018532778007543778\n",
      "TRAIN: EPOCH 22/100 | BATCH 49/71 | LOSS: 0.00018614942746353336\n",
      "TRAIN: EPOCH 22/100 | BATCH 50/71 | LOSS: 0.00018534354974547693\n",
      "TRAIN: EPOCH 22/100 | BATCH 51/71 | LOSS: 0.0001837984453651678\n",
      "TRAIN: EPOCH 22/100 | BATCH 52/71 | LOSS: 0.0001840966352522988\n",
      "TRAIN: EPOCH 22/100 | BATCH 53/71 | LOSS: 0.00018303036887556555\n",
      "TRAIN: EPOCH 22/100 | BATCH 54/71 | LOSS: 0.00018254973150429909\n",
      "TRAIN: EPOCH 22/100 | BATCH 55/71 | LOSS: 0.00018142240527855132\n",
      "TRAIN: EPOCH 22/100 | BATCH 56/71 | LOSS: 0.00018096349767323487\n",
      "TRAIN: EPOCH 22/100 | BATCH 57/71 | LOSS: 0.0001802421622791183\n",
      "TRAIN: EPOCH 22/100 | BATCH 58/71 | LOSS: 0.00017934581928322156\n",
      "TRAIN: EPOCH 22/100 | BATCH 59/71 | LOSS: 0.00017803156079025938\n",
      "TRAIN: EPOCH 22/100 | BATCH 60/71 | LOSS: 0.00017713671970013225\n",
      "TRAIN: EPOCH 22/100 | BATCH 61/71 | LOSS: 0.00017782825893992857\n",
      "TRAIN: EPOCH 22/100 | BATCH 62/71 | LOSS: 0.0001786090641482068\n",
      "TRAIN: EPOCH 22/100 | BATCH 63/71 | LOSS: 0.00017748149059571006\n",
      "TRAIN: EPOCH 22/100 | BATCH 64/71 | LOSS: 0.00017677545737779628\n",
      "TRAIN: EPOCH 22/100 | BATCH 65/71 | LOSS: 0.00017607831438054387\n",
      "TRAIN: EPOCH 22/100 | BATCH 66/71 | LOSS: 0.00017626363592510765\n",
      "TRAIN: EPOCH 22/100 | BATCH 67/71 | LOSS: 0.00017598736506924443\n",
      "TRAIN: EPOCH 22/100 | BATCH 68/71 | LOSS: 0.00017512037416251943\n",
      "TRAIN: EPOCH 22/100 | BATCH 69/71 | LOSS: 0.000174552302009293\n",
      "TRAIN: EPOCH 22/100 | BATCH 70/71 | LOSS: 0.00017502700027019363\n",
      "VAL: EPOCH 22/100 | BATCH 0/8 | LOSS: 0.0001511456212028861\n",
      "VAL: EPOCH 22/100 | BATCH 1/8 | LOSS: 0.00014587465557269752\n",
      "VAL: EPOCH 22/100 | BATCH 2/8 | LOSS: 0.00014515693571108082\n",
      "VAL: EPOCH 22/100 | BATCH 3/8 | LOSS: 0.00015671450455556624\n",
      "VAL: EPOCH 22/100 | BATCH 4/8 | LOSS: 0.0001512844319222495\n",
      "VAL: EPOCH 22/100 | BATCH 5/8 | LOSS: 0.00014834788210767633\n",
      "VAL: EPOCH 22/100 | BATCH 6/8 | LOSS: 0.00015402415010612458\n",
      "VAL: EPOCH 22/100 | BATCH 7/8 | LOSS: 0.00014664318496215856\n",
      "TRAIN: EPOCH 23/100 | BATCH 0/71 | LOSS: 0.00014347568503580987\n",
      "TRAIN: EPOCH 23/100 | BATCH 1/71 | LOSS: 0.00015419307601405308\n",
      "TRAIN: EPOCH 23/100 | BATCH 2/71 | LOSS: 0.00015011857612989843\n",
      "TRAIN: EPOCH 23/100 | BATCH 3/71 | LOSS: 0.00016176211647689342\n",
      "TRAIN: EPOCH 23/100 | BATCH 4/71 | LOSS: 0.00014800241915509106\n",
      "TRAIN: EPOCH 23/100 | BATCH 5/71 | LOSS: 0.00014277559239417315\n",
      "TRAIN: EPOCH 23/100 | BATCH 6/71 | LOSS: 0.00014044752294596816\n",
      "TRAIN: EPOCH 23/100 | BATCH 7/71 | LOSS: 0.00013766386837232858\n",
      "TRAIN: EPOCH 23/100 | BATCH 8/71 | LOSS: 0.0001362257510966932\n",
      "TRAIN: EPOCH 23/100 | BATCH 9/71 | LOSS: 0.00013619221281260251\n",
      "TRAIN: EPOCH 23/100 | BATCH 10/71 | LOSS: 0.00013603016056797722\n",
      "TRAIN: EPOCH 23/100 | BATCH 11/71 | LOSS: 0.00014668523605602482\n",
      "TRAIN: EPOCH 23/100 | BATCH 12/71 | LOSS: 0.00014757551476717569\n",
      "TRAIN: EPOCH 23/100 | BATCH 13/71 | LOSS: 0.00014526889780037372\n",
      "TRAIN: EPOCH 23/100 | BATCH 14/71 | LOSS: 0.00014435718670332184\n",
      "TRAIN: EPOCH 23/100 | BATCH 15/71 | LOSS: 0.00014226067696654354\n",
      "TRAIN: EPOCH 23/100 | BATCH 16/71 | LOSS: 0.0001455610605155337\n",
      "TRAIN: EPOCH 23/100 | BATCH 17/71 | LOSS: 0.00014609801655751653\n",
      "TRAIN: EPOCH 23/100 | BATCH 18/71 | LOSS: 0.0001466582337008348\n",
      "TRAIN: EPOCH 23/100 | BATCH 19/71 | LOSS: 0.00014677170729555656\n",
      "TRAIN: EPOCH 23/100 | BATCH 20/71 | LOSS: 0.0001443835925394004\n",
      "TRAIN: EPOCH 23/100 | BATCH 21/71 | LOSS: 0.0001450387998457617\n",
      "TRAIN: EPOCH 23/100 | BATCH 22/71 | LOSS: 0.00014466472361084965\n",
      "TRAIN: EPOCH 23/100 | BATCH 23/71 | LOSS: 0.00014602873761759838\n",
      "TRAIN: EPOCH 23/100 | BATCH 24/71 | LOSS: 0.00014707154041389004\n",
      "TRAIN: EPOCH 23/100 | BATCH 25/71 | LOSS: 0.00014545036118271618\n",
      "TRAIN: EPOCH 23/100 | BATCH 26/71 | LOSS: 0.00014397997546001097\n",
      "TRAIN: EPOCH 23/100 | BATCH 27/71 | LOSS: 0.00014406934886730078\n",
      "TRAIN: EPOCH 23/100 | BATCH 28/71 | LOSS: 0.00014374595951361196\n",
      "TRAIN: EPOCH 23/100 | BATCH 29/71 | LOSS: 0.00014566403575978862\n",
      "TRAIN: EPOCH 23/100 | BATCH 30/71 | LOSS: 0.00014550882957748047\n",
      "TRAIN: EPOCH 23/100 | BATCH 31/71 | LOSS: 0.00014733385091858509\n",
      "TRAIN: EPOCH 23/100 | BATCH 32/71 | LOSS: 0.00014947422724534673\n",
      "TRAIN: EPOCH 23/100 | BATCH 33/71 | LOSS: 0.0001482429864100756\n",
      "TRAIN: EPOCH 23/100 | BATCH 34/71 | LOSS: 0.0001471629524270871\n",
      "TRAIN: EPOCH 23/100 | BATCH 35/71 | LOSS: 0.0001492302120216967\n",
      "TRAIN: EPOCH 23/100 | BATCH 36/71 | LOSS: 0.00015227247864901515\n",
      "TRAIN: EPOCH 23/100 | BATCH 37/71 | LOSS: 0.00015066128119207476\n",
      "TRAIN: EPOCH 23/100 | BATCH 38/71 | LOSS: 0.0001493821913848082\n",
      "TRAIN: EPOCH 23/100 | BATCH 39/71 | LOSS: 0.00015047644556034356\n",
      "TRAIN: EPOCH 23/100 | BATCH 40/71 | LOSS: 0.00015260974689182348\n",
      "TRAIN: EPOCH 23/100 | BATCH 41/71 | LOSS: 0.0001530539039300666\n",
      "TRAIN: EPOCH 23/100 | BATCH 42/71 | LOSS: 0.00015274158693839211\n",
      "TRAIN: EPOCH 23/100 | BATCH 43/71 | LOSS: 0.00015490021145193498\n",
      "TRAIN: EPOCH 23/100 | BATCH 44/71 | LOSS: 0.00015592612940559371\n",
      "TRAIN: EPOCH 23/100 | BATCH 45/71 | LOSS: 0.00015664791040446448\n",
      "TRAIN: EPOCH 23/100 | BATCH 46/71 | LOSS: 0.00015649964761712211\n",
      "TRAIN: EPOCH 23/100 | BATCH 47/71 | LOSS: 0.00015610522647572603\n",
      "TRAIN: EPOCH 23/100 | BATCH 48/71 | LOSS: 0.00015708755515278223\n",
      "TRAIN: EPOCH 23/100 | BATCH 49/71 | LOSS: 0.00015798593376530333\n",
      "TRAIN: EPOCH 23/100 | BATCH 50/71 | LOSS: 0.00015790361965842107\n",
      "TRAIN: EPOCH 23/100 | BATCH 51/71 | LOSS: 0.00015917179776946656\n",
      "TRAIN: EPOCH 23/100 | BATCH 52/71 | LOSS: 0.00016209084643841774\n",
      "TRAIN: EPOCH 23/100 | BATCH 53/71 | LOSS: 0.00016238009400829605\n",
      "TRAIN: EPOCH 23/100 | BATCH 54/71 | LOSS: 0.00016157343409511007\n",
      "TRAIN: EPOCH 23/100 | BATCH 55/71 | LOSS: 0.00016133396833798282\n",
      "TRAIN: EPOCH 23/100 | BATCH 56/71 | LOSS: 0.00016152514121801543\n",
      "TRAIN: EPOCH 23/100 | BATCH 57/71 | LOSS: 0.00016439177689507828\n",
      "TRAIN: EPOCH 23/100 | BATCH 58/71 | LOSS: 0.0001634215239052645\n",
      "TRAIN: EPOCH 23/100 | BATCH 59/71 | LOSS: 0.00016238571988651528\n",
      "TRAIN: EPOCH 23/100 | BATCH 60/71 | LOSS: 0.00016137488428732288\n",
      "TRAIN: EPOCH 23/100 | BATCH 61/71 | LOSS: 0.00016076694197592235\n",
      "TRAIN: EPOCH 23/100 | BATCH 62/71 | LOSS: 0.0001597605662518329\n",
      "TRAIN: EPOCH 23/100 | BATCH 63/71 | LOSS: 0.00016084700052942935\n",
      "TRAIN: EPOCH 23/100 | BATCH 64/71 | LOSS: 0.00016139304755667511\n",
      "TRAIN: EPOCH 23/100 | BATCH 65/71 | LOSS: 0.0001613560859616634\n",
      "TRAIN: EPOCH 23/100 | BATCH 66/71 | LOSS: 0.00016137011231482723\n",
      "TRAIN: EPOCH 23/100 | BATCH 67/71 | LOSS: 0.00016178135207583718\n",
      "TRAIN: EPOCH 23/100 | BATCH 68/71 | LOSS: 0.00016193945339182392\n",
      "TRAIN: EPOCH 23/100 | BATCH 69/71 | LOSS: 0.00016174487172975204\n",
      "TRAIN: EPOCH 23/100 | BATCH 70/71 | LOSS: 0.00016310477096826808\n",
      "VAL: EPOCH 23/100 | BATCH 0/8 | LOSS: 0.00013722945004701614\n",
      "VAL: EPOCH 23/100 | BATCH 1/8 | LOSS: 0.00013303802552400157\n",
      "VAL: EPOCH 23/100 | BATCH 2/8 | LOSS: 0.00013179392650878677\n",
      "VAL: EPOCH 23/100 | BATCH 3/8 | LOSS: 0.00014210297013050877\n",
      "VAL: EPOCH 23/100 | BATCH 4/8 | LOSS: 0.00013644995779031887\n",
      "VAL: EPOCH 23/100 | BATCH 5/8 | LOSS: 0.00013483682535782768\n",
      "VAL: EPOCH 23/100 | BATCH 6/8 | LOSS: 0.00013959014592858563\n",
      "VAL: EPOCH 23/100 | BATCH 7/8 | LOSS: 0.0001334530988970073\n",
      "TRAIN: EPOCH 24/100 | BATCH 0/71 | LOSS: 0.00015524044283665717\n",
      "TRAIN: EPOCH 24/100 | BATCH 1/71 | LOSS: 0.0001300949152209796\n",
      "TRAIN: EPOCH 24/100 | BATCH 2/71 | LOSS: 0.00012445436247314015\n",
      "TRAIN: EPOCH 24/100 | BATCH 3/71 | LOSS: 0.0001329675505985506\n",
      "TRAIN: EPOCH 24/100 | BATCH 4/71 | LOSS: 0.0001344825606793165\n",
      "TRAIN: EPOCH 24/100 | BATCH 5/71 | LOSS: 0.0001361319858309192\n",
      "TRAIN: EPOCH 24/100 | BATCH 6/71 | LOSS: 0.00013567947150607194\n",
      "TRAIN: EPOCH 24/100 | BATCH 7/71 | LOSS: 0.00014030065722181462\n",
      "TRAIN: EPOCH 24/100 | BATCH 8/71 | LOSS: 0.00014851788809109066\n",
      "TRAIN: EPOCH 24/100 | BATCH 9/71 | LOSS: 0.00014381199871422724\n",
      "TRAIN: EPOCH 24/100 | BATCH 10/71 | LOSS: 0.00014199483848642558\n",
      "TRAIN: EPOCH 24/100 | BATCH 11/71 | LOSS: 0.00015132818104272397\n",
      "TRAIN: EPOCH 24/100 | BATCH 12/71 | LOSS: 0.00014608381304872007\n",
      "TRAIN: EPOCH 24/100 | BATCH 13/71 | LOSS: 0.00014827691744098308\n",
      "TRAIN: EPOCH 24/100 | BATCH 14/71 | LOSS: 0.0001475259453097048\n",
      "TRAIN: EPOCH 24/100 | BATCH 15/71 | LOSS: 0.00014625903577325516\n",
      "TRAIN: EPOCH 24/100 | BATCH 16/71 | LOSS: 0.00014464057826176833\n",
      "TRAIN: EPOCH 24/100 | BATCH 17/71 | LOSS: 0.0001422857615883307\n",
      "TRAIN: EPOCH 24/100 | BATCH 18/71 | LOSS: 0.00015044760067738886\n",
      "TRAIN: EPOCH 24/100 | BATCH 19/71 | LOSS: 0.00015019442544144112\n",
      "TRAIN: EPOCH 24/100 | BATCH 20/71 | LOSS: 0.00014980604291416793\n",
      "TRAIN: EPOCH 24/100 | BATCH 21/71 | LOSS: 0.00014806033901880835\n",
      "TRAIN: EPOCH 24/100 | BATCH 22/71 | LOSS: 0.00014628200290684143\n",
      "TRAIN: EPOCH 24/100 | BATCH 23/71 | LOSS: 0.00014770193229196593\n",
      "TRAIN: EPOCH 24/100 | BATCH 24/71 | LOSS: 0.00014765795320272445\n",
      "TRAIN: EPOCH 24/100 | BATCH 25/71 | LOSS: 0.00014682611799798906\n",
      "TRAIN: EPOCH 24/100 | BATCH 26/71 | LOSS: 0.00014517834750693982\n",
      "TRAIN: EPOCH 24/100 | BATCH 27/71 | LOSS: 0.00014423985807557723\n",
      "TRAIN: EPOCH 24/100 | BATCH 28/71 | LOSS: 0.00014265903305856446\n",
      "TRAIN: EPOCH 24/100 | BATCH 29/71 | LOSS: 0.00014562561555067077\n",
      "TRAIN: EPOCH 24/100 | BATCH 30/71 | LOSS: 0.00014593413629148517\n",
      "TRAIN: EPOCH 24/100 | BATCH 31/71 | LOSS: 0.00014633813634645776\n",
      "TRAIN: EPOCH 24/100 | BATCH 32/71 | LOSS: 0.0001463863763175058\n",
      "TRAIN: EPOCH 24/100 | BATCH 33/71 | LOSS: 0.00014786536656388575\n",
      "TRAIN: EPOCH 24/100 | BATCH 34/71 | LOSS: 0.00014711347300492758\n",
      "TRAIN: EPOCH 24/100 | BATCH 35/71 | LOSS: 0.00014702286999737326\n",
      "TRAIN: EPOCH 24/100 | BATCH 36/71 | LOSS: 0.00014829646435729856\n",
      "TRAIN: EPOCH 24/100 | BATCH 37/71 | LOSS: 0.00014908815080018125\n",
      "TRAIN: EPOCH 24/100 | BATCH 38/71 | LOSS: 0.0001506791013525799\n",
      "TRAIN: EPOCH 24/100 | BATCH 39/71 | LOSS: 0.00015195310370472726\n",
      "TRAIN: EPOCH 24/100 | BATCH 40/71 | LOSS: 0.00015133423638450573\n",
      "TRAIN: EPOCH 24/100 | BATCH 41/71 | LOSS: 0.00015000077006074468\n",
      "TRAIN: EPOCH 24/100 | BATCH 42/71 | LOSS: 0.0001509347856098916\n",
      "TRAIN: EPOCH 24/100 | BATCH 43/71 | LOSS: 0.00015268422562671319\n",
      "TRAIN: EPOCH 24/100 | BATCH 44/71 | LOSS: 0.00015254485770128668\n",
      "TRAIN: EPOCH 24/100 | BATCH 45/71 | LOSS: 0.00015172967497441593\n",
      "TRAIN: EPOCH 24/100 | BATCH 46/71 | LOSS: 0.00015280336671647557\n",
      "TRAIN: EPOCH 24/100 | BATCH 47/71 | LOSS: 0.00015201669536205978\n",
      "TRAIN: EPOCH 24/100 | BATCH 48/71 | LOSS: 0.0001533399815659267\n",
      "TRAIN: EPOCH 24/100 | BATCH 49/71 | LOSS: 0.00015348047410952857\n",
      "TRAIN: EPOCH 24/100 | BATCH 50/71 | LOSS: 0.00015395108451176544\n",
      "TRAIN: EPOCH 24/100 | BATCH 51/71 | LOSS: 0.00015374886494940327\n",
      "TRAIN: EPOCH 24/100 | BATCH 52/71 | LOSS: 0.00015332762089904995\n",
      "TRAIN: EPOCH 24/100 | BATCH 53/71 | LOSS: 0.00015228620907658264\n",
      "TRAIN: EPOCH 24/100 | BATCH 54/71 | LOSS: 0.00015168372165962039\n",
      "TRAIN: EPOCH 24/100 | BATCH 55/71 | LOSS: 0.00015188227163369966\n",
      "TRAIN: EPOCH 24/100 | BATCH 56/71 | LOSS: 0.00015129751026458842\n",
      "TRAIN: EPOCH 24/100 | BATCH 57/71 | LOSS: 0.00015050724812618713\n",
      "TRAIN: EPOCH 24/100 | BATCH 58/71 | LOSS: 0.00015042179697725953\n",
      "TRAIN: EPOCH 24/100 | BATCH 59/71 | LOSS: 0.00015039595831088567\n",
      "TRAIN: EPOCH 24/100 | BATCH 60/71 | LOSS: 0.00015023641503457988\n",
      "TRAIN: EPOCH 24/100 | BATCH 61/71 | LOSS: 0.00015122242280819092\n",
      "TRAIN: EPOCH 24/100 | BATCH 62/71 | LOSS: 0.00015008404397309595\n",
      "TRAIN: EPOCH 24/100 | BATCH 63/71 | LOSS: 0.00014953594700273243\n",
      "TRAIN: EPOCH 24/100 | BATCH 64/71 | LOSS: 0.0001497745151237513\n",
      "TRAIN: EPOCH 24/100 | BATCH 65/71 | LOSS: 0.0001492902828789683\n",
      "TRAIN: EPOCH 24/100 | BATCH 66/71 | LOSS: 0.00014840299089842324\n",
      "TRAIN: EPOCH 24/100 | BATCH 67/71 | LOSS: 0.00014880753395332517\n",
      "TRAIN: EPOCH 24/100 | BATCH 68/71 | LOSS: 0.00014877790670362774\n",
      "TRAIN: EPOCH 24/100 | BATCH 69/71 | LOSS: 0.00014838637590790832\n",
      "TRAIN: EPOCH 24/100 | BATCH 70/71 | LOSS: 0.00014733411162652234\n",
      "VAL: EPOCH 24/100 | BATCH 0/8 | LOSS: 0.00012528992374427617\n",
      "VAL: EPOCH 24/100 | BATCH 1/8 | LOSS: 0.00012033254824928008\n",
      "VAL: EPOCH 24/100 | BATCH 2/8 | LOSS: 0.00011961167183471844\n",
      "VAL: EPOCH 24/100 | BATCH 3/8 | LOSS: 0.00012962852815689985\n",
      "VAL: EPOCH 24/100 | BATCH 4/8 | LOSS: 0.00012420020502759143\n",
      "VAL: EPOCH 24/100 | BATCH 5/8 | LOSS: 0.00012289508231333457\n",
      "VAL: EPOCH 24/100 | BATCH 6/8 | LOSS: 0.00012738642960487465\n",
      "VAL: EPOCH 24/100 | BATCH 7/8 | LOSS: 0.00012158976187492954\n",
      "TRAIN: EPOCH 25/100 | BATCH 0/71 | LOSS: 0.00013806743663735688\n",
      "TRAIN: EPOCH 25/100 | BATCH 1/71 | LOSS: 0.00011899101082235575\n",
      "TRAIN: EPOCH 25/100 | BATCH 2/71 | LOSS: 0.00010997934441547841\n",
      "TRAIN: EPOCH 25/100 | BATCH 3/71 | LOSS: 0.00011413802712922916\n",
      "TRAIN: EPOCH 25/100 | BATCH 4/71 | LOSS: 0.0001115400402341038\n",
      "TRAIN: EPOCH 25/100 | BATCH 5/71 | LOSS: 0.00011154525782330893\n",
      "TRAIN: EPOCH 25/100 | BATCH 6/71 | LOSS: 0.00010820958206230509\n",
      "TRAIN: EPOCH 25/100 | BATCH 7/71 | LOSS: 0.00010910786295426078\n",
      "TRAIN: EPOCH 25/100 | BATCH 8/71 | LOSS: 0.00010718568713249017\n",
      "TRAIN: EPOCH 25/100 | BATCH 9/71 | LOSS: 0.00010683758300729096\n",
      "TRAIN: EPOCH 25/100 | BATCH 10/71 | LOSS: 0.00011476519681640308\n",
      "TRAIN: EPOCH 25/100 | BATCH 11/71 | LOSS: 0.00011261263716733083\n",
      "TRAIN: EPOCH 25/100 | BATCH 12/71 | LOSS: 0.00011320183190176837\n",
      "TRAIN: EPOCH 25/100 | BATCH 13/71 | LOSS: 0.00012014525990317842\n",
      "TRAIN: EPOCH 25/100 | BATCH 14/71 | LOSS: 0.00012098170263925567\n",
      "TRAIN: EPOCH 25/100 | BATCH 15/71 | LOSS: 0.00012051001658619498\n",
      "TRAIN: EPOCH 25/100 | BATCH 16/71 | LOSS: 0.00012484408259350697\n",
      "TRAIN: EPOCH 25/100 | BATCH 17/71 | LOSS: 0.00012877785654078858\n",
      "TRAIN: EPOCH 25/100 | BATCH 18/71 | LOSS: 0.00012982271934896218\n",
      "TRAIN: EPOCH 25/100 | BATCH 19/71 | LOSS: 0.00013263087421364617\n",
      "TRAIN: EPOCH 25/100 | BATCH 20/71 | LOSS: 0.0001321485764319299\n",
      "TRAIN: EPOCH 25/100 | BATCH 21/71 | LOSS: 0.00013146321957306512\n",
      "TRAIN: EPOCH 25/100 | BATCH 22/71 | LOSS: 0.00013135340850567445\n",
      "TRAIN: EPOCH 25/100 | BATCH 23/71 | LOSS: 0.00013004796225383566\n",
      "TRAIN: EPOCH 25/100 | BATCH 24/71 | LOSS: 0.0001297049777349457\n",
      "TRAIN: EPOCH 25/100 | BATCH 25/71 | LOSS: 0.00013481735760041585\n",
      "TRAIN: EPOCH 25/100 | BATCH 26/71 | LOSS: 0.00013330118208951144\n",
      "TRAIN: EPOCH 25/100 | BATCH 27/71 | LOSS: 0.0001313101651508727\n",
      "TRAIN: EPOCH 25/100 | BATCH 28/71 | LOSS: 0.0001337480154751543\n",
      "TRAIN: EPOCH 25/100 | BATCH 29/71 | LOSS: 0.0001367895929433871\n",
      "TRAIN: EPOCH 25/100 | BATCH 30/71 | LOSS: 0.00013652339557959368\n",
      "TRAIN: EPOCH 25/100 | BATCH 31/71 | LOSS: 0.000135658520321158\n",
      "TRAIN: EPOCH 25/100 | BATCH 32/71 | LOSS: 0.0001364418496836372\n",
      "TRAIN: EPOCH 25/100 | BATCH 33/71 | LOSS: 0.0001351442697341554\n",
      "TRAIN: EPOCH 25/100 | BATCH 34/71 | LOSS: 0.00013477896780906512\n",
      "TRAIN: EPOCH 25/100 | BATCH 35/71 | LOSS: 0.00013524168550955236\n",
      "TRAIN: EPOCH 25/100 | BATCH 36/71 | LOSS: 0.00013495741704655056\n",
      "TRAIN: EPOCH 25/100 | BATCH 37/71 | LOSS: 0.00013483112975097212\n",
      "TRAIN: EPOCH 25/100 | BATCH 38/71 | LOSS: 0.00013468820207680648\n",
      "TRAIN: EPOCH 25/100 | BATCH 39/71 | LOSS: 0.0001344110434729373\n",
      "TRAIN: EPOCH 25/100 | BATCH 40/71 | LOSS: 0.00013441212578937866\n",
      "TRAIN: EPOCH 25/100 | BATCH 41/71 | LOSS: 0.00013425784951929623\n",
      "TRAIN: EPOCH 25/100 | BATCH 42/71 | LOSS: 0.0001341336972599979\n",
      "TRAIN: EPOCH 25/100 | BATCH 43/71 | LOSS: 0.00013388710464245048\n",
      "TRAIN: EPOCH 25/100 | BATCH 44/71 | LOSS: 0.00013340134399994794\n",
      "TRAIN: EPOCH 25/100 | BATCH 45/71 | LOSS: 0.00013291257694262364\n",
      "TRAIN: EPOCH 25/100 | BATCH 46/71 | LOSS: 0.000132717270196039\n",
      "TRAIN: EPOCH 25/100 | BATCH 47/71 | LOSS: 0.00013448816692592422\n",
      "TRAIN: EPOCH 25/100 | BATCH 48/71 | LOSS: 0.0001343443160591533\n",
      "TRAIN: EPOCH 25/100 | BATCH 49/71 | LOSS: 0.00013483255810569972\n",
      "TRAIN: EPOCH 25/100 | BATCH 50/71 | LOSS: 0.00013471090813468704\n",
      "TRAIN: EPOCH 25/100 | BATCH 51/71 | LOSS: 0.00013519089574961422\n",
      "TRAIN: EPOCH 25/100 | BATCH 52/71 | LOSS: 0.00013579757371258411\n",
      "TRAIN: EPOCH 25/100 | BATCH 53/71 | LOSS: 0.00013532438552593467\n",
      "TRAIN: EPOCH 25/100 | BATCH 54/71 | LOSS: 0.0001349358258514919\n",
      "TRAIN: EPOCH 25/100 | BATCH 55/71 | LOSS: 0.0001347330928963077\n",
      "TRAIN: EPOCH 25/100 | BATCH 56/71 | LOSS: 0.00013588226258370764\n",
      "TRAIN: EPOCH 25/100 | BATCH 57/71 | LOSS: 0.00013605796316526188\n",
      "TRAIN: EPOCH 25/100 | BATCH 58/71 | LOSS: 0.00013620968635778054\n",
      "TRAIN: EPOCH 25/100 | BATCH 59/71 | LOSS: 0.0001363903735182248\n",
      "TRAIN: EPOCH 25/100 | BATCH 60/71 | LOSS: 0.0001379817091577427\n",
      "TRAIN: EPOCH 25/100 | BATCH 61/71 | LOSS: 0.00013857391041388075\n",
      "TRAIN: EPOCH 25/100 | BATCH 62/71 | LOSS: 0.00013820769723563912\n",
      "TRAIN: EPOCH 25/100 | BATCH 63/71 | LOSS: 0.00013830543412041152\n",
      "TRAIN: EPOCH 25/100 | BATCH 64/71 | LOSS: 0.00013785934178695943\n",
      "TRAIN: EPOCH 25/100 | BATCH 65/71 | LOSS: 0.00013869115090611476\n",
      "TRAIN: EPOCH 25/100 | BATCH 66/71 | LOSS: 0.0001384272256826134\n",
      "TRAIN: EPOCH 25/100 | BATCH 67/71 | LOSS: 0.00013838847660415065\n",
      "TRAIN: EPOCH 25/100 | BATCH 68/71 | LOSS: 0.00013795568868416646\n",
      "TRAIN: EPOCH 25/100 | BATCH 69/71 | LOSS: 0.00013757972389742332\n",
      "TRAIN: EPOCH 25/100 | BATCH 70/71 | LOSS: 0.00013673740705970387\n",
      "VAL: EPOCH 25/100 | BATCH 0/8 | LOSS: 0.00012325510033406317\n",
      "VAL: EPOCH 25/100 | BATCH 1/8 | LOSS: 0.00011418657231843099\n",
      "VAL: EPOCH 25/100 | BATCH 2/8 | LOSS: 0.00011241722677368671\n",
      "VAL: EPOCH 25/100 | BATCH 3/8 | LOSS: 0.00012214072194183245\n",
      "VAL: EPOCH 25/100 | BATCH 4/8 | LOSS: 0.00011633637332124636\n",
      "VAL: EPOCH 25/100 | BATCH 5/8 | LOSS: 0.00011492472549434751\n",
      "VAL: EPOCH 25/100 | BATCH 6/8 | LOSS: 0.00011860172214385654\n",
      "VAL: EPOCH 25/100 | BATCH 7/8 | LOSS: 0.00011389500014047371\n",
      "TRAIN: EPOCH 26/100 | BATCH 0/71 | LOSS: 0.00020768465765286237\n",
      "TRAIN: EPOCH 26/100 | BATCH 1/71 | LOSS: 0.00015373990027001128\n",
      "TRAIN: EPOCH 26/100 | BATCH 2/71 | LOSS: 0.00013775081606581807\n",
      "TRAIN: EPOCH 26/100 | BATCH 3/71 | LOSS: 0.00013277465041028336\n",
      "TRAIN: EPOCH 26/100 | BATCH 4/71 | LOSS: 0.00013419080059975385\n",
      "TRAIN: EPOCH 26/100 | BATCH 5/71 | LOSS: 0.00012989712073855722\n",
      "TRAIN: EPOCH 26/100 | BATCH 6/71 | LOSS: 0.00012722997260945185\n",
      "TRAIN: EPOCH 26/100 | BATCH 7/71 | LOSS: 0.0001289976244152058\n",
      "TRAIN: EPOCH 26/100 | BATCH 8/71 | LOSS: 0.0001325735583021823\n",
      "TRAIN: EPOCH 26/100 | BATCH 9/71 | LOSS: 0.00012876064429292455\n",
      "TRAIN: EPOCH 26/100 | BATCH 10/71 | LOSS: 0.000127319104318634\n",
      "TRAIN: EPOCH 26/100 | BATCH 11/71 | LOSS: 0.00012402980246406514\n",
      "TRAIN: EPOCH 26/100 | BATCH 12/71 | LOSS: 0.00012250098874434255\n",
      "TRAIN: EPOCH 26/100 | BATCH 13/71 | LOSS: 0.00012031836480933375\n",
      "TRAIN: EPOCH 26/100 | BATCH 14/71 | LOSS: 0.00011957945486453051\n",
      "TRAIN: EPOCH 26/100 | BATCH 15/71 | LOSS: 0.00012181956981294206\n",
      "TRAIN: EPOCH 26/100 | BATCH 16/71 | LOSS: 0.00012530696104102603\n",
      "TRAIN: EPOCH 26/100 | BATCH 17/71 | LOSS: 0.0001250626959922051\n",
      "TRAIN: EPOCH 26/100 | BATCH 18/71 | LOSS: 0.00012558011076180264\n",
      "TRAIN: EPOCH 26/100 | BATCH 19/71 | LOSS: 0.00012573790772876236\n",
      "TRAIN: EPOCH 26/100 | BATCH 20/71 | LOSS: 0.00012345898340711194\n",
      "TRAIN: EPOCH 26/100 | BATCH 21/71 | LOSS: 0.0001299474186618516\n",
      "TRAIN: EPOCH 26/100 | BATCH 22/71 | LOSS: 0.0001296014936246059\n",
      "TRAIN: EPOCH 26/100 | BATCH 23/71 | LOSS: 0.00012907574273413047\n",
      "TRAIN: EPOCH 26/100 | BATCH 24/71 | LOSS: 0.000128720196953509\n",
      "TRAIN: EPOCH 26/100 | BATCH 25/71 | LOSS: 0.0001286728390998458\n",
      "TRAIN: EPOCH 26/100 | BATCH 26/71 | LOSS: 0.00013190105204961987\n",
      "TRAIN: EPOCH 26/100 | BATCH 27/71 | LOSS: 0.00012973621908583612\n",
      "TRAIN: EPOCH 26/100 | BATCH 28/71 | LOSS: 0.00012908188088251086\n",
      "TRAIN: EPOCH 26/100 | BATCH 29/71 | LOSS: 0.00012790043886828545\n",
      "TRAIN: EPOCH 26/100 | BATCH 30/71 | LOSS: 0.00012674009626460894\n",
      "TRAIN: EPOCH 26/100 | BATCH 31/71 | LOSS: 0.0001269720423806575\n",
      "TRAIN: EPOCH 26/100 | BATCH 32/71 | LOSS: 0.0001274626856408731\n",
      "TRAIN: EPOCH 26/100 | BATCH 33/71 | LOSS: 0.00012653259845512153\n",
      "TRAIN: EPOCH 26/100 | BATCH 34/71 | LOSS: 0.0001265007015068217\n",
      "TRAIN: EPOCH 26/100 | BATCH 35/71 | LOSS: 0.00012524731189639878\n",
      "TRAIN: EPOCH 26/100 | BATCH 36/71 | LOSS: 0.00012747976025157432\n",
      "TRAIN: EPOCH 26/100 | BATCH 37/71 | LOSS: 0.00012643776522293737\n",
      "TRAIN: EPOCH 26/100 | BATCH 38/71 | LOSS: 0.00012628707284024224\n",
      "TRAIN: EPOCH 26/100 | BATCH 39/71 | LOSS: 0.0001276856582990149\n",
      "TRAIN: EPOCH 26/100 | BATCH 40/71 | LOSS: 0.00012807253032650162\n",
      "TRAIN: EPOCH 26/100 | BATCH 41/71 | LOSS: 0.00012829577670865028\n",
      "TRAIN: EPOCH 26/100 | BATCH 42/71 | LOSS: 0.000128212176414943\n",
      "TRAIN: EPOCH 26/100 | BATCH 43/71 | LOSS: 0.00012907067965980704\n",
      "TRAIN: EPOCH 26/100 | BATCH 44/71 | LOSS: 0.00012818494061422016\n",
      "TRAIN: EPOCH 26/100 | BATCH 45/71 | LOSS: 0.000128045808315105\n",
      "TRAIN: EPOCH 26/100 | BATCH 46/71 | LOSS: 0.00012776363400142997\n",
      "TRAIN: EPOCH 26/100 | BATCH 47/71 | LOSS: 0.0001278798489086815\n",
      "TRAIN: EPOCH 26/100 | BATCH 48/71 | LOSS: 0.0001281352874963088\n",
      "TRAIN: EPOCH 26/100 | BATCH 49/71 | LOSS: 0.0001276934782799799\n",
      "TRAIN: EPOCH 26/100 | BATCH 50/71 | LOSS: 0.00012687107892798296\n",
      "TRAIN: EPOCH 26/100 | BATCH 51/71 | LOSS: 0.00012685704244246992\n",
      "TRAIN: EPOCH 26/100 | BATCH 52/71 | LOSS: 0.0001262788577631274\n",
      "TRAIN: EPOCH 26/100 | BATCH 53/71 | LOSS: 0.00012601363643490777\n",
      "TRAIN: EPOCH 26/100 | BATCH 54/71 | LOSS: 0.0001262341156358492\n",
      "TRAIN: EPOCH 26/100 | BATCH 55/71 | LOSS: 0.00012643945865420392\n",
      "TRAIN: EPOCH 26/100 | BATCH 56/71 | LOSS: 0.0001255522538007139\n",
      "TRAIN: EPOCH 26/100 | BATCH 57/71 | LOSS: 0.00012522639121141703\n",
      "TRAIN: EPOCH 26/100 | BATCH 58/71 | LOSS: 0.0001268274599925015\n",
      "TRAIN: EPOCH 26/100 | BATCH 59/71 | LOSS: 0.00012721906811445174\n",
      "TRAIN: EPOCH 26/100 | BATCH 60/71 | LOSS: 0.00012707986048762456\n",
      "TRAIN: EPOCH 26/100 | BATCH 61/71 | LOSS: 0.00012753800957955632\n",
      "TRAIN: EPOCH 26/100 | BATCH 62/71 | LOSS: 0.00012786130733614314\n",
      "TRAIN: EPOCH 26/100 | BATCH 63/71 | LOSS: 0.00012809266547719744\n",
      "TRAIN: EPOCH 26/100 | BATCH 64/71 | LOSS: 0.0001284781090082386\n",
      "TRAIN: EPOCH 26/100 | BATCH 65/71 | LOSS: 0.0001281328253698479\n",
      "TRAIN: EPOCH 26/100 | BATCH 66/71 | LOSS: 0.0001279388000769888\n",
      "TRAIN: EPOCH 26/100 | BATCH 67/71 | LOSS: 0.00012755538998188807\n",
      "TRAIN: EPOCH 26/100 | BATCH 68/71 | LOSS: 0.00012717179355525371\n",
      "TRAIN: EPOCH 26/100 | BATCH 69/71 | LOSS: 0.0001268343306687062\n",
      "TRAIN: EPOCH 26/100 | BATCH 70/71 | LOSS: 0.0001262204804987332\n",
      "VAL: EPOCH 26/100 | BATCH 0/8 | LOSS: 0.00010939491039607674\n",
      "VAL: EPOCH 26/100 | BATCH 1/8 | LOSS: 0.00010312735685147345\n",
      "VAL: EPOCH 26/100 | BATCH 2/8 | LOSS: 0.00010204555292148143\n",
      "VAL: EPOCH 26/100 | BATCH 3/8 | LOSS: 0.00011114505105069838\n",
      "VAL: EPOCH 26/100 | BATCH 4/8 | LOSS: 0.00010534565662965179\n",
      "VAL: EPOCH 26/100 | BATCH 5/8 | LOSS: 0.00010472993502238144\n",
      "VAL: EPOCH 26/100 | BATCH 6/8 | LOSS: 0.00010791705322584935\n",
      "VAL: EPOCH 26/100 | BATCH 7/8 | LOSS: 0.00010373908025940182\n",
      "TRAIN: EPOCH 27/100 | BATCH 0/71 | LOSS: 0.0001128204312408343\n",
      "TRAIN: EPOCH 27/100 | BATCH 1/71 | LOSS: 9.607616448192857e-05\n",
      "TRAIN: EPOCH 27/100 | BATCH 2/71 | LOSS: 0.00011334272130625322\n",
      "TRAIN: EPOCH 27/100 | BATCH 3/71 | LOSS: 0.00012185553168819752\n",
      "TRAIN: EPOCH 27/100 | BATCH 4/71 | LOSS: 0.00011464323033578694\n",
      "TRAIN: EPOCH 27/100 | BATCH 5/71 | LOSS: 0.0001270592911168933\n",
      "TRAIN: EPOCH 27/100 | BATCH 6/71 | LOSS: 0.00013125472677139833\n",
      "TRAIN: EPOCH 27/100 | BATCH 7/71 | LOSS: 0.00013131641571817454\n",
      "TRAIN: EPOCH 27/100 | BATCH 8/71 | LOSS: 0.00012767384436705874\n",
      "TRAIN: EPOCH 27/100 | BATCH 9/71 | LOSS: 0.000125296285841614\n",
      "TRAIN: EPOCH 27/100 | BATCH 10/71 | LOSS: 0.00012387840781064534\n",
      "TRAIN: EPOCH 27/100 | BATCH 11/71 | LOSS: 0.00012134282102730747\n",
      "TRAIN: EPOCH 27/100 | BATCH 12/71 | LOSS: 0.00012078321070965522\n",
      "TRAIN: EPOCH 27/100 | BATCH 13/71 | LOSS: 0.00011982570816014362\n",
      "TRAIN: EPOCH 27/100 | BATCH 14/71 | LOSS: 0.00011714046268025413\n",
      "TRAIN: EPOCH 27/100 | BATCH 15/71 | LOSS: 0.00011555383798622643\n",
      "TRAIN: EPOCH 27/100 | BATCH 16/71 | LOSS: 0.00011395515064256924\n",
      "TRAIN: EPOCH 27/100 | BATCH 17/71 | LOSS: 0.00011319048038179365\n",
      "TRAIN: EPOCH 27/100 | BATCH 18/71 | LOSS: 0.00011298202145170715\n",
      "TRAIN: EPOCH 27/100 | BATCH 19/71 | LOSS: 0.00011080685326305683\n",
      "TRAIN: EPOCH 27/100 | BATCH 20/71 | LOSS: 0.00011024851777446678\n",
      "TRAIN: EPOCH 27/100 | BATCH 21/71 | LOSS: 0.00011622865481926551\n",
      "TRAIN: EPOCH 27/100 | BATCH 22/71 | LOSS: 0.00011567332757808997\n",
      "TRAIN: EPOCH 27/100 | BATCH 23/71 | LOSS: 0.00011747717599064345\n",
      "TRAIN: EPOCH 27/100 | BATCH 24/71 | LOSS: 0.00012312321778154\n",
      "TRAIN: EPOCH 27/100 | BATCH 25/71 | LOSS: 0.000122650432156381\n",
      "TRAIN: EPOCH 27/100 | BATCH 26/71 | LOSS: 0.00012161701710687744\n",
      "TRAIN: EPOCH 27/100 | BATCH 27/71 | LOSS: 0.00012347156578990898\n",
      "TRAIN: EPOCH 27/100 | BATCH 28/71 | LOSS: 0.00012441705972566429\n",
      "TRAIN: EPOCH 27/100 | BATCH 29/71 | LOSS: 0.0001233970853112017\n",
      "TRAIN: EPOCH 27/100 | BATCH 30/71 | LOSS: 0.00012259538033826937\n",
      "TRAIN: EPOCH 27/100 | BATCH 31/71 | LOSS: 0.00012206426094962808\n",
      "TRAIN: EPOCH 27/100 | BATCH 32/71 | LOSS: 0.0001222210308752079\n",
      "TRAIN: EPOCH 27/100 | BATCH 33/71 | LOSS: 0.00012089344925377244\n",
      "TRAIN: EPOCH 27/100 | BATCH 34/71 | LOSS: 0.0001203374789578707\n",
      "TRAIN: EPOCH 27/100 | BATCH 35/71 | LOSS: 0.00011896114443111906\n",
      "TRAIN: EPOCH 27/100 | BATCH 36/71 | LOSS: 0.00011943020591216564\n",
      "TRAIN: EPOCH 27/100 | BATCH 37/71 | LOSS: 0.00011993136573245895\n",
      "TRAIN: EPOCH 27/100 | BATCH 38/71 | LOSS: 0.00011941987908451269\n",
      "TRAIN: EPOCH 27/100 | BATCH 39/71 | LOSS: 0.00011918961572519038\n",
      "TRAIN: EPOCH 27/100 | BATCH 40/71 | LOSS: 0.00011839550421516434\n",
      "TRAIN: EPOCH 27/100 | BATCH 41/71 | LOSS: 0.0001184082024597696\n",
      "TRAIN: EPOCH 27/100 | BATCH 42/71 | LOSS: 0.00011824453953480305\n",
      "TRAIN: EPOCH 27/100 | BATCH 43/71 | LOSS: 0.00011766286181889221\n",
      "TRAIN: EPOCH 27/100 | BATCH 44/71 | LOSS: 0.00011702342922540589\n",
      "TRAIN: EPOCH 27/100 | BATCH 45/71 | LOSS: 0.00011756905176140287\n",
      "TRAIN: EPOCH 27/100 | BATCH 46/71 | LOSS: 0.00011723703473182197\n",
      "TRAIN: EPOCH 27/100 | BATCH 47/71 | LOSS: 0.00011705940778483637\n",
      "TRAIN: EPOCH 27/100 | BATCH 48/71 | LOSS: 0.00011744097704352925\n",
      "TRAIN: EPOCH 27/100 | BATCH 49/71 | LOSS: 0.00011734334839275107\n",
      "TRAIN: EPOCH 27/100 | BATCH 50/71 | LOSS: 0.00011834321405473805\n",
      "TRAIN: EPOCH 27/100 | BATCH 51/71 | LOSS: 0.00011903791164513677\n",
      "TRAIN: EPOCH 27/100 | BATCH 52/71 | LOSS: 0.00011838976110393217\n",
      "TRAIN: EPOCH 27/100 | BATCH 53/71 | LOSS: 0.0001195092419813456\n",
      "TRAIN: EPOCH 27/100 | BATCH 54/71 | LOSS: 0.00011932410786605695\n",
      "TRAIN: EPOCH 27/100 | BATCH 55/71 | LOSS: 0.00011949144079283412\n",
      "TRAIN: EPOCH 27/100 | BATCH 56/71 | LOSS: 0.00011980527166375204\n",
      "TRAIN: EPOCH 27/100 | BATCH 57/71 | LOSS: 0.0001199485341203399\n",
      "TRAIN: EPOCH 27/100 | BATCH 58/71 | LOSS: 0.00011998839785889485\n",
      "TRAIN: EPOCH 27/100 | BATCH 59/71 | LOSS: 0.00011964440382143948\n",
      "TRAIN: EPOCH 27/100 | BATCH 60/71 | LOSS: 0.00012017753116927704\n",
      "TRAIN: EPOCH 27/100 | BATCH 61/71 | LOSS: 0.00011946629855364952\n",
      "TRAIN: EPOCH 27/100 | BATCH 62/71 | LOSS: 0.00011872318709575911\n",
      "TRAIN: EPOCH 27/100 | BATCH 63/71 | LOSS: 0.00011845274491406599\n",
      "TRAIN: EPOCH 27/100 | BATCH 64/71 | LOSS: 0.00011816332329404899\n",
      "TRAIN: EPOCH 27/100 | BATCH 65/71 | LOSS: 0.00011825119486964081\n",
      "TRAIN: EPOCH 27/100 | BATCH 66/71 | LOSS: 0.00011836082525568695\n",
      "TRAIN: EPOCH 27/100 | BATCH 67/71 | LOSS: 0.00011793812261791626\n",
      "TRAIN: EPOCH 27/100 | BATCH 68/71 | LOSS: 0.00011795448823460796\n",
      "TRAIN: EPOCH 27/100 | BATCH 69/71 | LOSS: 0.00011752699158803027\n",
      "TRAIN: EPOCH 27/100 | BATCH 70/71 | LOSS: 0.00011848500844920842\n",
      "VAL: EPOCH 27/100 | BATCH 0/8 | LOSS: 0.00010948666022159159\n",
      "VAL: EPOCH 27/100 | BATCH 1/8 | LOSS: 9.887085616355762e-05\n",
      "VAL: EPOCH 27/100 | BATCH 2/8 | LOSS: 9.678135150655483e-05\n",
      "VAL: EPOCH 27/100 | BATCH 3/8 | LOSS: 0.000104722113974276\n",
      "VAL: EPOCH 27/100 | BATCH 4/8 | LOSS: 9.911456581903621e-05\n",
      "VAL: EPOCH 27/100 | BATCH 5/8 | LOSS: 9.79315615647162e-05\n",
      "VAL: EPOCH 27/100 | BATCH 6/8 | LOSS: 0.00010072408622363582\n",
      "VAL: EPOCH 27/100 | BATCH 7/8 | LOSS: 9.707474873721367e-05\n",
      "TRAIN: EPOCH 28/100 | BATCH 0/71 | LOSS: 0.00016936691827140749\n",
      "TRAIN: EPOCH 28/100 | BATCH 1/71 | LOSS: 0.00012860348215326667\n",
      "TRAIN: EPOCH 28/100 | BATCH 2/71 | LOSS: 0.0001157043637552609\n",
      "TRAIN: EPOCH 28/100 | BATCH 3/71 | LOSS: 0.00010788400868477765\n",
      "TRAIN: EPOCH 28/100 | BATCH 4/71 | LOSS: 0.0001358789115329273\n",
      "TRAIN: EPOCH 28/100 | BATCH 5/71 | LOSS: 0.00012684941369419297\n",
      "TRAIN: EPOCH 28/100 | BATCH 6/71 | LOSS: 0.00011773447477025911\n",
      "TRAIN: EPOCH 28/100 | BATCH 7/71 | LOSS: 0.00011926156457775505\n",
      "TRAIN: EPOCH 28/100 | BATCH 8/71 | LOSS: 0.0001162210303138838\n",
      "TRAIN: EPOCH 28/100 | BATCH 9/71 | LOSS: 0.00011269703027210199\n",
      "TRAIN: EPOCH 28/100 | BATCH 10/71 | LOSS: 0.00011578041978116909\n",
      "TRAIN: EPOCH 28/100 | BATCH 11/71 | LOSS: 0.00011443166658864357\n",
      "TRAIN: EPOCH 28/100 | BATCH 12/71 | LOSS: 0.00011719953349361625\n",
      "TRAIN: EPOCH 28/100 | BATCH 13/71 | LOSS: 0.0001166404778944395\n",
      "TRAIN: EPOCH 28/100 | BATCH 14/71 | LOSS: 0.00011558952925649161\n",
      "TRAIN: EPOCH 28/100 | BATCH 15/71 | LOSS: 0.00011416775669204071\n",
      "TRAIN: EPOCH 28/100 | BATCH 16/71 | LOSS: 0.00011404228537096915\n",
      "TRAIN: EPOCH 28/100 | BATCH 17/71 | LOSS: 0.00011190063459151942\n",
      "TRAIN: EPOCH 28/100 | BATCH 18/71 | LOSS: 0.00011129880677847388\n",
      "TRAIN: EPOCH 28/100 | BATCH 19/71 | LOSS: 0.00011401516203477514\n",
      "TRAIN: EPOCH 28/100 | BATCH 20/71 | LOSS: 0.00011351617909635284\n",
      "TRAIN: EPOCH 28/100 | BATCH 21/71 | LOSS: 0.00011282965450928631\n",
      "TRAIN: EPOCH 28/100 | BATCH 22/71 | LOSS: 0.00011295918470157473\n",
      "TRAIN: EPOCH 28/100 | BATCH 23/71 | LOSS: 0.00011109812263991141\n",
      "TRAIN: EPOCH 28/100 | BATCH 24/71 | LOSS: 0.00011253684904659167\n",
      "TRAIN: EPOCH 28/100 | BATCH 25/71 | LOSS: 0.00011525625758132755\n",
      "TRAIN: EPOCH 28/100 | BATCH 26/71 | LOSS: 0.00011654108082573792\n",
      "TRAIN: EPOCH 28/100 | BATCH 27/71 | LOSS: 0.00011563236148504075\n",
      "TRAIN: EPOCH 28/100 | BATCH 28/71 | LOSS: 0.00011478721240439035\n",
      "TRAIN: EPOCH 28/100 | BATCH 29/71 | LOSS: 0.0001135557113836209\n",
      "TRAIN: EPOCH 28/100 | BATCH 30/71 | LOSS: 0.00011313768810660187\n",
      "TRAIN: EPOCH 28/100 | BATCH 31/71 | LOSS: 0.00011376579846000823\n",
      "TRAIN: EPOCH 28/100 | BATCH 32/71 | LOSS: 0.00011272132939087565\n",
      "TRAIN: EPOCH 28/100 | BATCH 33/71 | LOSS: 0.00011212001481908374\n",
      "TRAIN: EPOCH 28/100 | BATCH 34/71 | LOSS: 0.00011140553916837754\n",
      "TRAIN: EPOCH 28/100 | BATCH 35/71 | LOSS: 0.00011172878678027903\n",
      "TRAIN: EPOCH 28/100 | BATCH 36/71 | LOSS: 0.00011095839392903538\n",
      "TRAIN: EPOCH 28/100 | BATCH 37/71 | LOSS: 0.00011013100236979019\n",
      "TRAIN: EPOCH 28/100 | BATCH 38/71 | LOSS: 0.00010950626146484118\n",
      "TRAIN: EPOCH 28/100 | BATCH 39/71 | LOSS: 0.00010981375598930753\n",
      "TRAIN: EPOCH 28/100 | BATCH 40/71 | LOSS: 0.00010954774348292408\n",
      "TRAIN: EPOCH 28/100 | BATCH 41/71 | LOSS: 0.00010889082200481512\n",
      "TRAIN: EPOCH 28/100 | BATCH 42/71 | LOSS: 0.00010829122918441371\n",
      "TRAIN: EPOCH 28/100 | BATCH 43/71 | LOSS: 0.00010841363804171455\n",
      "TRAIN: EPOCH 28/100 | BATCH 44/71 | LOSS: 0.00011022232809207506\n",
      "TRAIN: EPOCH 28/100 | BATCH 45/71 | LOSS: 0.00010932828666662554\n",
      "TRAIN: EPOCH 28/100 | BATCH 46/71 | LOSS: 0.00010851107753238621\n",
      "TRAIN: EPOCH 28/100 | BATCH 47/71 | LOSS: 0.00010885041198586502\n",
      "TRAIN: EPOCH 28/100 | BATCH 48/71 | LOSS: 0.00010936624609285547\n",
      "TRAIN: EPOCH 28/100 | BATCH 49/71 | LOSS: 0.00011002152547007427\n",
      "TRAIN: EPOCH 28/100 | BATCH 50/71 | LOSS: 0.00011128069080786743\n",
      "TRAIN: EPOCH 28/100 | BATCH 51/71 | LOSS: 0.0001104165720999635\n",
      "TRAIN: EPOCH 28/100 | BATCH 52/71 | LOSS: 0.00011005581184707687\n",
      "TRAIN: EPOCH 28/100 | BATCH 53/71 | LOSS: 0.00011001521300015695\n",
      "TRAIN: EPOCH 28/100 | BATCH 54/71 | LOSS: 0.00010938707855530083\n",
      "TRAIN: EPOCH 28/100 | BATCH 55/71 | LOSS: 0.00011081368627076569\n",
      "TRAIN: EPOCH 28/100 | BATCH 56/71 | LOSS: 0.00011063922934944888\n",
      "TRAIN: EPOCH 28/100 | BATCH 57/71 | LOSS: 0.0001109766812517372\n",
      "TRAIN: EPOCH 28/100 | BATCH 58/71 | LOSS: 0.0001107225432830184\n",
      "TRAIN: EPOCH 28/100 | BATCH 59/71 | LOSS: 0.00011143304509459994\n",
      "TRAIN: EPOCH 28/100 | BATCH 60/71 | LOSS: 0.00011108453446602235\n",
      "TRAIN: EPOCH 28/100 | BATCH 61/71 | LOSS: 0.00011076113948928974\n",
      "TRAIN: EPOCH 28/100 | BATCH 62/71 | LOSS: 0.00011029912298360455\n",
      "TRAIN: EPOCH 28/100 | BATCH 63/71 | LOSS: 0.0001105702428958466\n",
      "TRAIN: EPOCH 28/100 | BATCH 64/71 | LOSS: 0.00011041090899827675\n",
      "TRAIN: EPOCH 28/100 | BATCH 65/71 | LOSS: 0.00011028039752449276\n",
      "TRAIN: EPOCH 28/100 | BATCH 66/71 | LOSS: 0.00011029107662946431\n",
      "TRAIN: EPOCH 28/100 | BATCH 67/71 | LOSS: 0.0001097420450498569\n",
      "TRAIN: EPOCH 28/100 | BATCH 68/71 | LOSS: 0.00010927293792981115\n",
      "TRAIN: EPOCH 28/100 | BATCH 69/71 | LOSS: 0.00010880351609167909\n",
      "TRAIN: EPOCH 28/100 | BATCH 70/71 | LOSS: 0.00010866096528561813\n",
      "VAL: EPOCH 28/100 | BATCH 0/8 | LOSS: 0.00010108856076840311\n",
      "VAL: EPOCH 28/100 | BATCH 1/8 | LOSS: 9.078445145860314e-05\n",
      "VAL: EPOCH 28/100 | BATCH 2/8 | LOSS: 8.970953058451414e-05\n",
      "VAL: EPOCH 28/100 | BATCH 3/8 | LOSS: 9.841632709139958e-05\n",
      "VAL: EPOCH 28/100 | BATCH 4/8 | LOSS: 9.27772605791688e-05\n",
      "VAL: EPOCH 28/100 | BATCH 5/8 | LOSS: 9.133539424510673e-05\n",
      "VAL: EPOCH 28/100 | BATCH 6/8 | LOSS: 9.338922141718545e-05\n",
      "VAL: EPOCH 28/100 | BATCH 7/8 | LOSS: 9.000076443044236e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 0/71 | LOSS: 9.746516298037022e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 1/71 | LOSS: 8.514177170582116e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 2/71 | LOSS: 0.00010974339481132726\n",
      "TRAIN: EPOCH 29/100 | BATCH 3/71 | LOSS: 0.0001253609443665482\n",
      "TRAIN: EPOCH 29/100 | BATCH 4/71 | LOSS: 0.00011996347748208792\n",
      "TRAIN: EPOCH 29/100 | BATCH 5/71 | LOSS: 0.00011196265647110219\n",
      "TRAIN: EPOCH 29/100 | BATCH 6/71 | LOSS: 0.00011329168981839237\n",
      "TRAIN: EPOCH 29/100 | BATCH 7/71 | LOSS: 0.00010894575552811148\n",
      "TRAIN: EPOCH 29/100 | BATCH 8/71 | LOSS: 0.00010700728469398907\n",
      "TRAIN: EPOCH 29/100 | BATCH 9/71 | LOSS: 0.00010186805411649402\n",
      "TRAIN: EPOCH 29/100 | BATCH 10/71 | LOSS: 9.938595460632563e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 11/71 | LOSS: 0.00010077377464767778\n",
      "TRAIN: EPOCH 29/100 | BATCH 12/71 | LOSS: 9.806495458738376e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 13/71 | LOSS: 9.76539098961179e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 14/71 | LOSS: 0.00010011803654682201\n",
      "TRAIN: EPOCH 29/100 | BATCH 15/71 | LOSS: 9.806753428165393e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 16/71 | LOSS: 9.748617915594128e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 17/71 | LOSS: 9.657207697778681e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 18/71 | LOSS: 9.642748179765859e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 19/71 | LOSS: 9.578486333339242e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 20/71 | LOSS: 9.70986021413756e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 21/71 | LOSS: 9.959840148936068e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 22/71 | LOSS: 0.00010226708479382543\n",
      "TRAIN: EPOCH 29/100 | BATCH 23/71 | LOSS: 0.00010158796718921319\n",
      "TRAIN: EPOCH 29/100 | BATCH 24/71 | LOSS: 0.00010037695450591855\n",
      "TRAIN: EPOCH 29/100 | BATCH 25/71 | LOSS: 9.968683144297056e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 26/71 | LOSS: 0.00010321339707136051\n",
      "TRAIN: EPOCH 29/100 | BATCH 27/71 | LOSS: 0.00010211316852551786\n",
      "TRAIN: EPOCH 29/100 | BATCH 28/71 | LOSS: 0.0001028126482259691\n",
      "TRAIN: EPOCH 29/100 | BATCH 29/71 | LOSS: 0.00010204213528292409\n",
      "TRAIN: EPOCH 29/100 | BATCH 30/71 | LOSS: 0.00010112309117234432\n",
      "TRAIN: EPOCH 29/100 | BATCH 31/71 | LOSS: 0.00010031893373252387\n",
      "TRAIN: EPOCH 29/100 | BATCH 32/71 | LOSS: 9.976142973755486e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 33/71 | LOSS: 0.00010166620343036757\n",
      "TRAIN: EPOCH 29/100 | BATCH 34/71 | LOSS: 0.00010077501345741828\n",
      "TRAIN: EPOCH 29/100 | BATCH 35/71 | LOSS: 0.00010010571279255803\n",
      "TRAIN: EPOCH 29/100 | BATCH 36/71 | LOSS: 0.00010030087808420806\n",
      "TRAIN: EPOCH 29/100 | BATCH 37/71 | LOSS: 0.0001002718788521107\n",
      "TRAIN: EPOCH 29/100 | BATCH 38/71 | LOSS: 9.988439565999075e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 39/71 | LOSS: 9.959389089999603e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 40/71 | LOSS: 9.978077420184003e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 41/71 | LOSS: 9.965104142057022e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 42/71 | LOSS: 9.943039092535781e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 43/71 | LOSS: 9.940349784962812e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 44/71 | LOSS: 9.939733063220046e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 45/71 | LOSS: 9.887834968812176e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 46/71 | LOSS: 9.838626229271114e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 47/71 | LOSS: 9.847460864875757e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 48/71 | LOSS: 9.801191043486458e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 49/71 | LOSS: 9.755631494044792e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 50/71 | LOSS: 9.998887902589095e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 51/71 | LOSS: 0.00010064132437456955\n",
      "TRAIN: EPOCH 29/100 | BATCH 52/71 | LOSS: 0.00010025897125091913\n",
      "TRAIN: EPOCH 29/100 | BATCH 53/71 | LOSS: 0.00010131345314018997\n",
      "TRAIN: EPOCH 29/100 | BATCH 54/71 | LOSS: 0.00010085777936513874\n",
      "TRAIN: EPOCH 29/100 | BATCH 55/71 | LOSS: 0.00010028931027201387\n",
      "TRAIN: EPOCH 29/100 | BATCH 56/71 | LOSS: 0.00010019451209493063\n",
      "TRAIN: EPOCH 29/100 | BATCH 57/71 | LOSS: 0.00010165088597518504\n",
      "TRAIN: EPOCH 29/100 | BATCH 58/71 | LOSS: 0.00010134127000652616\n",
      "TRAIN: EPOCH 29/100 | BATCH 59/71 | LOSS: 0.00010080433573117868\n",
      "TRAIN: EPOCH 29/100 | BATCH 60/71 | LOSS: 0.00010067353782016921\n",
      "TRAIN: EPOCH 29/100 | BATCH 61/71 | LOSS: 0.00010001355545799953\n",
      "TRAIN: EPOCH 29/100 | BATCH 62/71 | LOSS: 0.0001004466428148878\n",
      "TRAIN: EPOCH 29/100 | BATCH 63/71 | LOSS: 0.00010068227226156523\n",
      "TRAIN: EPOCH 29/100 | BATCH 64/71 | LOSS: 0.00010066273227074326\n",
      "TRAIN: EPOCH 29/100 | BATCH 65/71 | LOSS: 0.0001004312034086924\n",
      "TRAIN: EPOCH 29/100 | BATCH 66/71 | LOSS: 0.00010012444605010062\n",
      "TRAIN: EPOCH 29/100 | BATCH 67/71 | LOSS: 9.972836612766568e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 68/71 | LOSS: 9.96157564193287e-05\n",
      "TRAIN: EPOCH 29/100 | BATCH 69/71 | LOSS: 0.00010041148889285977\n",
      "TRAIN: EPOCH 29/100 | BATCH 70/71 | LOSS: 9.984153116484252e-05\n",
      "VAL: EPOCH 29/100 | BATCH 0/8 | LOSS: 9.146383672486991e-05\n",
      "VAL: EPOCH 29/100 | BATCH 1/8 | LOSS: 8.282859926111996e-05\n",
      "VAL: EPOCH 29/100 | BATCH 2/8 | LOSS: 8.170941873686388e-05\n",
      "VAL: EPOCH 29/100 | BATCH 3/8 | LOSS: 8.898641499399673e-05\n",
      "VAL: EPOCH 29/100 | BATCH 4/8 | LOSS: 8.393473835894838e-05\n",
      "VAL: EPOCH 29/100 | BATCH 5/8 | LOSS: 8.347066977876239e-05\n",
      "VAL: EPOCH 29/100 | BATCH 6/8 | LOSS: 8.571270362673593e-05\n",
      "VAL: EPOCH 29/100 | BATCH 7/8 | LOSS: 8.297860404127277e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 0/71 | LOSS: 0.0001073592939064838\n",
      "TRAIN: EPOCH 30/100 | BATCH 1/71 | LOSS: 0.00010751248191809282\n",
      "TRAIN: EPOCH 30/100 | BATCH 2/71 | LOSS: 0.00011239146018245567\n",
      "TRAIN: EPOCH 30/100 | BATCH 3/71 | LOSS: 0.00011328300024615601\n",
      "TRAIN: EPOCH 30/100 | BATCH 4/71 | LOSS: 0.00011360903445165605\n",
      "TRAIN: EPOCH 30/100 | BATCH 5/71 | LOSS: 0.00010932560083650363\n",
      "TRAIN: EPOCH 30/100 | BATCH 6/71 | LOSS: 0.00010523293687063935\n",
      "TRAIN: EPOCH 30/100 | BATCH 7/71 | LOSS: 9.93645453490899e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 8/71 | LOSS: 9.814615461639025e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 9/71 | LOSS: 9.557143566780723e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 10/71 | LOSS: 9.428152737779205e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 11/71 | LOSS: 9.157113890978508e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 12/71 | LOSS: 9.270649080952772e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 13/71 | LOSS: 9.711143710384411e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 14/71 | LOSS: 9.476326861962055e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 15/71 | LOSS: 9.387818909090129e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 16/71 | LOSS: 9.236061519272077e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 17/71 | LOSS: 9.304244304075837e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 18/71 | LOSS: 9.286920746825145e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 19/71 | LOSS: 9.258039099222515e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 20/71 | LOSS: 9.236643208645372e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 21/71 | LOSS: 9.243580445234494e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 22/71 | LOSS: 9.165832496997292e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 23/71 | LOSS: 9.26784941839287e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 24/71 | LOSS: 9.238427126547322e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 25/71 | LOSS: 9.368731088425893e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 26/71 | LOSS: 9.290203861719756e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 27/71 | LOSS: 9.210586826416797e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 28/71 | LOSS: 9.180752636768438e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 29/71 | LOSS: 9.161448300195237e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 30/71 | LOSS: 9.268349939731942e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 31/71 | LOSS: 9.440967141927104e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 32/71 | LOSS: 9.59888288476081e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 33/71 | LOSS: 9.513313831549193e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 34/71 | LOSS: 9.7238522097801e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 35/71 | LOSS: 9.673761148102737e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 36/71 | LOSS: 9.55686390060126e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 37/71 | LOSS: 9.52132169531021e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 38/71 | LOSS: 9.441901578938063e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 39/71 | LOSS: 9.467187301197555e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 40/71 | LOSS: 9.494467791760476e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 41/71 | LOSS: 9.549158563632296e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 42/71 | LOSS: 9.646051042837762e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 43/71 | LOSS: 9.606458628364966e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 44/71 | LOSS: 9.534775631942062e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 45/71 | LOSS: 9.465563625385782e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 46/71 | LOSS: 9.408035576269229e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 47/71 | LOSS: 9.358725962253327e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 48/71 | LOSS: 9.329546713959236e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 49/71 | LOSS: 9.434009858523495e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 50/71 | LOSS: 9.399614831565094e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 51/71 | LOSS: 9.367661004944239e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 52/71 | LOSS: 9.322734750073649e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 53/71 | LOSS: 9.241889313637296e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 54/71 | LOSS: 9.261087761842645e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 55/71 | LOSS: 9.210347518871589e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 56/71 | LOSS: 9.242937295838553e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 57/71 | LOSS: 9.246234442579062e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 58/71 | LOSS: 9.233403243805219e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 59/71 | LOSS: 9.232812377983161e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 60/71 | LOSS: 9.242168022462808e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 61/71 | LOSS: 9.286880272303543e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 62/71 | LOSS: 9.251023139869276e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 63/71 | LOSS: 9.203250436939925e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 64/71 | LOSS: 9.255742697860115e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 65/71 | LOSS: 9.240782426125158e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 66/71 | LOSS: 9.185598830652507e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 67/71 | LOSS: 9.16665997411816e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 68/71 | LOSS: 9.241717260379843e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 69/71 | LOSS: 9.257938212873082e-05\n",
      "TRAIN: EPOCH 30/100 | BATCH 70/71 | LOSS: 9.227056769714822e-05\n",
      "VAL: EPOCH 30/100 | BATCH 0/8 | LOSS: 8.997565601021051e-05\n",
      "VAL: EPOCH 30/100 | BATCH 1/8 | LOSS: 8.023618647712283e-05\n",
      "VAL: EPOCH 30/100 | BATCH 2/8 | LOSS: 7.832660776330158e-05\n",
      "VAL: EPOCH 30/100 | BATCH 3/8 | LOSS: 8.38604446471436e-05\n",
      "VAL: EPOCH 30/100 | BATCH 4/8 | LOSS: 7.878484029788524e-05\n",
      "VAL: EPOCH 30/100 | BATCH 5/8 | LOSS: 7.780415641415554e-05\n",
      "VAL: EPOCH 30/100 | BATCH 6/8 | LOSS: 7.983729821197423e-05\n",
      "VAL: EPOCH 30/100 | BATCH 7/8 | LOSS: 7.718302276771283e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 0/71 | LOSS: 7.861418271204457e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 1/71 | LOSS: 8.477338269585744e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 2/71 | LOSS: 8.08084102269883e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 3/71 | LOSS: 7.478736461052904e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 4/71 | LOSS: 7.97393404354807e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 5/71 | LOSS: 7.800056907096102e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 6/71 | LOSS: 7.870030978145743e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 7/71 | LOSS: 8.197643774110475e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 8/71 | LOSS: 8.024712870893482e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 9/71 | LOSS: 8.086584166449029e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 10/71 | LOSS: 8.094121684817682e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 11/71 | LOSS: 8.777524969142785e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 12/71 | LOSS: 9.014252310197872e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 13/71 | LOSS: 9.015083118616271e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 14/71 | LOSS: 9.075017369468697e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 15/71 | LOSS: 8.922959773371986e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 16/71 | LOSS: 8.902629184269565e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 17/71 | LOSS: 8.845988627904767e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 18/71 | LOSS: 8.708775318442157e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 19/71 | LOSS: 8.612932379037375e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 20/71 | LOSS: 8.609379351366355e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 21/71 | LOSS: 8.680239675000352e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 22/71 | LOSS: 8.591462439811869e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 23/71 | LOSS: 8.603382245079653e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 24/71 | LOSS: 8.538384587154724e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 25/71 | LOSS: 8.565317651888248e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 26/71 | LOSS: 8.480377234971254e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 27/71 | LOSS: 8.45376935103559e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 28/71 | LOSS: 8.40442402916559e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 29/71 | LOSS: 8.632868084532675e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 30/71 | LOSS: 8.688039639720603e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 31/71 | LOSS: 8.610815291376639e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 32/71 | LOSS: 8.633489503126563e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 33/71 | LOSS: 8.635448926618076e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 34/71 | LOSS: 8.808518416896862e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 35/71 | LOSS: 8.815610039568532e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 36/71 | LOSS: 8.792469305115299e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 37/71 | LOSS: 8.809296454465336e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 38/71 | LOSS: 8.814294685576804e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 39/71 | LOSS: 8.741508399907616e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 40/71 | LOSS: 8.867711203936602e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 41/71 | LOSS: 8.921478980656026e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 42/71 | LOSS: 8.853356899466192e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 43/71 | LOSS: 8.79996648357271e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 44/71 | LOSS: 8.83063691112006e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 45/71 | LOSS: 8.801840079325737e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 46/71 | LOSS: 8.853188866967375e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 47/71 | LOSS: 8.826363682601368e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 48/71 | LOSS: 8.883501111401474e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 49/71 | LOSS: 8.8218253076775e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 50/71 | LOSS: 8.749759153408162e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 51/71 | LOSS: 8.698694777054837e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 52/71 | LOSS: 8.622028395140385e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 53/71 | LOSS: 8.585536953533947e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 54/71 | LOSS: 8.581638348087753e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 55/71 | LOSS: 8.618310097777535e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 56/71 | LOSS: 8.628430837313972e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 57/71 | LOSS: 8.587057947869622e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 58/71 | LOSS: 8.598044661408432e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 59/71 | LOSS: 8.55443178807036e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 60/71 | LOSS: 8.629566170876372e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 61/71 | LOSS: 8.598403145433329e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 62/71 | LOSS: 8.606073444954948e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 63/71 | LOSS: 8.592177692889891e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 64/71 | LOSS: 8.538524388523701e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 65/71 | LOSS: 8.567779155837894e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 66/71 | LOSS: 8.513960319896117e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 67/71 | LOSS: 8.494032082286568e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 68/71 | LOSS: 8.519205685065506e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 69/71 | LOSS: 8.52169066222684e-05\n",
      "TRAIN: EPOCH 31/100 | BATCH 70/71 | LOSS: 8.519958784848019e-05\n",
      "VAL: EPOCH 31/100 | BATCH 0/8 | LOSS: 8.805027755443007e-05\n",
      "VAL: EPOCH 31/100 | BATCH 1/8 | LOSS: 7.691474456805736e-05\n",
      "VAL: EPOCH 31/100 | BATCH 2/8 | LOSS: 7.450503956836958e-05\n",
      "VAL: EPOCH 31/100 | BATCH 3/8 | LOSS: 7.871350862842519e-05\n",
      "VAL: EPOCH 31/100 | BATCH 4/8 | LOSS: 7.38107853976544e-05\n",
      "VAL: EPOCH 31/100 | BATCH 5/8 | LOSS: 7.266027205332648e-05\n",
      "VAL: EPOCH 31/100 | BATCH 6/8 | LOSS: 7.437033751297609e-05\n",
      "VAL: EPOCH 31/100 | BATCH 7/8 | LOSS: 7.228440017570392e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 0/71 | LOSS: 0.0001047875703079626\n",
      "TRAIN: EPOCH 32/100 | BATCH 1/71 | LOSS: 0.0001040565621224232\n",
      "TRAIN: EPOCH 32/100 | BATCH 2/71 | LOSS: 9.560086133812244e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 3/71 | LOSS: 8.888889169611502e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 4/71 | LOSS: 8.318704640259966e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 5/71 | LOSS: 8.551634406709734e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 6/71 | LOSS: 8.273554392092462e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 7/71 | LOSS: 7.90603462519357e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 8/71 | LOSS: 7.572980434310416e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 9/71 | LOSS: 7.43135988159338e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 10/71 | LOSS: 7.282023447756232e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 11/71 | LOSS: 7.268580217593505e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 12/71 | LOSS: 7.357910810522019e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 13/71 | LOSS: 7.704022937105037e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 14/71 | LOSS: 7.788508955854922e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 15/71 | LOSS: 7.73285955801839e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 16/71 | LOSS: 7.715860042748424e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 17/71 | LOSS: 7.692470535403118e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 18/71 | LOSS: 7.652070857339392e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 19/71 | LOSS: 7.588049411424435e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 20/71 | LOSS: 7.455527139384122e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 21/71 | LOSS: 7.675445389891551e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 22/71 | LOSS: 7.656536953341539e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 23/71 | LOSS: 7.932948271142475e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 24/71 | LOSS: 7.887635001679882e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 25/71 | LOSS: 7.81795567420956e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 26/71 | LOSS: 7.769996860857916e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 27/71 | LOSS: 7.789381535466029e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 28/71 | LOSS: 7.733370298881019e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 29/71 | LOSS: 7.797562357154675e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 30/71 | LOSS: 7.708710895272933e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 31/71 | LOSS: 7.670680338378588e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 32/71 | LOSS: 7.67526549735163e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 33/71 | LOSS: 7.69020508040044e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 34/71 | LOSS: 7.758407196628728e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 35/71 | LOSS: 7.696128624148615e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 36/71 | LOSS: 7.665294912373144e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 37/71 | LOSS: 7.603786338892717e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 38/71 | LOSS: 7.582735550106288e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 39/71 | LOSS: 7.766292164888e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 40/71 | LOSS: 7.799441433545748e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 41/71 | LOSS: 7.802359461839799e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 42/71 | LOSS: 7.797261922712286e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 43/71 | LOSS: 7.966967231451153e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 44/71 | LOSS: 7.959224765525303e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 45/71 | LOSS: 7.898867458963018e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 46/71 | LOSS: 7.818557052350444e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 47/71 | LOSS: 7.806752554036696e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 48/71 | LOSS: 7.813511086967584e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 49/71 | LOSS: 7.784063338476698e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 50/71 | LOSS: 7.745950150659636e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 51/71 | LOSS: 7.733305742126853e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 52/71 | LOSS: 7.692039132893595e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 53/71 | LOSS: 7.728658487461939e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 54/71 | LOSS: 7.735747421975248e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 55/71 | LOSS: 7.740209425069875e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 56/71 | LOSS: 7.720408610654423e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 57/71 | LOSS: 7.711482029317894e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 58/71 | LOSS: 7.763323681162655e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 59/71 | LOSS: 7.783555878025558e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 60/71 | LOSS: 7.796214568308462e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 61/71 | LOSS: 7.780887062835973e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 62/71 | LOSS: 7.817283738635496e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 63/71 | LOSS: 7.844692726166613e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 64/71 | LOSS: 7.829565701495784e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 65/71 | LOSS: 7.798649103003272e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 66/71 | LOSS: 7.784499410181932e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 67/71 | LOSS: 7.887204104490058e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 68/71 | LOSS: 7.85981245681414e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 69/71 | LOSS: 7.954132753574023e-05\n",
      "TRAIN: EPOCH 32/100 | BATCH 70/71 | LOSS: 7.919985026245753e-05\n",
      "VAL: EPOCH 32/100 | BATCH 0/8 | LOSS: 7.650536281289533e-05\n",
      "VAL: EPOCH 32/100 | BATCH 1/8 | LOSS: 6.85594422975555e-05\n",
      "VAL: EPOCH 32/100 | BATCH 2/8 | LOSS: 6.683870985095079e-05\n",
      "VAL: EPOCH 32/100 | BATCH 3/8 | LOSS: 7.135712439776398e-05\n",
      "VAL: EPOCH 32/100 | BATCH 4/8 | LOSS: 6.73012706101872e-05\n",
      "VAL: EPOCH 32/100 | BATCH 5/8 | LOSS: 6.667109482805245e-05\n",
      "VAL: EPOCH 32/100 | BATCH 6/8 | LOSS: 6.831120013625227e-05\n",
      "VAL: EPOCH 32/100 | BATCH 7/8 | LOSS: 6.664417242063791e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 0/71 | LOSS: 7.251241186168045e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 1/71 | LOSS: 8.336147584486753e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 2/71 | LOSS: 7.8415983201315e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 3/71 | LOSS: 7.397662466246402e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 4/71 | LOSS: 8.717015662114136e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 5/71 | LOSS: 8.164978559458784e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 6/71 | LOSS: 7.887096944614314e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 7/71 | LOSS: 7.605225164297735e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 8/71 | LOSS: 7.523255286893497e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 9/71 | LOSS: 7.663607902941294e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 10/71 | LOSS: 7.778917510718615e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 11/71 | LOSS: 7.839990636663667e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 12/71 | LOSS: 7.889664546103001e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 13/71 | LOSS: 7.802889116906695e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 14/71 | LOSS: 7.834405842004344e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 15/71 | LOSS: 7.963763709994964e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 16/71 | LOSS: 7.857839741251048e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 17/71 | LOSS: 7.776099988000674e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 18/71 | LOSS: 7.783083435098983e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 19/71 | LOSS: 7.669361875741742e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 20/71 | LOSS: 7.781833846820518e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 21/71 | LOSS: 7.694659523787612e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 22/71 | LOSS: 7.681985073798822e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 23/71 | LOSS: 7.659476477783755e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 24/71 | LOSS: 7.605694103403949e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 25/71 | LOSS: 7.61949915893358e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 26/71 | LOSS: 7.555934172597956e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 27/71 | LOSS: 7.502617388256892e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 28/71 | LOSS: 7.493034605306156e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 29/71 | LOSS: 7.515443770292526e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 30/71 | LOSS: 7.42593394768905e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 31/71 | LOSS: 7.408513260998006e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 32/71 | LOSS: 7.563533687392588e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 33/71 | LOSS: 7.534991591990761e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 34/71 | LOSS: 7.510163372249475e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 35/71 | LOSS: 7.454538061008659e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 36/71 | LOSS: 7.508101082970136e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 37/71 | LOSS: 7.45412825812022e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 38/71 | LOSS: 7.563426949603196e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 39/71 | LOSS: 7.497545957448892e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 40/71 | LOSS: 7.472269303767329e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 41/71 | LOSS: 7.48715084184715e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 42/71 | LOSS: 7.431898848153651e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 43/71 | LOSS: 7.40456806934989e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 44/71 | LOSS: 7.365251593809161e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 45/71 | LOSS: 7.359686945995276e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 46/71 | LOSS: 7.422096848344193e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 47/71 | LOSS: 7.403363724733936e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 48/71 | LOSS: 7.396476100940182e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 49/71 | LOSS: 7.39168660948053e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 50/71 | LOSS: 7.394869516799957e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 51/71 | LOSS: 7.386030147944648e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 52/71 | LOSS: 7.357744937696723e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 53/71 | LOSS: 7.331926220407088e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 54/71 | LOSS: 7.385034735886042e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 55/71 | LOSS: 7.371243191300891e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 56/71 | LOSS: 7.381031530696833e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 57/71 | LOSS: 7.352120839390534e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 58/71 | LOSS: 7.33542352864495e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 59/71 | LOSS: 7.36369351822456e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 60/71 | LOSS: 7.403529418267875e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 61/71 | LOSS: 7.371258412134804e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 62/71 | LOSS: 7.362979037376742e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 63/71 | LOSS: 7.3384733354942e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 64/71 | LOSS: 7.381323890653081e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 65/71 | LOSS: 7.344207314776158e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 66/71 | LOSS: 7.35074146490009e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 67/71 | LOSS: 7.349187511435422e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 68/71 | LOSS: 7.434590103609177e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 69/71 | LOSS: 7.46117517597408e-05\n",
      "TRAIN: EPOCH 33/100 | BATCH 70/71 | LOSS: 7.427446300272083e-05\n",
      "VAL: EPOCH 33/100 | BATCH 0/8 | LOSS: 7.199773244792596e-05\n",
      "VAL: EPOCH 33/100 | BATCH 1/8 | LOSS: 6.356920675898436e-05\n",
      "VAL: EPOCH 33/100 | BATCH 2/8 | LOSS: 6.22510500155234e-05\n",
      "VAL: EPOCH 33/100 | BATCH 3/8 | LOSS: 6.649416445725365e-05\n",
      "VAL: EPOCH 33/100 | BATCH 4/8 | LOSS: 6.264210751396603e-05\n",
      "VAL: EPOCH 33/100 | BATCH 5/8 | LOSS: 6.191610433840349e-05\n",
      "VAL: EPOCH 33/100 | BATCH 6/8 | LOSS: 6.329069843299553e-05\n",
      "VAL: EPOCH 33/100 | BATCH 7/8 | LOSS: 6.203842849572538e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 0/71 | LOSS: 7.296960393432528e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 1/71 | LOSS: 6.487483187811449e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 2/71 | LOSS: 7.025419472483918e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 3/71 | LOSS: 6.831718746980187e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 4/71 | LOSS: 6.53118797345087e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 5/71 | LOSS: 6.290897241948794e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 6/71 | LOSS: 6.240080983843654e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 7/71 | LOSS: 6.296592619037256e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 8/71 | LOSS: 6.348661554511636e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 9/71 | LOSS: 6.291470708674751e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 10/71 | LOSS: 6.26859281593087e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 11/71 | LOSS: 6.467326056736056e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 12/71 | LOSS: 6.389566652405147e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 13/71 | LOSS: 6.390523724673715e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 14/71 | LOSS: 6.335908353018264e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 15/71 | LOSS: 6.410223613784183e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 16/71 | LOSS: 6.572237993275528e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 17/71 | LOSS: 6.571518962219771e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 18/71 | LOSS: 6.475067704358432e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 19/71 | LOSS: 6.473566481872695e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 20/71 | LOSS: 6.481824623603773e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 21/71 | LOSS: 6.46746691266096e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 22/71 | LOSS: 6.384106809096208e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 23/71 | LOSS: 6.625040092937222e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 24/71 | LOSS: 7.011327383224853e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 25/71 | LOSS: 7.012635992745905e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 26/71 | LOSS: 6.957017655993156e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 27/71 | LOSS: 7.038518430947858e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 28/71 | LOSS: 6.998700261866706e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 29/71 | LOSS: 7.010464990647355e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 30/71 | LOSS: 6.987010783632286e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 31/71 | LOSS: 6.949169585368509e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 32/71 | LOSS: 7.00613230891935e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 33/71 | LOSS: 7.213328150529003e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 34/71 | LOSS: 7.246110170464298e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 35/71 | LOSS: 7.248370123609978e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 36/71 | LOSS: 7.251093022106845e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 37/71 | LOSS: 7.214785067557578e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 38/71 | LOSS: 7.148482584922861e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 39/71 | LOSS: 7.13150452611444e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 40/71 | LOSS: 7.230584289972308e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 41/71 | LOSS: 7.270790497603316e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 42/71 | LOSS: 7.210037091321875e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 43/71 | LOSS: 7.172647209782471e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 44/71 | LOSS: 7.145604831748642e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 45/71 | LOSS: 7.111505099443173e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 46/71 | LOSS: 7.156893046525743e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 47/71 | LOSS: 7.149647012738569e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 48/71 | LOSS: 7.10599945575398e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 49/71 | LOSS: 7.095583787304349e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 50/71 | LOSS: 7.093667698106455e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 51/71 | LOSS: 7.030100459815003e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 52/71 | LOSS: 7.039535338199644e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 53/71 | LOSS: 6.995601884059229e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 54/71 | LOSS: 6.942708800620908e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 55/71 | LOSS: 6.983081604110859e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 56/71 | LOSS: 6.95039727776148e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 57/71 | LOSS: 7.034681678713492e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 58/71 | LOSS: 7.044158836972695e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 59/71 | LOSS: 7.053305052977521e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 60/71 | LOSS: 7.041035800672244e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 61/71 | LOSS: 7.005229118447224e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 62/71 | LOSS: 6.988386782708519e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 63/71 | LOSS: 6.965268579506301e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 64/71 | LOSS: 6.930841219190579e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 65/71 | LOSS: 6.887981709786055e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 66/71 | LOSS: 6.899868995828707e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 67/71 | LOSS: 6.87022827739437e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 68/71 | LOSS: 6.839936474495082e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 69/71 | LOSS: 6.82891576226601e-05\n",
      "TRAIN: EPOCH 34/100 | BATCH 70/71 | LOSS: 6.795315939726465e-05\n",
      "VAL: EPOCH 34/100 | BATCH 0/8 | LOSS: 6.483052129624411e-05\n",
      "VAL: EPOCH 34/100 | BATCH 1/8 | LOSS: 5.8431349316379055e-05\n",
      "VAL: EPOCH 34/100 | BATCH 2/8 | LOSS: 5.720590706914663e-05\n",
      "VAL: EPOCH 34/100 | BATCH 3/8 | LOSS: 6.0464513808256015e-05\n",
      "VAL: EPOCH 34/100 | BATCH 4/8 | LOSS: 5.72975492104888e-05\n",
      "VAL: EPOCH 34/100 | BATCH 5/8 | LOSS: 5.7025480297549315e-05\n",
      "VAL: EPOCH 34/100 | BATCH 6/8 | LOSS: 5.8529130391044805e-05\n",
      "VAL: EPOCH 34/100 | BATCH 7/8 | LOSS: 5.751457638325519e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 0/71 | LOSS: 6.458467396441847e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 1/71 | LOSS: 6.397667311830446e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 2/71 | LOSS: 7.303098633807774e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 3/71 | LOSS: 6.818279507569969e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 4/71 | LOSS: 7.551567396149039e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 5/71 | LOSS: 7.482227374566719e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 6/71 | LOSS: 7.402732860230441e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 7/71 | LOSS: 7.236545116029447e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 8/71 | LOSS: 6.998740961231913e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 9/71 | LOSS: 6.856627478555311e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 10/71 | LOSS: 6.71795288243712e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 11/71 | LOSS: 6.773570536703725e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 12/71 | LOSS: 6.940114102550209e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 13/71 | LOSS: 6.81738095279018e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 14/71 | LOSS: 6.726342180627399e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 15/71 | LOSS: 6.628140567954688e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 16/71 | LOSS: 6.638293908669285e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 17/71 | LOSS: 6.49869725748431e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 18/71 | LOSS: 6.497238255657354e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 19/71 | LOSS: 6.463295467256103e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 20/71 | LOSS: 6.441101158132023e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 21/71 | LOSS: 6.656463648885784e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 22/71 | LOSS: 6.587864734920795e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 23/71 | LOSS: 6.633144797281905e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 24/71 | LOSS: 6.575209263246507e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 25/71 | LOSS: 6.653489477824993e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 26/71 | LOSS: 6.656219732206039e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 27/71 | LOSS: 6.663503258356027e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 28/71 | LOSS: 6.6433980527077e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 29/71 | LOSS: 6.570123005076312e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 30/71 | LOSS: 6.527523369333076e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 31/71 | LOSS: 6.468656545166596e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 32/71 | LOSS: 6.399289453507994e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 33/71 | LOSS: 6.436677936375524e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 34/71 | LOSS: 6.469190453312227e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 35/71 | LOSS: 6.435120829135283e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 36/71 | LOSS: 6.429819906180775e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 37/71 | LOSS: 6.407730267566971e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 38/71 | LOSS: 6.402155439644001e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 39/71 | LOSS: 6.401554801414023e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 40/71 | LOSS: 6.356856341348825e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 41/71 | LOSS: 6.342269383215655e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 42/71 | LOSS: 6.313353929557664e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 43/71 | LOSS: 6.344819906768284e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 44/71 | LOSS: 6.407647314416762e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 45/71 | LOSS: 6.375322798545392e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 46/71 | LOSS: 6.367851583103828e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 47/71 | LOSS: 6.344167847297892e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 48/71 | LOSS: 6.325837929747353e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 49/71 | LOSS: 6.299565342487767e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 50/71 | LOSS: 6.309448465746006e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 51/71 | LOSS: 6.272623194001007e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 52/71 | LOSS: 6.304663097494686e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 53/71 | LOSS: 6.270555227443024e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 54/71 | LOSS: 6.286409545943818e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 55/71 | LOSS: 6.408958142856136e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 56/71 | LOSS: 6.411583002007224e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 57/71 | LOSS: 6.384839880097545e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 58/71 | LOSS: 6.369927260971439e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 59/71 | LOSS: 6.318905270745745e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 60/71 | LOSS: 6.329397937868814e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 61/71 | LOSS: 6.349929818468573e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 62/71 | LOSS: 6.329484162889125e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 63/71 | LOSS: 6.327029643671267e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 64/71 | LOSS: 6.293187000735018e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 65/71 | LOSS: 6.26707181006416e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 66/71 | LOSS: 6.294195256161211e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 67/71 | LOSS: 6.27182078674463e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 68/71 | LOSS: 6.276145480417956e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 69/71 | LOSS: 6.342823440458492e-05\n",
      "TRAIN: EPOCH 35/100 | BATCH 70/71 | LOSS: 6.313445125452348e-05\n",
      "VAL: EPOCH 35/100 | BATCH 0/8 | LOSS: 5.535615491680801e-05\n",
      "VAL: EPOCH 35/100 | BATCH 1/8 | LOSS: 5.2080151363043115e-05\n",
      "VAL: EPOCH 35/100 | BATCH 2/8 | LOSS: 5.179017171030864e-05\n",
      "VAL: EPOCH 35/100 | BATCH 3/8 | LOSS: 5.518072248378303e-05\n",
      "VAL: EPOCH 35/100 | BATCH 4/8 | LOSS: 5.271426198305562e-05\n",
      "VAL: EPOCH 35/100 | BATCH 5/8 | LOSS: 5.2828389622542694e-05\n",
      "VAL: EPOCH 35/100 | BATCH 6/8 | LOSS: 5.429492687523764e-05\n",
      "VAL: EPOCH 35/100 | BATCH 7/8 | LOSS: 5.337741731636925e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 0/71 | LOSS: 8.238485315814614e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 1/71 | LOSS: 6.519337694044225e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 2/71 | LOSS: 5.6240967145034425e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 3/71 | LOSS: 5.757813596574124e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 4/71 | LOSS: 5.6591919565107675e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 5/71 | LOSS: 5.820603352428103e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 6/71 | LOSS: 5.691896928640615e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 7/71 | LOSS: 5.545124986383598e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 8/71 | LOSS: 5.842971505545494e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 9/71 | LOSS: 5.694894134649076e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 10/71 | LOSS: 5.781438482120972e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 11/71 | LOSS: 5.928518658038229e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 12/71 | LOSS: 5.754585268172937e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 13/71 | LOSS: 5.6794408010318875e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 14/71 | LOSS: 5.678449815604836e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 15/71 | LOSS: 5.8299433931097155e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 16/71 | LOSS: 5.749144303776762e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 17/71 | LOSS: 5.6841967307263985e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 18/71 | LOSS: 5.746256609104181e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 19/71 | LOSS: 5.716232935810694e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 20/71 | LOSS: 5.699904622382573e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 21/71 | LOSS: 5.807578732360112e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 22/71 | LOSS: 5.847681424474222e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 23/71 | LOSS: 5.841820363154208e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 24/71 | LOSS: 6.0606232291320336e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 25/71 | LOSS: 6.004921739916496e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 26/71 | LOSS: 5.9912326294040586e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 27/71 | LOSS: 5.9313376888375e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 28/71 | LOSS: 5.95842812517417e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 29/71 | LOSS: 5.952979699941352e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 30/71 | LOSS: 6.160791493922232e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 31/71 | LOSS: 6.0978949363743595e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 32/71 | LOSS: 6.043445440203262e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 33/71 | LOSS: 5.9971764772746933e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 34/71 | LOSS: 5.9593736451850935e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 35/71 | LOSS: 5.941633248261901e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 36/71 | LOSS: 5.968010451839733e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 37/71 | LOSS: 5.951959945760839e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 38/71 | LOSS: 5.978804526817746e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 39/71 | LOSS: 5.9811268602061316e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 40/71 | LOSS: 5.9626958362769516e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 41/71 | LOSS: 5.9628304543799635e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 42/71 | LOSS: 5.9427372075016283e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 43/71 | LOSS: 5.948605038693958e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 44/71 | LOSS: 5.9678186031912145e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 45/71 | LOSS: 5.964654487995532e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 46/71 | LOSS: 5.943707490471152e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 47/71 | LOSS: 5.916175170265584e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 48/71 | LOSS: 5.8818914335368355e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 49/71 | LOSS: 5.843766477482859e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 50/71 | LOSS: 5.8463906373385815e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 51/71 | LOSS: 5.8802823402332324e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 52/71 | LOSS: 5.8999367402580936e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 53/71 | LOSS: 5.8855725540154335e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 54/71 | LOSS: 5.889894190741788e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 55/71 | LOSS: 5.9403211902723084e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 56/71 | LOSS: 5.9117785559991666e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 57/71 | LOSS: 5.894483948740626e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 58/71 | LOSS: 5.8790881636106586e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 59/71 | LOSS: 5.9158597953986224e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 60/71 | LOSS: 5.9291570040408796e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 61/71 | LOSS: 5.908050796631048e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 62/71 | LOSS: 5.9225857671302194e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 63/71 | LOSS: 5.9052796700598265e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 64/71 | LOSS: 5.926834576082631e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 65/71 | LOSS: 5.930610694287514e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 66/71 | LOSS: 5.910743509876236e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 67/71 | LOSS: 5.8907289284808724e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 68/71 | LOSS: 5.891000045517433e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 69/71 | LOSS: 5.876854130682269e-05\n",
      "TRAIN: EPOCH 36/100 | BATCH 70/71 | LOSS: 5.844743840575126e-05\n",
      "VAL: EPOCH 36/100 | BATCH 0/8 | LOSS: 5.82346910960041e-05\n",
      "VAL: EPOCH 36/100 | BATCH 1/8 | LOSS: 5.462723129312508e-05\n",
      "VAL: EPOCH 36/100 | BATCH 2/8 | LOSS: 5.3528157877735794e-05\n",
      "VAL: EPOCH 36/100 | BATCH 3/8 | LOSS: 5.478774255607277e-05\n",
      "VAL: EPOCH 36/100 | BATCH 4/8 | LOSS: 5.199483057367615e-05\n",
      "VAL: EPOCH 36/100 | BATCH 5/8 | LOSS: 5.2090694225626066e-05\n",
      "VAL: EPOCH 36/100 | BATCH 6/8 | LOSS: 5.3163262366849396e-05\n",
      "VAL: EPOCH 36/100 | BATCH 7/8 | LOSS: 5.240110795057262e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 0/71 | LOSS: 6.847645272500813e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 1/71 | LOSS: 7.723330782027915e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 2/71 | LOSS: 7.033671257280123e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 3/71 | LOSS: 6.176357874210225e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 4/71 | LOSS: 5.880139433429576e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 5/71 | LOSS: 5.736972889280878e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 6/71 | LOSS: 5.7065189529177065e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 7/71 | LOSS: 6.151526258690865e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 8/71 | LOSS: 6.124300408474583e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 9/71 | LOSS: 5.980397218081634e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 10/71 | LOSS: 5.7757865182462744e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 11/71 | LOSS: 5.62595772256221e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 12/71 | LOSS: 5.580148158165125e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 13/71 | LOSS: 5.460393731482327e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 14/71 | LOSS: 5.414017020181442e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 15/71 | LOSS: 5.465672302307212e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 16/71 | LOSS: 5.4949467619949515e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 17/71 | LOSS: 5.505318949872162e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 18/71 | LOSS: 5.563377041988516e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 19/71 | LOSS: 5.55762857402442e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 20/71 | LOSS: 5.8053095320550106e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 21/71 | LOSS: 5.757739150298717e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 22/71 | LOSS: 5.730381267114907e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 23/71 | LOSS: 5.640613350503069e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 24/71 | LOSS: 5.557994314585812e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 25/71 | LOSS: 5.4950420795089136e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 26/71 | LOSS: 5.45962355789487e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 27/71 | LOSS: 5.486307366351996e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 28/71 | LOSS: 5.4717348167945725e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 29/71 | LOSS: 5.442234129683736e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 30/71 | LOSS: 5.5842684824094776e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 31/71 | LOSS: 5.531438660000276e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 32/71 | LOSS: 5.563602374450332e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 33/71 | LOSS: 5.537798749762313e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 34/71 | LOSS: 5.590293751863231e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 35/71 | LOSS: 5.5703766621364695e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 36/71 | LOSS: 5.5197660470651966e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 37/71 | LOSS: 5.528099283860923e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 38/71 | LOSS: 5.604822991956145e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 39/71 | LOSS: 5.586183278865065e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 40/71 | LOSS: 5.596554795529966e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 41/71 | LOSS: 5.5628203231857955e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 42/71 | LOSS: 5.525262743101975e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 43/71 | LOSS: 5.515647419749505e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 44/71 | LOSS: 5.551152312869413e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 45/71 | LOSS: 5.52684720352535e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 46/71 | LOSS: 5.489651208490807e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 47/71 | LOSS: 5.498373441999623e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 48/71 | LOSS: 5.490379957410944e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 49/71 | LOSS: 5.4640751841361634e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 50/71 | LOSS: 5.471375890706173e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 51/71 | LOSS: 5.452017928921403e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 52/71 | LOSS: 5.497007256259544e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 53/71 | LOSS: 5.476060613108092e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 54/71 | LOSS: 5.4780664702940903e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 55/71 | LOSS: 5.478376046344887e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 56/71 | LOSS: 5.4581510388137154e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 57/71 | LOSS: 5.464282247854059e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 58/71 | LOSS: 5.4323766207898637e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 59/71 | LOSS: 5.4187350906431674e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 60/71 | LOSS: 5.410903395386413e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 61/71 | LOSS: 5.406994486909224e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 62/71 | LOSS: 5.4336958119021136e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 63/71 | LOSS: 5.42738163744616e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 64/71 | LOSS: 5.403195794385213e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 65/71 | LOSS: 5.438565605083678e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 66/71 | LOSS: 5.419857004212577e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 67/71 | LOSS: 5.408197038752191e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 68/71 | LOSS: 5.412385244330769e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 69/71 | LOSS: 5.452073580402482e-05\n",
      "TRAIN: EPOCH 37/100 | BATCH 70/71 | LOSS: 5.4390145546484505e-05\n",
      "VAL: EPOCH 37/100 | BATCH 0/8 | LOSS: 4.555566192721017e-05\n",
      "VAL: EPOCH 37/100 | BATCH 1/8 | LOSS: 4.359868398751132e-05\n",
      "VAL: EPOCH 37/100 | BATCH 2/8 | LOSS: 4.442537950429445e-05\n",
      "VAL: EPOCH 37/100 | BATCH 3/8 | LOSS: 4.753939174406696e-05\n",
      "VAL: EPOCH 37/100 | BATCH 4/8 | LOSS: 4.577816289383918e-05\n",
      "VAL: EPOCH 37/100 | BATCH 5/8 | LOSS: 4.5817921394094206e-05\n",
      "VAL: EPOCH 37/100 | BATCH 6/8 | LOSS: 4.7267672406243424e-05\n",
      "VAL: EPOCH 37/100 | BATCH 7/8 | LOSS: 4.657993031287333e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 0/71 | LOSS: 5.908303501200862e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 1/71 | LOSS: 5.170205440663267e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 2/71 | LOSS: 4.801609247806482e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 3/71 | LOSS: 5.3568300245387945e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 4/71 | LOSS: 5.38730455446057e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 5/71 | LOSS: 5.1180484661017545e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 6/71 | LOSS: 4.975256316746319e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 7/71 | LOSS: 5.096839640827966e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 8/71 | LOSS: 5.0598428869206044e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 9/71 | LOSS: 5.021476317779161e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 10/71 | LOSS: 5.002711986890063e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 11/71 | LOSS: 4.838496791611154e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 12/71 | LOSS: 4.749198272363104e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 13/71 | LOSS: 4.7767815626035114e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 14/71 | LOSS: 4.760887156104824e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 15/71 | LOSS: 4.7898273464852537e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 16/71 | LOSS: 4.735334828985608e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 17/71 | LOSS: 4.7398051517828004e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 18/71 | LOSS: 4.818979725410157e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 19/71 | LOSS: 4.879202942902339e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 20/71 | LOSS: 4.816909677174408e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 21/71 | LOSS: 4.79942990833396e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 22/71 | LOSS: 4.7655075673980676e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 23/71 | LOSS: 4.793575385519944e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 24/71 | LOSS: 4.8543194498051887e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 25/71 | LOSS: 4.847328294160364e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 26/71 | LOSS: 4.8331597874359296e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 27/71 | LOSS: 5.045716073124952e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 28/71 | LOSS: 5.0237802811195784e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 29/71 | LOSS: 5.0114376851221705e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 30/71 | LOSS: 5.203636599969374e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 31/71 | LOSS: 5.1848593614067795e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 32/71 | LOSS: 5.2092797914431713e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 33/71 | LOSS: 5.327033127019497e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 34/71 | LOSS: 5.406094169302378e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 35/71 | LOSS: 5.3869954222641856e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 36/71 | LOSS: 5.359814683804477e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 37/71 | LOSS: 5.3382210449183875e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 38/71 | LOSS: 5.3223797994388195e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 39/71 | LOSS: 5.283881851028127e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 40/71 | LOSS: 5.388831649303561e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 41/71 | LOSS: 5.3584086041761715e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 42/71 | LOSS: 5.347141374185476e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 43/71 | LOSS: 5.3049895913401e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 44/71 | LOSS: 5.277298441797029e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 45/71 | LOSS: 5.2642939052653354e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 46/71 | LOSS: 5.2838198439047195e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 47/71 | LOSS: 5.24457862335718e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 48/71 | LOSS: 5.295870353305019e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 49/71 | LOSS: 5.2787536951655056e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 50/71 | LOSS: 5.2636837840420376e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 51/71 | LOSS: 5.2429172718313144e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 52/71 | LOSS: 5.2168101662899506e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 53/71 | LOSS: 5.2468150796711496e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 54/71 | LOSS: 5.236207901113878e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 55/71 | LOSS: 5.23627821361775e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 56/71 | LOSS: 5.214670298364741e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 57/71 | LOSS: 5.1856207329396496e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 58/71 | LOSS: 5.2118195588078934e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 59/71 | LOSS: 5.1950093529740114e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 60/71 | LOSS: 5.169577334676373e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 61/71 | LOSS: 5.152338144398527e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 62/71 | LOSS: 5.2072082932378784e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 63/71 | LOSS: 5.197972197379386e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 64/71 | LOSS: 5.1761615395662375e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 65/71 | LOSS: 5.160837140878853e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 66/71 | LOSS: 5.131118312202665e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 67/71 | LOSS: 5.109974579360877e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 68/71 | LOSS: 5.109432904514572e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 69/71 | LOSS: 5.082111963149925e-05\n",
      "TRAIN: EPOCH 38/100 | BATCH 70/71 | LOSS: 5.129219448783363e-05\n",
      "VAL: EPOCH 38/100 | BATCH 0/8 | LOSS: 4.8221270844805986e-05\n",
      "VAL: EPOCH 38/100 | BATCH 1/8 | LOSS: 4.425707993505057e-05\n",
      "VAL: EPOCH 38/100 | BATCH 2/8 | LOSS: 4.413295998044001e-05\n",
      "VAL: EPOCH 38/100 | BATCH 3/8 | LOSS: 4.611392523656832e-05\n",
      "VAL: EPOCH 38/100 | BATCH 4/8 | LOSS: 4.435781011125073e-05\n",
      "VAL: EPOCH 38/100 | BATCH 5/8 | LOSS: 4.409549728734419e-05\n",
      "VAL: EPOCH 38/100 | BATCH 6/8 | LOSS: 4.521414666669443e-05\n",
      "VAL: EPOCH 38/100 | BATCH 7/8 | LOSS: 4.486368698053411e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 0/71 | LOSS: 6.821085116825998e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 1/71 | LOSS: 5.404266994446516e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 2/71 | LOSS: 5.241007359776025e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 3/71 | LOSS: 5.2509089073282667e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 4/71 | LOSS: 4.999035445507616e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 5/71 | LOSS: 4.8768173049514495e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 6/71 | LOSS: 4.855210240098781e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 7/71 | LOSS: 4.744439547721413e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 8/71 | LOSS: 4.9183501283146645e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 9/71 | LOSS: 4.8151528608286755e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 10/71 | LOSS: 4.76750465168152e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 11/71 | LOSS: 4.743670372893879e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 12/71 | LOSS: 4.6450848458334804e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 13/71 | LOSS: 4.570360579236876e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 14/71 | LOSS: 4.5381776726571846e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 15/71 | LOSS: 4.463229993234563e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 16/71 | LOSS: 4.5314004262977775e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 17/71 | LOSS: 4.5191758343005655e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 18/71 | LOSS: 4.6186313595267404e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 19/71 | LOSS: 4.670157850341639e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 20/71 | LOSS: 4.658407973086772e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 21/71 | LOSS: 4.781007919518743e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 22/71 | LOSS: 4.756406143196332e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 23/71 | LOSS: 4.70625867213433e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 24/71 | LOSS: 4.689705951022916e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 25/71 | LOSS: 4.6586638116143426e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 26/71 | LOSS: 4.671193501390344e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 27/71 | LOSS: 4.670745628183275e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 28/71 | LOSS: 4.623137258398668e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 29/71 | LOSS: 4.638061327568721e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 30/71 | LOSS: 4.5907717233809134e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 31/71 | LOSS: 4.539292598337852e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 32/71 | LOSS: 4.5232832593628146e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 33/71 | LOSS: 4.518779288071742e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 34/71 | LOSS: 4.5605907975446566e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 35/71 | LOSS: 4.615072718176331e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 36/71 | LOSS: 4.6981957820134014e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 37/71 | LOSS: 4.709572237372509e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 38/71 | LOSS: 4.7043886064411476e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 39/71 | LOSS: 4.6686462019351894e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 40/71 | LOSS: 4.6565939511510363e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 41/71 | LOSS: 4.649889225610033e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 42/71 | LOSS: 4.673836862588529e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 43/71 | LOSS: 4.6554601009450956e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 44/71 | LOSS: 4.6327852315799744e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 45/71 | LOSS: 4.6660246287343476e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 46/71 | LOSS: 4.6731412673599405e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 47/71 | LOSS: 4.662367174053846e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 48/71 | LOSS: 4.6750055324602204e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 49/71 | LOSS: 4.696170184615767e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 50/71 | LOSS: 4.7287582406972756e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 51/71 | LOSS: 4.701484735610743e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 52/71 | LOSS: 4.788959188961187e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 53/71 | LOSS: 4.7538229780372977e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 54/71 | LOSS: 4.7403867640902965e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 55/71 | LOSS: 4.7397138814631035e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 56/71 | LOSS: 4.770413110427877e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 57/71 | LOSS: 4.821538054941278e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 58/71 | LOSS: 4.801671059532841e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 59/71 | LOSS: 4.872778830152432e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 60/71 | LOSS: 4.868863378179099e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 61/71 | LOSS: 4.8645640045804374e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 62/71 | LOSS: 4.866273798503994e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 63/71 | LOSS: 4.848055331763135e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 64/71 | LOSS: 4.830236114055599e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 65/71 | LOSS: 4.830948011768862e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 66/71 | LOSS: 4.829227319573671e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 67/71 | LOSS: 4.836238619699383e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 68/71 | LOSS: 4.820730709584693e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 69/71 | LOSS: 4.8088526688973486e-05\n",
      "TRAIN: EPOCH 39/100 | BATCH 70/71 | LOSS: 4.795097170949382e-05\n",
      "VAL: EPOCH 39/100 | BATCH 0/8 | LOSS: 4.4973196054343134e-05\n",
      "VAL: EPOCH 39/100 | BATCH 1/8 | LOSS: 4.0805376556818374e-05\n",
      "VAL: EPOCH 39/100 | BATCH 2/8 | LOSS: 4.135734949765416e-05\n",
      "VAL: EPOCH 39/100 | BATCH 3/8 | LOSS: 4.3178685700695496e-05\n",
      "VAL: EPOCH 39/100 | BATCH 4/8 | LOSS: 4.152989276917651e-05\n",
      "VAL: EPOCH 39/100 | BATCH 5/8 | LOSS: 4.107494593578546e-05\n",
      "VAL: EPOCH 39/100 | BATCH 6/8 | LOSS: 4.200361217954196e-05\n",
      "VAL: EPOCH 39/100 | BATCH 7/8 | LOSS: 4.168008354099584e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 0/71 | LOSS: 4.056796751683578e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 1/71 | LOSS: 3.951769758714363e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 2/71 | LOSS: 3.763244118696699e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 3/71 | LOSS: 4.2557336200843565e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 4/71 | LOSS: 4.1555939242243765e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 5/71 | LOSS: 4.3755663985696934e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 6/71 | LOSS: 4.50024485222197e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 7/71 | LOSS: 4.608622748492053e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 8/71 | LOSS: 4.4573226963014655e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 9/71 | LOSS: 4.4151688052807e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 10/71 | LOSS: 4.254687668104783e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 11/71 | LOSS: 4.186291835139855e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 12/71 | LOSS: 4.1860820252063255e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 13/71 | LOSS: 4.3337240250756235e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 14/71 | LOSS: 4.3390157831405914e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 15/71 | LOSS: 4.343275270457525e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 16/71 | LOSS: 4.34909434261499e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 17/71 | LOSS: 4.3620916383386226e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 18/71 | LOSS: 4.471370658919975e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 19/71 | LOSS: 4.416803594722296e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 20/71 | LOSS: 4.395437480568598e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 21/71 | LOSS: 4.404390371994602e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 22/71 | LOSS: 4.372095890689904e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 23/71 | LOSS: 4.3432806933196844e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 24/71 | LOSS: 4.320042899053078e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 25/71 | LOSS: 4.3161976464034524e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 26/71 | LOSS: 4.384791874919621e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 27/71 | LOSS: 4.536895637232062e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 28/71 | LOSS: 4.5457468425818915e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 29/71 | LOSS: 4.636939308208336e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 30/71 | LOSS: 4.651015332192108e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 31/71 | LOSS: 4.6217706824336346e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 32/71 | LOSS: 4.616486303072606e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 33/71 | LOSS: 4.627251479071085e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 34/71 | LOSS: 4.604745985748845e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 35/71 | LOSS: 4.59010478455942e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 36/71 | LOSS: 4.58396509595725e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 37/71 | LOSS: 4.623561323425843e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 38/71 | LOSS: 4.6279975998540744e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 39/71 | LOSS: 4.5874369561715866e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 40/71 | LOSS: 4.664677043165727e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 41/71 | LOSS: 4.63837474550625e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 42/71 | LOSS: 4.615996789822063e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 43/71 | LOSS: 4.598730418530811e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 44/71 | LOSS: 4.581086715107732e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 45/71 | LOSS: 4.577417574024445e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 46/71 | LOSS: 4.561117117539633e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 47/71 | LOSS: 4.552192372860494e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 48/71 | LOSS: 4.528205091465141e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 49/71 | LOSS: 4.522873266978422e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 50/71 | LOSS: 4.508457202652661e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 51/71 | LOSS: 4.486716006095566e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 52/71 | LOSS: 4.476545960505115e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 53/71 | LOSS: 4.480645141787439e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 54/71 | LOSS: 4.5083466929182495e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 55/71 | LOSS: 4.57569718556832e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 56/71 | LOSS: 4.573170017944281e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 57/71 | LOSS: 4.5880227624930455e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 58/71 | LOSS: 4.593779272385044e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 59/71 | LOSS: 4.569791905547997e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 60/71 | LOSS: 4.5600756254957485e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 61/71 | LOSS: 4.5485521607793825e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 62/71 | LOSS: 4.540104121602665e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 63/71 | LOSS: 4.52642383663715e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 64/71 | LOSS: 4.520297267075735e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 65/71 | LOSS: 4.50389566277877e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 66/71 | LOSS: 4.508771737381545e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 67/71 | LOSS: 4.507668839270242e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 68/71 | LOSS: 4.50772249810077e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 69/71 | LOSS: 4.494416199512281e-05\n",
      "TRAIN: EPOCH 40/100 | BATCH 70/71 | LOSS: 4.4971872573024855e-05\n",
      "VAL: EPOCH 40/100 | BATCH 0/8 | LOSS: 4.3273106712149456e-05\n",
      "VAL: EPOCH 40/100 | BATCH 1/8 | LOSS: 3.96551113226451e-05\n",
      "VAL: EPOCH 40/100 | BATCH 2/8 | LOSS: 4.026855337239491e-05\n",
      "VAL: EPOCH 40/100 | BATCH 3/8 | LOSS: 4.182247084827395e-05\n",
      "VAL: EPOCH 40/100 | BATCH 4/8 | LOSS: 4.0247473953058945e-05\n",
      "VAL: EPOCH 40/100 | BATCH 5/8 | LOSS: 3.963923700212035e-05\n",
      "VAL: EPOCH 40/100 | BATCH 6/8 | LOSS: 4.044534424403017e-05\n",
      "VAL: EPOCH 40/100 | BATCH 7/8 | LOSS: 3.995761198893888e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 0/71 | LOSS: 2.8912505513289943e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 1/71 | LOSS: 5.0900032874778844e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 2/71 | LOSS: 4.530466928069169e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 3/71 | LOSS: 5.067828715255018e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 4/71 | LOSS: 5.0434329023119065e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 5/71 | LOSS: 4.706625077233184e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 6/71 | LOSS: 4.638285203587397e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 7/71 | LOSS: 4.570078317556181e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 8/71 | LOSS: 4.528071238180726e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 9/71 | LOSS: 4.554812439891975e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 10/71 | LOSS: 4.603366919168779e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 11/71 | LOSS: 4.4742987483914476e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 12/71 | LOSS: 4.419958545790555e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 13/71 | LOSS: 4.4448581060610847e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 14/71 | LOSS: 4.4085768604418266e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 15/71 | LOSS: 4.381844337331131e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 16/71 | LOSS: 4.4454089434379165e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 17/71 | LOSS: 4.3713474143361156e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 18/71 | LOSS: 4.321449944798491e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 19/71 | LOSS: 4.301272565498948e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 20/71 | LOSS: 4.222582408276919e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 21/71 | LOSS: 4.2111162573829375e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 22/71 | LOSS: 4.247834911676245e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 23/71 | LOSS: 4.211505423275715e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 24/71 | LOSS: 4.205096789519302e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 25/71 | LOSS: 4.1736142240831846e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 26/71 | LOSS: 4.1618899693410776e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 27/71 | LOSS: 4.191791828946277e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 28/71 | LOSS: 4.229719129868719e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 29/71 | LOSS: 4.208563150314149e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 30/71 | LOSS: 4.316329443921155e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 31/71 | LOSS: 4.307589222207753e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 32/71 | LOSS: 4.28026000780378e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 33/71 | LOSS: 4.2668792529558926e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 34/71 | LOSS: 4.2560906045504715e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 35/71 | LOSS: 4.2627472390045616e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 36/71 | LOSS: 4.2625857089218255e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 37/71 | LOSS: 4.2772891614923104e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 38/71 | LOSS: 4.291624011388287e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 39/71 | LOSS: 4.284487422410166e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 40/71 | LOSS: 4.2624918888777314e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 41/71 | LOSS: 4.320231077837802e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 42/71 | LOSS: 4.333226050136524e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 43/71 | LOSS: 4.322562199376989e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 44/71 | LOSS: 4.406261755826159e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 45/71 | LOSS: 4.394224426663562e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 46/71 | LOSS: 4.394364975481988e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 47/71 | LOSS: 4.415418281193221e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 48/71 | LOSS: 4.400116568478775e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 49/71 | LOSS: 4.387063076137565e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 50/71 | LOSS: 4.3870630749962384e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 51/71 | LOSS: 4.364932612168979e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 52/71 | LOSS: 4.4017533685480474e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 53/71 | LOSS: 4.3829021605132664e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 54/71 | LOSS: 4.389369708157822e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 55/71 | LOSS: 4.3564260782399545e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 56/71 | LOSS: 4.3328664967128516e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 57/71 | LOSS: 4.344899307711628e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 58/71 | LOSS: 4.337009220577429e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 59/71 | LOSS: 4.331359680994259e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 60/71 | LOSS: 4.3539109221873344e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 61/71 | LOSS: 4.360837186639753e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 62/71 | LOSS: 4.3479639677412156e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 63/71 | LOSS: 4.339786448781524e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 64/71 | LOSS: 4.330347079443387e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 65/71 | LOSS: 4.3354661102057435e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 66/71 | LOSS: 4.3262814490016854e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 67/71 | LOSS: 4.312781857152004e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 68/71 | LOSS: 4.297807838842003e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 69/71 | LOSS: 4.2859117924568376e-05\n",
      "TRAIN: EPOCH 41/100 | BATCH 70/71 | LOSS: 4.2730139502452056e-05\n",
      "VAL: EPOCH 41/100 | BATCH 0/8 | LOSS: 3.7078596506034955e-05\n",
      "VAL: EPOCH 41/100 | BATCH 1/8 | LOSS: 3.3566589081601705e-05\n",
      "VAL: EPOCH 41/100 | BATCH 2/8 | LOSS: 3.581493062180622e-05\n",
      "VAL: EPOCH 41/100 | BATCH 3/8 | LOSS: 3.862764924633666e-05\n",
      "VAL: EPOCH 41/100 | BATCH 4/8 | LOSS: 3.7650596277671865e-05\n",
      "VAL: EPOCH 41/100 | BATCH 5/8 | LOSS: 3.705284719520326e-05\n",
      "VAL: EPOCH 41/100 | BATCH 6/8 | LOSS: 3.820041794304936e-05\n",
      "VAL: EPOCH 41/100 | BATCH 7/8 | LOSS: 3.787493574236578e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 0/71 | LOSS: 4.0348415495827794e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 1/71 | LOSS: 3.6983292375225574e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 2/71 | LOSS: 3.7969605424829446e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 3/71 | LOSS: 3.574429774744203e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 4/71 | LOSS: 3.615807581809349e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 5/71 | LOSS: 3.597196155169513e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 6/71 | LOSS: 3.6044541047886014e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 7/71 | LOSS: 3.60354542863206e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 8/71 | LOSS: 3.5872058409343786e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 9/71 | LOSS: 3.6648561945185065e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 10/71 | LOSS: 3.771187634397806e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 11/71 | LOSS: 3.7842443816771265e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 12/71 | LOSS: 3.753139110183558e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 13/71 | LOSS: 3.7841287589149685e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 14/71 | LOSS: 3.810808824103636e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 15/71 | LOSS: 3.809595136772259e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 16/71 | LOSS: 3.804767381071168e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 17/71 | LOSS: 3.7586038969392474e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 18/71 | LOSS: 3.7788965097493734e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 19/71 | LOSS: 3.775246204895666e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 20/71 | LOSS: 3.803738443474729e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 21/71 | LOSS: 3.826758041792676e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 22/71 | LOSS: 3.9810795145888775e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 23/71 | LOSS: 3.975828985858243e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 24/71 | LOSS: 3.961572743719444e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 25/71 | LOSS: 3.993345210171985e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 26/71 | LOSS: 3.9803022648104365e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 27/71 | LOSS: 4.0934915627336265e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 28/71 | LOSS: 4.089777061820094e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 29/71 | LOSS: 4.0511464248993434e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 30/71 | LOSS: 4.037590124719446e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 31/71 | LOSS: 4.0396716713075875e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 32/71 | LOSS: 4.0183794088520564e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 33/71 | LOSS: 4.078526078100207e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 34/71 | LOSS: 4.117380324584831e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 35/71 | LOSS: 4.1002297520107175e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 36/71 | LOSS: 4.084071603043609e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 37/71 | LOSS: 4.038150090261632e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 38/71 | LOSS: 4.039242412545718e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 39/71 | LOSS: 4.023007904834231e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 40/71 | LOSS: 3.99402204316064e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 41/71 | LOSS: 4.014349750442696e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 42/71 | LOSS: 4.000131966042614e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 43/71 | LOSS: 3.9974959773561835e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 44/71 | LOSS: 4.084247339051217e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 45/71 | LOSS: 4.068298886008763e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 46/71 | LOSS: 4.079226717759082e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 47/71 | LOSS: 4.0509806998064356e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 48/71 | LOSS: 4.0395957196834594e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 49/71 | LOSS: 4.0860070439521225e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 50/71 | LOSS: 4.076104881938127e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 51/71 | LOSS: 4.075016407184124e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 52/71 | LOSS: 4.0777168115315396e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 53/71 | LOSS: 4.0539543786306036e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 54/71 | LOSS: 4.0897597500588745e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 55/71 | LOSS: 4.0944678208429e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 56/71 | LOSS: 4.0826414381929986e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 57/71 | LOSS: 4.059415362556545e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 58/71 | LOSS: 4.056872933426225e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 59/71 | LOSS: 4.1149052837378504e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 60/71 | LOSS: 4.0998699794309674e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 61/71 | LOSS: 4.0844227940749165e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 62/71 | LOSS: 4.081348887421117e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 63/71 | LOSS: 4.077108866340495e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 64/71 | LOSS: 4.065111979098919e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 65/71 | LOSS: 4.070733192554943e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 66/71 | LOSS: 4.0947968412169824e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 67/71 | LOSS: 4.096907051622539e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 68/71 | LOSS: 4.092851409897802e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 69/71 | LOSS: 4.0806636908590526e-05\n",
      "TRAIN: EPOCH 42/100 | BATCH 70/71 | LOSS: 4.080173199378203e-05\n",
      "VAL: EPOCH 42/100 | BATCH 0/8 | LOSS: 3.550633482518606e-05\n",
      "VAL: EPOCH 42/100 | BATCH 1/8 | LOSS: 3.18565325869713e-05\n",
      "VAL: EPOCH 42/100 | BATCH 2/8 | LOSS: 3.4039990471986435e-05\n",
      "VAL: EPOCH 42/100 | BATCH 3/8 | LOSS: 3.6398017982719466e-05\n",
      "VAL: EPOCH 42/100 | BATCH 4/8 | LOSS: 3.539439348969609e-05\n",
      "VAL: EPOCH 42/100 | BATCH 5/8 | LOSS: 3.48598356746758e-05\n",
      "VAL: EPOCH 42/100 | BATCH 6/8 | LOSS: 3.583725837026057e-05\n",
      "VAL: EPOCH 42/100 | BATCH 7/8 | LOSS: 3.555815601430368e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 0/71 | LOSS: 4.4279506255406886e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 1/71 | LOSS: 5.0605151045601815e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 2/71 | LOSS: 4.633502855237263e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 3/71 | LOSS: 4.579313190333778e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 4/71 | LOSS: 4.3449004442663865e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 5/71 | LOSS: 4.174713224832279e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 6/71 | LOSS: 4.01794895878993e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 7/71 | LOSS: 4.2072631913470104e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 8/71 | LOSS: 4.110067190291981e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 9/71 | LOSS: 4.018933104816824e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 10/71 | LOSS: 4.122581561783921e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 11/71 | LOSS: 4.201496540190419e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 12/71 | LOSS: 4.3113761561331696e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 13/71 | LOSS: 4.2964151102101565e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 14/71 | LOSS: 4.220069531584159e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 15/71 | LOSS: 4.17267349348549e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 16/71 | LOSS: 4.249181588152971e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 17/71 | LOSS: 4.195238984215798e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 18/71 | LOSS: 4.1523069261242394e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 19/71 | LOSS: 4.1166343180520926e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 20/71 | LOSS: 4.120621954262744e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 21/71 | LOSS: 4.084533661212349e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 22/71 | LOSS: 4.057826062426498e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 23/71 | LOSS: 4.021852373625734e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 24/71 | LOSS: 4.040233136038296e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 25/71 | LOSS: 3.9949832451104434e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 26/71 | LOSS: 3.979554761220546e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 27/71 | LOSS: 3.965259754425331e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 28/71 | LOSS: 3.952203449694809e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 29/71 | LOSS: 3.978133736382006e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 30/71 | LOSS: 3.9775595958476075e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 31/71 | LOSS: 3.9603463108051074e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 32/71 | LOSS: 3.95670849684393e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 33/71 | LOSS: 4.043992066726787e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 34/71 | LOSS: 4.0167797591753436e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 35/71 | LOSS: 4.03345772813029e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 36/71 | LOSS: 4.019280817915019e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 37/71 | LOSS: 4.0088611106176885e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 38/71 | LOSS: 3.9883554797061384e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 39/71 | LOSS: 3.970289449171105e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 40/71 | LOSS: 3.9351932594776946e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 41/71 | LOSS: 3.9194848579340725e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 42/71 | LOSS: 3.9337381379080524e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 43/71 | LOSS: 3.915758394544934e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 44/71 | LOSS: 4.0040185194811786e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 45/71 | LOSS: 3.974143893846422e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 46/71 | LOSS: 3.9628356117123557e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 47/71 | LOSS: 3.94715969302221e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 48/71 | LOSS: 3.927466638501477e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 49/71 | LOSS: 3.935849224944832e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 50/71 | LOSS: 3.925496678800532e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 51/71 | LOSS: 3.9282769543765775e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 52/71 | LOSS: 3.949085084797226e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 53/71 | LOSS: 3.945124313354932e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 54/71 | LOSS: 3.938201496732125e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 55/71 | LOSS: 3.9364734058316185e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 56/71 | LOSS: 3.915499824838582e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 57/71 | LOSS: 3.923653915488354e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 58/71 | LOSS: 3.9286599238008514e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 59/71 | LOSS: 3.923473323084181e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 60/71 | LOSS: 3.909332568940447e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 61/71 | LOSS: 3.8944005462971904e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 62/71 | LOSS: 3.9116079798467194e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 63/71 | LOSS: 3.889318259098218e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 64/71 | LOSS: 3.865025668906478e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 65/71 | LOSS: 3.8547235486806706e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 66/71 | LOSS: 3.849694860661263e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 67/71 | LOSS: 3.8473087175482174e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 68/71 | LOSS: 3.839003640704491e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 69/71 | LOSS: 3.8339238069186516e-05\n",
      "TRAIN: EPOCH 43/100 | BATCH 70/71 | LOSS: 3.80608619797014e-05\n",
      "VAL: EPOCH 43/100 | BATCH 0/8 | LOSS: 3.505260974634439e-05\n",
      "VAL: EPOCH 43/100 | BATCH 1/8 | LOSS: 3.1381343433167785e-05\n",
      "VAL: EPOCH 43/100 | BATCH 2/8 | LOSS: 3.3271887029210724e-05\n",
      "VAL: EPOCH 43/100 | BATCH 3/8 | LOSS: 3.4719021641649306e-05\n",
      "VAL: EPOCH 43/100 | BATCH 4/8 | LOSS: 3.342658383189701e-05\n",
      "VAL: EPOCH 43/100 | BATCH 5/8 | LOSS: 3.306534078243809e-05\n",
      "VAL: EPOCH 43/100 | BATCH 6/8 | LOSS: 3.380202386844238e-05\n",
      "VAL: EPOCH 43/100 | BATCH 7/8 | LOSS: 3.365159955137642e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 0/71 | LOSS: 2.6160683773923665e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 1/71 | LOSS: 3.812922659562901e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 2/71 | LOSS: 3.846714632042373e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 3/71 | LOSS: 3.632502284744987e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 4/71 | LOSS: 3.4335301097598855e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 5/71 | LOSS: 3.313767198657539e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 6/71 | LOSS: 3.381352222017345e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 7/71 | LOSS: 3.369419550836028e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 8/71 | LOSS: 3.3475356556462226e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 9/71 | LOSS: 3.531132151692873e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 10/71 | LOSS: 3.45505774351874e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 11/71 | LOSS: 3.4325911201449344e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 12/71 | LOSS: 3.389217356296005e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 13/71 | LOSS: 3.387530224634767e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 14/71 | LOSS: 3.335716961980021e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 15/71 | LOSS: 3.3571383823982615e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 16/71 | LOSS: 3.311125767461764e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 17/71 | LOSS: 3.371209752205889e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 18/71 | LOSS: 3.549630041800954e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 19/71 | LOSS: 3.50033325048571e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 20/71 | LOSS: 3.642222968448463e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 21/71 | LOSS: 3.609049301964908e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 22/71 | LOSS: 3.6241146596654765e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 23/71 | LOSS: 3.69059495521166e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 24/71 | LOSS: 3.6614381533581765e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 25/71 | LOSS: 3.7020037360399816e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 26/71 | LOSS: 3.709202617217131e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 27/71 | LOSS: 3.72215533518881e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 28/71 | LOSS: 3.7011755294174536e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 29/71 | LOSS: 3.6823419698824485e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 30/71 | LOSS: 3.680000490050823e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 31/71 | LOSS: 3.7267089282977395e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 32/71 | LOSS: 3.7309249102801196e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 33/71 | LOSS: 3.7401265859336814e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 34/71 | LOSS: 3.714954436873086e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 35/71 | LOSS: 3.7122439405518686e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 36/71 | LOSS: 3.723775109785886e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 37/71 | LOSS: 3.772863017634096e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 38/71 | LOSS: 3.7530744194578474e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 39/71 | LOSS: 3.73646894331614e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 40/71 | LOSS: 3.7323464334249586e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 41/71 | LOSS: 3.729668259482077e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 42/71 | LOSS: 3.715909389389116e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 43/71 | LOSS: 3.729502532223705e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 44/71 | LOSS: 3.715855095328556e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 45/71 | LOSS: 3.685160284120407e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 46/71 | LOSS: 3.666883175684872e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 47/71 | LOSS: 3.666279864470804e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 48/71 | LOSS: 3.6513811581392715e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 49/71 | LOSS: 3.633151598478435e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 50/71 | LOSS: 3.60990207435166e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 51/71 | LOSS: 3.603214613926516e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 52/71 | LOSS: 3.587923842956716e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 53/71 | LOSS: 3.5950673245005656e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 54/71 | LOSS: 3.5938812917183066e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 55/71 | LOSS: 3.600977119536505e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 56/71 | LOSS: 3.6007745636692434e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 57/71 | LOSS: 3.6100770275566147e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 58/71 | LOSS: 3.65333282882032e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 59/71 | LOSS: 3.6450338059997495e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 60/71 | LOSS: 3.630661847667204e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 61/71 | LOSS: 3.62291622525757e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 62/71 | LOSS: 3.641317437988676e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 63/71 | LOSS: 3.647893794322954e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 64/71 | LOSS: 3.6623306816461714e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 65/71 | LOSS: 3.645881737280764e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 66/71 | LOSS: 3.653846970201581e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 67/71 | LOSS: 3.649279947507346e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 68/71 | LOSS: 3.6445199404387615e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 69/71 | LOSS: 3.641553848865442e-05\n",
      "TRAIN: EPOCH 44/100 | BATCH 70/71 | LOSS: 3.6521933689585586e-05\n",
      "VAL: EPOCH 44/100 | BATCH 0/8 | LOSS: 3.454529360169545e-05\n",
      "VAL: EPOCH 44/100 | BATCH 1/8 | LOSS: 3.1451884751731995e-05\n",
      "VAL: EPOCH 44/100 | BATCH 2/8 | LOSS: 3.3034496179122165e-05\n",
      "VAL: EPOCH 44/100 | BATCH 3/8 | LOSS: 3.431668710618396e-05\n",
      "VAL: EPOCH 44/100 | BATCH 4/8 | LOSS: 3.319769566587638e-05\n",
      "VAL: EPOCH 44/100 | BATCH 5/8 | LOSS: 3.26025256072171e-05\n",
      "VAL: EPOCH 44/100 | BATCH 6/8 | LOSS: 3.331182753234836e-05\n",
      "VAL: EPOCH 44/100 | BATCH 7/8 | LOSS: 3.3055287076422246e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 0/71 | LOSS: 2.746897007455118e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 1/71 | LOSS: 2.6473580874153413e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 2/71 | LOSS: 3.0879382393322885e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 3/71 | LOSS: 3.092778206337243e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 4/71 | LOSS: 3.107514712610282e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 5/71 | LOSS: 3.326717281500654e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 6/71 | LOSS: 3.343500637648893e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 7/71 | LOSS: 3.397001546545653e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 8/71 | LOSS: 3.3647502681964804e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 9/71 | LOSS: 3.415783176023979e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 10/71 | LOSS: 3.325778710528869e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 11/71 | LOSS: 3.222176231550596e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 12/71 | LOSS: 3.3000935498141466e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 13/71 | LOSS: 3.43435559833389e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 14/71 | LOSS: 3.4837321679030234e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 15/71 | LOSS: 3.430671904425253e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 16/71 | LOSS: 3.450338964980534e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 17/71 | LOSS: 3.5980632674181834e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 18/71 | LOSS: 3.529903092145871e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 19/71 | LOSS: 3.497622201393824e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 20/71 | LOSS: 3.4437007256894415e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 21/71 | LOSS: 3.4528112419965595e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 22/71 | LOSS: 3.5030318742982395e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 23/71 | LOSS: 3.503740041802909e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 24/71 | LOSS: 3.4845556365326046e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 25/71 | LOSS: 3.489331440785183e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 26/71 | LOSS: 3.506689002582183e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 27/71 | LOSS: 3.524806899284678e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 28/71 | LOSS: 3.5349057599950325e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 29/71 | LOSS: 3.496933107574781e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 30/71 | LOSS: 3.533703889960664e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 31/71 | LOSS: 3.5225607234679046e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 32/71 | LOSS: 3.5000523039894745e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 33/71 | LOSS: 3.5340397302848834e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 34/71 | LOSS: 3.545180755151835e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 35/71 | LOSS: 3.5451955227472354e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 36/71 | LOSS: 3.5443711641238294e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 37/71 | LOSS: 3.519566254046942e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 38/71 | LOSS: 3.580927389124647e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 39/71 | LOSS: 3.554514464667591e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 40/71 | LOSS: 3.5542792297414185e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 41/71 | LOSS: 3.516086703062423e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 42/71 | LOSS: 3.6082101613892306e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 43/71 | LOSS: 3.6134663746038726e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 44/71 | LOSS: 3.5883386711551184e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 45/71 | LOSS: 3.5693542011595916e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 46/71 | LOSS: 3.568357099906512e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 47/71 | LOSS: 3.5424054165863104e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 48/71 | LOSS: 3.5296205020799514e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 49/71 | LOSS: 3.5269937725388446e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 50/71 | LOSS: 3.50409800910925e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 51/71 | LOSS: 3.502983995181589e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 52/71 | LOSS: 3.498018970345103e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 53/71 | LOSS: 3.47939249641732e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 54/71 | LOSS: 3.479518448082011e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 55/71 | LOSS: 3.4640169782765694e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 56/71 | LOSS: 3.454642944888563e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 57/71 | LOSS: 3.4511567436907326e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 58/71 | LOSS: 3.446846297311684e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 59/71 | LOSS: 3.4394694830552906e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 60/71 | LOSS: 3.433868023123187e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 61/71 | LOSS: 3.4323487001998844e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 62/71 | LOSS: 3.424077915533128e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 63/71 | LOSS: 3.423592394824482e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 64/71 | LOSS: 3.414991193634565e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 65/71 | LOSS: 3.4363607179936175e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 66/71 | LOSS: 3.450819248930419e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 67/71 | LOSS: 3.4511179873173683e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 68/71 | LOSS: 3.445338431144288e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 69/71 | LOSS: 3.464815534763537e-05\n",
      "TRAIN: EPOCH 45/100 | BATCH 70/71 | LOSS: 3.46229011432717e-05\n",
      "VAL: EPOCH 45/100 | BATCH 0/8 | LOSS: 3.234909308957867e-05\n",
      "VAL: EPOCH 45/100 | BATCH 1/8 | LOSS: 2.9507544240914285e-05\n",
      "VAL: EPOCH 45/100 | BATCH 2/8 | LOSS: 3.066099452553317e-05\n",
      "VAL: EPOCH 45/100 | BATCH 3/8 | LOSS: 3.2021426704886835e-05\n",
      "VAL: EPOCH 45/100 | BATCH 4/8 | LOSS: 3.1098834006115796e-05\n",
      "VAL: EPOCH 45/100 | BATCH 5/8 | LOSS: 3.068884416279616e-05\n",
      "VAL: EPOCH 45/100 | BATCH 6/8 | LOSS: 3.134020283011653e-05\n",
      "VAL: EPOCH 45/100 | BATCH 7/8 | LOSS: 3.1167765428108396e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 0/71 | LOSS: 2.063229476334527e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 1/71 | LOSS: 3.3933232771232724e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 2/71 | LOSS: 3.29621810427246e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 3/71 | LOSS: 3.172853212163318e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 4/71 | LOSS: 3.072024737775792e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 5/71 | LOSS: 3.0219840785624303e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 6/71 | LOSS: 2.9669214200112037e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 7/71 | LOSS: 3.131202424810908e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 8/71 | LOSS: 3.212511385047239e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 9/71 | LOSS: 3.2450982143927834e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 10/71 | LOSS: 3.4511522079066545e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 11/71 | LOSS: 3.363093568016969e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 12/71 | LOSS: 3.360349294850424e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 13/71 | LOSS: 3.330414191233493e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 14/71 | LOSS: 3.465466979832854e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 15/71 | LOSS: 3.417171205910563e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 16/71 | LOSS: 3.3759592470767745e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 17/71 | LOSS: 3.34031312579302e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 18/71 | LOSS: 3.307142833364196e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 19/71 | LOSS: 3.260699386373744e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 20/71 | LOSS: 3.266812364884009e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 21/71 | LOSS: 3.348039015029578e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 22/71 | LOSS: 3.339551079514659e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 23/71 | LOSS: 3.3041286163400706e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 24/71 | LOSS: 3.271557674452197e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 25/71 | LOSS: 3.392404260711583e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 26/71 | LOSS: 3.385698659662416e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 27/71 | LOSS: 3.371973727033557e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 28/71 | LOSS: 3.334236950242233e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 29/71 | LOSS: 3.312252829346107e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 30/71 | LOSS: 3.28674683377387e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 31/71 | LOSS: 3.289174924248073e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 32/71 | LOSS: 3.30086794830012e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 33/71 | LOSS: 3.306813146884772e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 34/71 | LOSS: 3.31223919472125e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 35/71 | LOSS: 3.297607973864716e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 36/71 | LOSS: 3.283496325083605e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 37/71 | LOSS: 3.332512240280936e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 38/71 | LOSS: 3.320724327073922e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 39/71 | LOSS: 3.309050421194115e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 40/71 | LOSS: 3.298380619437411e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 41/71 | LOSS: 3.291430779687029e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 42/71 | LOSS: 3.317861812193282e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 43/71 | LOSS: 3.324676084627175e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 44/71 | LOSS: 3.321323395842531e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 45/71 | LOSS: 3.324455250865709e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 46/71 | LOSS: 3.3163500158983146e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 47/71 | LOSS: 3.3058370793999835e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 48/71 | LOSS: 3.30500001930014e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 49/71 | LOSS: 3.3140791638288645e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 50/71 | LOSS: 3.354877830419105e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 51/71 | LOSS: 3.376815108752523e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 52/71 | LOSS: 3.356383010368346e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 53/71 | LOSS: 3.333577946991513e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 54/71 | LOSS: 3.341318836398634e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 55/71 | LOSS: 3.355376322490754e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 56/71 | LOSS: 3.3481267548682936e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 57/71 | LOSS: 3.346868743392406e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 58/71 | LOSS: 3.332394614625304e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 59/71 | LOSS: 3.32867027282191e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 60/71 | LOSS: 3.3320809663683714e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 61/71 | LOSS: 3.3080667303546494e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 62/71 | LOSS: 3.320427193044914e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 63/71 | LOSS: 3.310077525497945e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 64/71 | LOSS: 3.3115384716298786e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 65/71 | LOSS: 3.3073032213768315e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 66/71 | LOSS: 3.295449476272959e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 67/71 | LOSS: 3.294158194180722e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 68/71 | LOSS: 3.287469274031194e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 69/71 | LOSS: 3.282810069192367e-05\n",
      "TRAIN: EPOCH 46/100 | BATCH 70/71 | LOSS: 3.275702941427368e-05\n",
      "VAL: EPOCH 46/100 | BATCH 0/8 | LOSS: 3.0176744985510595e-05\n",
      "VAL: EPOCH 46/100 | BATCH 1/8 | LOSS: 2.8076312446501106e-05\n",
      "VAL: EPOCH 46/100 | BATCH 2/8 | LOSS: 2.933587408430564e-05\n",
      "VAL: EPOCH 46/100 | BATCH 3/8 | LOSS: 3.069694685109425e-05\n",
      "VAL: EPOCH 46/100 | BATCH 4/8 | LOSS: 2.9880077636335046e-05\n",
      "VAL: EPOCH 46/100 | BATCH 5/8 | LOSS: 2.946883675273663e-05\n",
      "VAL: EPOCH 46/100 | BATCH 6/8 | LOSS: 3.0226626612212775e-05\n",
      "VAL: EPOCH 46/100 | BATCH 7/8 | LOSS: 2.993875159518211e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 0/71 | LOSS: 2.206413591920864e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 1/71 | LOSS: 2.7701310500560794e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 2/71 | LOSS: 2.9817667988633428e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 3/71 | LOSS: 3.26488211612741e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 4/71 | LOSS: 3.380828238732647e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 5/71 | LOSS: 3.3884618157268655e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 6/71 | LOSS: 3.616940609520368e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 7/71 | LOSS: 3.5752005942413234e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 8/71 | LOSS: 3.588150159177733e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 9/71 | LOSS: 3.5843781552102884e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 10/71 | LOSS: 3.529117665576368e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 11/71 | LOSS: 3.488991615085979e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 12/71 | LOSS: 3.48252146627951e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 13/71 | LOSS: 3.6197621869048036e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 14/71 | LOSS: 3.560518792558772e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 15/71 | LOSS: 3.6172422596791876e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 16/71 | LOSS: 3.671076731681002e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 17/71 | LOSS: 3.678644558628245e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 18/71 | LOSS: 3.6491654961872355e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 19/71 | LOSS: 3.618818373070098e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 20/71 | LOSS: 3.552732457700629e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 21/71 | LOSS: 3.52405388019344e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 22/71 | LOSS: 3.485148179448594e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 23/71 | LOSS: 3.506791449581215e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 24/71 | LOSS: 3.5028101774514655e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 25/71 | LOSS: 3.538045033425218e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 26/71 | LOSS: 3.496709683127442e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 27/71 | LOSS: 3.486980527408637e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 28/71 | LOSS: 3.452019242605933e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 29/71 | LOSS: 3.4236597396860206e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 30/71 | LOSS: 3.432025762102116e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 31/71 | LOSS: 3.432507605793944e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 32/71 | LOSS: 3.455887560972549e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 33/71 | LOSS: 3.458892216953887e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 34/71 | LOSS: 3.4687083721759595e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 35/71 | LOSS: 3.4651898406284294e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 36/71 | LOSS: 3.460758799101805e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 37/71 | LOSS: 3.44774820596764e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 38/71 | LOSS: 3.442259642328374e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 39/71 | LOSS: 3.433553329159622e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 40/71 | LOSS: 3.438480292438393e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 41/71 | LOSS: 3.42535241933157e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 42/71 | LOSS: 3.429464578762553e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 43/71 | LOSS: 3.399373351633337e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 44/71 | LOSS: 3.395835891650576e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 45/71 | LOSS: 3.411319931214878e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 46/71 | LOSS: 3.400860801543495e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 47/71 | LOSS: 3.3991519141333505e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 48/71 | LOSS: 3.3918258739096035e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 49/71 | LOSS: 3.3791515088523736e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 50/71 | LOSS: 3.370271584643371e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 51/71 | LOSS: 3.3699736762243374e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 52/71 | LOSS: 3.363260372491325e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 53/71 | LOSS: 3.345113071662598e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 54/71 | LOSS: 3.332172077948185e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 55/71 | LOSS: 3.319570620468377e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 56/71 | LOSS: 3.308423622097747e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 57/71 | LOSS: 3.277902023596809e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 58/71 | LOSS: 3.278752271815053e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 59/71 | LOSS: 3.2739602738729444e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 60/71 | LOSS: 3.261841680557795e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 61/71 | LOSS: 3.250620595222685e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 62/71 | LOSS: 3.2488238387500926e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 63/71 | LOSS: 3.2368159963880316e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 64/71 | LOSS: 3.2463826601787544e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 65/71 | LOSS: 3.2426097746021256e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 66/71 | LOSS: 3.2315041363602435e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 67/71 | LOSS: 3.2157563376077794e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 68/71 | LOSS: 3.22487000854723e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 69/71 | LOSS: 3.2233983360388914e-05\n",
      "TRAIN: EPOCH 47/100 | BATCH 70/71 | LOSS: 3.19486350018054e-05\n",
      "VAL: EPOCH 47/100 | BATCH 0/8 | LOSS: 3.067313809879124e-05\n",
      "VAL: EPOCH 47/100 | BATCH 1/8 | LOSS: 2.692343514354434e-05\n",
      "VAL: EPOCH 47/100 | BATCH 2/8 | LOSS: 2.813072327019957e-05\n",
      "VAL: EPOCH 47/100 | BATCH 3/8 | LOSS: 2.9416130018944386e-05\n",
      "VAL: EPOCH 47/100 | BATCH 4/8 | LOSS: 2.861836910597049e-05\n",
      "VAL: EPOCH 47/100 | BATCH 5/8 | LOSS: 2.792940419264293e-05\n",
      "VAL: EPOCH 47/100 | BATCH 6/8 | LOSS: 2.8586458418950705e-05\n",
      "VAL: EPOCH 47/100 | BATCH 7/8 | LOSS: 2.8499837981144083e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 0/71 | LOSS: 3.925258351955563e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 1/71 | LOSS: 4.040562635054812e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 2/71 | LOSS: 3.581863287157224e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 3/71 | LOSS: 3.36419175255287e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 4/71 | LOSS: 3.373216277395841e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 5/71 | LOSS: 3.335570697042082e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 6/71 | LOSS: 3.198641882460963e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 7/71 | LOSS: 3.119792199868243e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 8/71 | LOSS: 3.330420602449319e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 9/71 | LOSS: 3.28639151121024e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 10/71 | LOSS: 3.2732457012488425e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 11/71 | LOSS: 3.2305951208400074e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 12/71 | LOSS: 3.12512038362911e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 13/71 | LOSS: 3.1205418963509146e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 14/71 | LOSS: 3.054714264483967e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 15/71 | LOSS: 3.0374399671018182e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 16/71 | LOSS: 3.06112983440364e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 17/71 | LOSS: 3.0327210071037472e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 18/71 | LOSS: 3.053365981031675e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 19/71 | LOSS: 3.0327529657370178e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 20/71 | LOSS: 3.0233227464902613e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 21/71 | LOSS: 2.985456177901307e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 22/71 | LOSS: 2.9941067148921203e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 23/71 | LOSS: 2.998105128426687e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 24/71 | LOSS: 2.9712806281168014e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 25/71 | LOSS: 2.9599930432875855e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 26/71 | LOSS: 2.9916399162849068e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 27/71 | LOSS: 2.9762341975063983e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 28/71 | LOSS: 2.9523493328175892e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 29/71 | LOSS: 2.941306417293769e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 30/71 | LOSS: 2.9523457405664358e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 31/71 | LOSS: 2.959079739639492e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 32/71 | LOSS: 2.9863828978815228e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 33/71 | LOSS: 2.979103047597592e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 34/71 | LOSS: 3.0125638451344066e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 35/71 | LOSS: 3.022464802092549e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 36/71 | LOSS: 3.0231653196293535e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 37/71 | LOSS: 3.029346096713868e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 38/71 | LOSS: 3.0208127310078664e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 39/71 | LOSS: 3.0240351816246402e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 40/71 | LOSS: 3.0116028716381663e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 41/71 | LOSS: 3.008985017264162e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 42/71 | LOSS: 3.0031998353796403e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 43/71 | LOSS: 3.0064896856691288e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 44/71 | LOSS: 3.004286938650896e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 45/71 | LOSS: 3.0061852592790156e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 46/71 | LOSS: 3.011025418868001e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 47/71 | LOSS: 3.0212798530252865e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 48/71 | LOSS: 3.035657124312319e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 49/71 | LOSS: 3.0298720848804804e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 50/71 | LOSS: 3.0501113524580576e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 51/71 | LOSS: 3.0415620270874708e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 52/71 | LOSS: 3.0561730626395045e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 53/71 | LOSS: 3.048038521498071e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 54/71 | LOSS: 3.053142278407574e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 55/71 | LOSS: 3.0563890212371394e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 56/71 | LOSS: 3.0383772402848608e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 57/71 | LOSS: 3.0466341615528077e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 58/71 | LOSS: 3.0405346879595497e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 59/71 | LOSS: 3.0464198819875794e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 60/71 | LOSS: 3.058683841282189e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 61/71 | LOSS: 3.0559284442138195e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 62/71 | LOSS: 3.068493369648913e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 63/71 | LOSS: 3.0709863438005414e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 64/71 | LOSS: 3.06798551789073e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 65/71 | LOSS: 3.0863092213235866e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 66/71 | LOSS: 3.0882884545204817e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 67/71 | LOSS: 3.107303101548014e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 68/71 | LOSS: 3.097240717223663e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 69/71 | LOSS: 3.0879584898814624e-05\n",
      "TRAIN: EPOCH 48/100 | BATCH 70/71 | LOSS: 3.08845730461131e-05\n",
      "VAL: EPOCH 48/100 | BATCH 0/8 | LOSS: 3.4400756703689694e-05\n",
      "VAL: EPOCH 48/100 | BATCH 1/8 | LOSS: 3.091525468335021e-05\n",
      "VAL: EPOCH 48/100 | BATCH 2/8 | LOSS: 3.066934489955505e-05\n",
      "VAL: EPOCH 48/100 | BATCH 3/8 | LOSS: 3.1771959584148135e-05\n",
      "VAL: EPOCH 48/100 | BATCH 4/8 | LOSS: 3.126070441794582e-05\n",
      "VAL: EPOCH 48/100 | BATCH 5/8 | LOSS: 3.052782509863997e-05\n",
      "VAL: EPOCH 48/100 | BATCH 6/8 | LOSS: 3.0955165649564674e-05\n",
      "VAL: EPOCH 48/100 | BATCH 7/8 | LOSS: 3.0747614118809e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 0/71 | LOSS: 3.894740075338632e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 1/71 | LOSS: 3.415346327528823e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 2/71 | LOSS: 3.242338304213869e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 3/71 | LOSS: 3.6708751395053696e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 4/71 | LOSS: 3.760846666409634e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 5/71 | LOSS: 3.85010880563641e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 6/71 | LOSS: 3.9250422040432955e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 7/71 | LOSS: 3.79553962375212e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 8/71 | LOSS: 3.7792156136775804e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 9/71 | LOSS: 3.811913884419482e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 10/71 | LOSS: 3.738807737880217e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 11/71 | LOSS: 3.6434400650856937e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 12/71 | LOSS: 3.589467171248263e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 13/71 | LOSS: 3.5658559032266825e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 14/71 | LOSS: 3.535234573064372e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 15/71 | LOSS: 3.452256896707695e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 16/71 | LOSS: 3.410668839077356e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 17/71 | LOSS: 3.3589265715110945e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 18/71 | LOSS: 3.400768649666325e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 19/71 | LOSS: 3.3604837426537415e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 20/71 | LOSS: 3.3020711877422095e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 21/71 | LOSS: 3.3190130538555835e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 22/71 | LOSS: 3.3105732971149415e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 23/71 | LOSS: 3.263586343867549e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 24/71 | LOSS: 3.228783018130343e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 25/71 | LOSS: 3.191202134845438e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 26/71 | LOSS: 3.1495353983633254e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 27/71 | LOSS: 3.1240162245792036e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 28/71 | LOSS: 3.0949810087773146e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 29/71 | LOSS: 3.096655291301431e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 30/71 | LOSS: 3.059231705499828e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 31/71 | LOSS: 3.03510036019361e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 32/71 | LOSS: 3.040960126037879e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 33/71 | LOSS: 3.0276080478335428e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 34/71 | LOSS: 3.0148569879072185e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 35/71 | LOSS: 2.9988238060872794e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 36/71 | LOSS: 3.0106892891863765e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 37/71 | LOSS: 3.0086504203178554e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 38/71 | LOSS: 3.0084307861845726e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 39/71 | LOSS: 2.9956955904708595e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 40/71 | LOSS: 2.980066586510142e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 41/71 | LOSS: 2.9950980595978243e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 42/71 | LOSS: 2.9832314392605928e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 43/71 | LOSS: 3.0196850606113333e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 44/71 | LOSS: 3.0104627391362253e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 45/71 | LOSS: 2.996259635161234e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 46/71 | LOSS: 2.981779393752383e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 47/71 | LOSS: 2.9812978937115986e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 48/71 | LOSS: 2.9836385599955232e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 49/71 | LOSS: 2.9784249963995535e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 50/71 | LOSS: 2.976341384768888e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 51/71 | LOSS: 2.967906539197429e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 52/71 | LOSS: 2.9726872457584523e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 53/71 | LOSS: 2.959738368965056e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 54/71 | LOSS: 2.955240342998877e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 55/71 | LOSS: 2.9524740739361732e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 56/71 | LOSS: 2.9414465595772e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 57/71 | LOSS: 2.9364450366019497e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 58/71 | LOSS: 2.9491441621804233e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 59/71 | LOSS: 2.953391406966451e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 60/71 | LOSS: 2.9665302031016985e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 61/71 | LOSS: 2.950939444652308e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 62/71 | LOSS: 2.9547732068865103e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 63/71 | LOSS: 2.9602052563859615e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 64/71 | LOSS: 2.9571700305006322e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 65/71 | LOSS: 2.966771367165634e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 66/71 | LOSS: 2.9611992892989918e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 67/71 | LOSS: 2.957976603818893e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 68/71 | LOSS: 2.9448978370899145e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 69/71 | LOSS: 2.9441726188192012e-05\n",
      "TRAIN: EPOCH 49/100 | BATCH 70/71 | LOSS: 2.9322691210529204e-05\n",
      "VAL: EPOCH 49/100 | BATCH 0/8 | LOSS: 3.01089294225676e-05\n",
      "VAL: EPOCH 49/100 | BATCH 1/8 | LOSS: 2.7478356059873477e-05\n",
      "VAL: EPOCH 49/100 | BATCH 2/8 | LOSS: 2.8195951017551124e-05\n",
      "VAL: EPOCH 49/100 | BATCH 3/8 | LOSS: 2.894903445849195e-05\n",
      "VAL: EPOCH 49/100 | BATCH 4/8 | LOSS: 2.8080684569431468e-05\n",
      "VAL: EPOCH 49/100 | BATCH 5/8 | LOSS: 2.7465852933043305e-05\n",
      "VAL: EPOCH 49/100 | BATCH 6/8 | LOSS: 2.788281407057574e-05\n",
      "VAL: EPOCH 49/100 | BATCH 7/8 | LOSS: 2.759881272140774e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 0/71 | LOSS: 2.7959940780419856e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 1/71 | LOSS: 2.4226003006333485e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 2/71 | LOSS: 2.5976539594315302e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 3/71 | LOSS: 2.6888257707469165e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 4/71 | LOSS: 2.6129831530852245e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 5/71 | LOSS: 2.6230229802119236e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 6/71 | LOSS: 2.7140491251235028e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 7/71 | LOSS: 2.630114340718137e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 8/71 | LOSS: 2.5955273789198447e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 9/71 | LOSS: 2.60975935816532e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 10/71 | LOSS: 2.6159656871724028e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 11/71 | LOSS: 2.544967052623785e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 12/71 | LOSS: 2.7221139484586623e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 13/71 | LOSS: 2.6885061483231505e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 14/71 | LOSS: 2.7016496460419148e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 15/71 | LOSS: 2.6972842420036613e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 16/71 | LOSS: 2.737730788152583e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 17/71 | LOSS: 2.7059697964028197e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 18/71 | LOSS: 2.7049005726248172e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 19/71 | LOSS: 2.767517098618555e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 20/71 | LOSS: 2.8052357135623295e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 21/71 | LOSS: 2.8516787923846014e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 22/71 | LOSS: 2.8453272455447838e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 23/71 | LOSS: 2.8358696681607398e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 24/71 | LOSS: 2.8215584534336814e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 25/71 | LOSS: 2.8345495103656922e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 26/71 | LOSS: 2.814210172261853e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 27/71 | LOSS: 2.793662412971441e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 28/71 | LOSS: 2.8195133413176116e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 29/71 | LOSS: 2.8255521283426787e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 30/71 | LOSS: 2.816452251863666e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 31/71 | LOSS: 2.8171029043733142e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 32/71 | LOSS: 2.7892926564990457e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 33/71 | LOSS: 2.7758987765564062e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 34/71 | LOSS: 2.7632940171835277e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 35/71 | LOSS: 2.7486481131846733e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 36/71 | LOSS: 2.754558443337226e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 37/71 | LOSS: 2.7926853554641927e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 38/71 | LOSS: 2.7775252769620587e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 39/71 | LOSS: 2.7730079909815687e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 40/71 | LOSS: 2.765106932556813e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 41/71 | LOSS: 2.7601953098339782e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 42/71 | LOSS: 2.753624713202657e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 43/71 | LOSS: 2.782297145710221e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 44/71 | LOSS: 2.771037773830661e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 45/71 | LOSS: 2.762777192243527e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 46/71 | LOSS: 2.7544234863524522e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 47/71 | LOSS: 2.755118820611339e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 48/71 | LOSS: 2.760191202877631e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 49/71 | LOSS: 2.7563584844756405e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 50/71 | LOSS: 2.755637087473445e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 51/71 | LOSS: 2.7518208123812936e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 52/71 | LOSS: 2.738389947516448e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 53/71 | LOSS: 2.729519699776709e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 54/71 | LOSS: 2.745914730026429e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 55/71 | LOSS: 2.7382650484599124e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 56/71 | LOSS: 2.7391329447773548e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 57/71 | LOSS: 2.7476691590483574e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 58/71 | LOSS: 2.752233660307326e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 59/71 | LOSS: 2.7846164478736075e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 60/71 | LOSS: 2.7785445454952184e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 61/71 | LOSS: 2.784729032658018e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 62/71 | LOSS: 2.7764815910366393e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 63/71 | LOSS: 2.767772585343664e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 64/71 | LOSS: 2.7668753962363832e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 65/71 | LOSS: 2.7926227217407856e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 66/71 | LOSS: 2.7876781281683275e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 67/71 | LOSS: 2.7854364896698182e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 68/71 | LOSS: 2.790945657487601e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 69/71 | LOSS: 2.7829434189438222e-05\n",
      "TRAIN: EPOCH 50/100 | BATCH 70/71 | LOSS: 2.775950469839072e-05\n",
      "VAL: EPOCH 50/100 | BATCH 0/8 | LOSS: 2.640007369336672e-05\n",
      "VAL: EPOCH 50/100 | BATCH 1/8 | LOSS: 2.3057407815940678e-05\n",
      "VAL: EPOCH 50/100 | BATCH 2/8 | LOSS: 2.497653622413054e-05\n",
      "VAL: EPOCH 50/100 | BATCH 3/8 | LOSS: 2.6598996555549093e-05\n",
      "VAL: EPOCH 50/100 | BATCH 4/8 | LOSS: 2.6112319028470664e-05\n",
      "VAL: EPOCH 50/100 | BATCH 5/8 | LOSS: 2.539963406889001e-05\n",
      "VAL: EPOCH 50/100 | BATCH 6/8 | LOSS: 2.6139641444647816e-05\n",
      "VAL: EPOCH 50/100 | BATCH 7/8 | LOSS: 2.5999259150921716e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 0/71 | LOSS: 3.867239865940064e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 1/71 | LOSS: 3.2300386010319926e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 2/71 | LOSS: 2.8853127029530395e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 3/71 | LOSS: 2.7177004540135385e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 4/71 | LOSS: 2.6557573073660022e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 5/71 | LOSS: 2.486883416471149e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 6/71 | LOSS: 2.4703743682558915e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 7/71 | LOSS: 2.5975436756198178e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 8/71 | LOSS: 2.546960503322124e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 9/71 | LOSS: 2.5654741148173343e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 10/71 | LOSS: 2.58205891441321e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 11/71 | LOSS: 2.597767115730676e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 12/71 | LOSS: 2.581913829019938e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 13/71 | LOSS: 2.5312497330430362e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 14/71 | LOSS: 2.5402808508564096e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 15/71 | LOSS: 2.5079696570173837e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 16/71 | LOSS: 2.4860064299633342e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 17/71 | LOSS: 2.502585893024742e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 18/71 | LOSS: 2.5008613527844365e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 19/71 | LOSS: 2.4896649028960384e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 20/71 | LOSS: 2.491638423678177e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 21/71 | LOSS: 2.4904452898640144e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 22/71 | LOSS: 2.548889611004953e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 23/71 | LOSS: 2.555295418460446e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 24/71 | LOSS: 2.573462938016746e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 25/71 | LOSS: 2.566609006092991e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 26/71 | LOSS: 2.5571092852192965e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 27/71 | LOSS: 2.5493182160321988e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 28/71 | LOSS: 2.5409293184879813e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 29/71 | LOSS: 2.530387458439994e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 30/71 | LOSS: 2.5687650591979975e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 31/71 | LOSS: 2.6290531934591854e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 32/71 | LOSS: 2.6231278807796173e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 33/71 | LOSS: 2.6158943268562915e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 34/71 | LOSS: 2.6150159257148127e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 35/71 | LOSS: 2.632184406239604e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 36/71 | LOSS: 2.6427827047096013e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 37/71 | LOSS: 2.6432922966609784e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 38/71 | LOSS: 2.633725787932352e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 39/71 | LOSS: 2.6324416239731363e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 40/71 | LOSS: 2.6453136586763034e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 41/71 | LOSS: 2.6405833956405766e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 42/71 | LOSS: 2.6316285203752478e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 43/71 | LOSS: 2.63268515564877e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 44/71 | LOSS: 2.628967093995824e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 45/71 | LOSS: 2.6315902252226014e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 46/71 | LOSS: 2.6353013534854325e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 47/71 | LOSS: 2.6477530544373924e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 48/71 | LOSS: 2.6648587849269126e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 49/71 | LOSS: 2.6560509395494593e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 50/71 | LOSS: 2.67263574856128e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 51/71 | LOSS: 2.6611716223739608e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 52/71 | LOSS: 2.6781126267314045e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 53/71 | LOSS: 2.6688954642816464e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 54/71 | LOSS: 2.6589128240382044e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 55/71 | LOSS: 2.6437903540811802e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 56/71 | LOSS: 2.6500729482473905e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 57/71 | LOSS: 2.678953343751471e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 58/71 | LOSS: 2.672908821126438e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 59/71 | LOSS: 2.6794307844587215e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 60/71 | LOSS: 2.6757931380893377e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 61/71 | LOSS: 2.6752358163291994e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 62/71 | LOSS: 2.688857787847704e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 63/71 | LOSS: 2.688651201765424e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 64/71 | LOSS: 2.690464783741985e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 65/71 | LOSS: 2.693512989494731e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 66/71 | LOSS: 2.688039774566006e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 67/71 | LOSS: 2.7002588654688545e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 68/71 | LOSS: 2.7018149078197684e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 69/71 | LOSS: 2.6942144566939014e-05\n",
      "TRAIN: EPOCH 51/100 | BATCH 70/71 | LOSS: 2.7058145809553e-05\n",
      "VAL: EPOCH 51/100 | BATCH 0/8 | LOSS: 3.149875556118786e-05\n",
      "VAL: EPOCH 51/100 | BATCH 1/8 | LOSS: 2.9145794542273507e-05\n",
      "VAL: EPOCH 51/100 | BATCH 2/8 | LOSS: 2.9285994969541207e-05\n",
      "VAL: EPOCH 51/100 | BATCH 3/8 | LOSS: 2.981459238071693e-05\n",
      "VAL: EPOCH 51/100 | BATCH 4/8 | LOSS: 2.90346892143134e-05\n",
      "VAL: EPOCH 51/100 | BATCH 5/8 | LOSS: 2.8310547956304315e-05\n",
      "VAL: EPOCH 51/100 | BATCH 6/8 | LOSS: 2.849098927981686e-05\n",
      "VAL: EPOCH 51/100 | BATCH 7/8 | LOSS: 2.8164844025013736e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 0/71 | LOSS: 2.6527297450229526e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 1/71 | LOSS: 2.8582388040376827e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 2/71 | LOSS: 2.8630650679891307e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 3/71 | LOSS: 2.8038954496878432e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 4/71 | LOSS: 3.104244569840375e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 5/71 | LOSS: 3.0358887973610155e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 6/71 | LOSS: 2.995020622620359e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 7/71 | LOSS: 2.991482961078873e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 8/71 | LOSS: 2.9761364001185735e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 9/71 | LOSS: 2.9040298977633937e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 10/71 | LOSS: 2.901145969438155e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 11/71 | LOSS: 2.8743893229451107e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 12/71 | LOSS: 2.8668900715554348e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 13/71 | LOSS: 2.8470961556844747e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 14/71 | LOSS: 2.8336660398053938e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 15/71 | LOSS: 2.814804463469045e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 16/71 | LOSS: 2.8024015324156018e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 17/71 | LOSS: 2.7699349225763905e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 18/71 | LOSS: 2.753050849653511e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 19/71 | LOSS: 2.7336979383107973e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 20/71 | LOSS: 2.735822263708715e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 21/71 | LOSS: 2.7098724951263815e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 22/71 | LOSS: 2.7075684161188647e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 23/71 | LOSS: 2.6689260948842275e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 24/71 | LOSS: 2.6685496122809128e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 25/71 | LOSS: 2.6611738016631884e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 26/71 | LOSS: 2.6382702748782725e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 27/71 | LOSS: 2.6564924805175645e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 28/71 | LOSS: 2.65389127638753e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 29/71 | LOSS: 2.6304471733359e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 30/71 | LOSS: 2.641494962341723e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 31/71 | LOSS: 2.6610411168803694e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 32/71 | LOSS: 2.657691385882737e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 33/71 | LOSS: 2.6593254433999607e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 34/71 | LOSS: 2.6555886246829425e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 35/71 | LOSS: 2.6469231266269668e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 36/71 | LOSS: 2.6347480416990112e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 37/71 | LOSS: 2.6268732271043854e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 38/71 | LOSS: 2.6506748420219177e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 39/71 | LOSS: 2.682385634216189e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 40/71 | LOSS: 2.6793433124378393e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 41/71 | LOSS: 2.680379938759697e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 42/71 | LOSS: 2.666853516678012e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 43/71 | LOSS: 2.6689181636770215e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 44/71 | LOSS: 2.6576644894602293e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 45/71 | LOSS: 2.653126597104584e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 46/71 | LOSS: 2.6549994529093654e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 47/71 | LOSS: 2.6451726284903998e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 48/71 | LOSS: 2.6347881393288548e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 49/71 | LOSS: 2.6465547271072865e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 50/71 | LOSS: 2.6372275504970685e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 51/71 | LOSS: 2.642200829191447e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 52/71 | LOSS: 2.6242512659628806e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 53/71 | LOSS: 2.6173659669573816e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 54/71 | LOSS: 2.606932235225527e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 55/71 | LOSS: 2.6130265333839426e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 56/71 | LOSS: 2.6069827487063185e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 57/71 | LOSS: 2.6078673068570486e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 58/71 | LOSS: 2.6094719387247558e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 59/71 | LOSS: 2.616969874604062e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 60/71 | LOSS: 2.6111862814000457e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 61/71 | LOSS: 2.61939868294341e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 62/71 | LOSS: 2.6093477197083835e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 63/71 | LOSS: 2.612645056387919e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 64/71 | LOSS: 2.605312824576126e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 65/71 | LOSS: 2.6085183476838708e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 66/71 | LOSS: 2.5952815474792898e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 67/71 | LOSS: 2.590310195420684e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 68/71 | LOSS: 2.591192756169577e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 69/71 | LOSS: 2.592066518575718e-05\n",
      "TRAIN: EPOCH 52/100 | BATCH 70/71 | LOSS: 2.595437281727332e-05\n",
      "VAL: EPOCH 52/100 | BATCH 0/8 | LOSS: 2.9157439712435007e-05\n",
      "VAL: EPOCH 52/100 | BATCH 1/8 | LOSS: 2.4596196453785524e-05\n",
      "VAL: EPOCH 52/100 | BATCH 2/8 | LOSS: 2.577588747953996e-05\n",
      "VAL: EPOCH 52/100 | BATCH 3/8 | LOSS: 2.6421535039844457e-05\n",
      "VAL: EPOCH 52/100 | BATCH 4/8 | LOSS: 2.5608275973354466e-05\n",
      "VAL: EPOCH 52/100 | BATCH 5/8 | LOSS: 2.48734052850826e-05\n",
      "VAL: EPOCH 52/100 | BATCH 6/8 | LOSS: 2.541949834267143e-05\n",
      "VAL: EPOCH 52/100 | BATCH 7/8 | LOSS: 2.5501254185655853e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 0/71 | LOSS: 2.128433516190853e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 1/71 | LOSS: 2.2265247935138177e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 2/71 | LOSS: 2.93943345847462e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 3/71 | LOSS: 3.0236814836825943e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 4/71 | LOSS: 2.9340589753701353e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 5/71 | LOSS: 2.7535363187780604e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 6/71 | LOSS: 2.6629019495365875e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 7/71 | LOSS: 2.70920872935676e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 8/71 | LOSS: 2.6967960366164334e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 9/71 | LOSS: 2.6144215917156542e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 10/71 | LOSS: 2.5875078129254028e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 11/71 | LOSS: 2.610746211454777e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 12/71 | LOSS: 2.5579193858832765e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 13/71 | LOSS: 2.5771437062108555e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 14/71 | LOSS: 2.553369874173465e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 15/71 | LOSS: 2.514583479751309e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 16/71 | LOSS: 2.5171989845522842e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 17/71 | LOSS: 2.542037832932288e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 18/71 | LOSS: 2.5267664903750348e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 19/71 | LOSS: 2.521931855881121e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 20/71 | LOSS: 2.6321213919713738e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 21/71 | LOSS: 2.6359930746945214e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 22/71 | LOSS: 2.6516430174840775e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 23/71 | LOSS: 2.6326651550334645e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 24/71 | LOSS: 2.6418141787871717e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 25/71 | LOSS: 2.6382947558326683e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 26/71 | LOSS: 2.6167718749126868e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 27/71 | LOSS: 2.6032805505695122e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 28/71 | LOSS: 2.5851444849555765e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 29/71 | LOSS: 2.5715295547949304e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 30/71 | LOSS: 2.569360537651641e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 31/71 | LOSS: 2.578390069629677e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 32/71 | LOSS: 2.5681136652234603e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 33/71 | LOSS: 2.5637844480704243e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 34/71 | LOSS: 2.5617719698597544e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 35/71 | LOSS: 2.572674748080317e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 36/71 | LOSS: 2.5677674932159937e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 37/71 | LOSS: 2.5852982281848152e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 38/71 | LOSS: 2.586237622801071e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 39/71 | LOSS: 2.6038489886559545e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 40/71 | LOSS: 2.5974453109949713e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 41/71 | LOSS: 2.5921643880012978e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 42/71 | LOSS: 2.5873297111734986e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 43/71 | LOSS: 2.572539655044569e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 44/71 | LOSS: 2.573417114035692e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 45/71 | LOSS: 2.592204102414974e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 46/71 | LOSS: 2.5903706897379573e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 47/71 | LOSS: 2.577348743670882e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 48/71 | LOSS: 2.5773281758658264e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 49/71 | LOSS: 2.5631329881434793e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 50/71 | LOSS: 2.545895803646714e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 51/71 | LOSS: 2.5453849397868347e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 52/71 | LOSS: 2.5704009667091634e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 53/71 | LOSS: 2.5633888071752153e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 54/71 | LOSS: 2.5555755068622107e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 55/71 | LOSS: 2.550661614415211e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 56/71 | LOSS: 2.5537893458947603e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 57/71 | LOSS: 2.563633624331801e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 58/71 | LOSS: 2.5649551975282336e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 59/71 | LOSS: 2.5593316756082156e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 60/71 | LOSS: 2.5609708722722793e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 61/71 | LOSS: 2.5567431381909223e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 62/71 | LOSS: 2.551773838012383e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 63/71 | LOSS: 2.5494384203739173e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 64/71 | LOSS: 2.5484895517225735e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 65/71 | LOSS: 2.549398028129486e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 66/71 | LOSS: 2.5443220815148705e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 67/71 | LOSS: 2.5393171734536093e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 68/71 | LOSS: 2.5367408733987048e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 69/71 | LOSS: 2.5352382674879795e-05\n",
      "TRAIN: EPOCH 53/100 | BATCH 70/71 | LOSS: 2.5211480892145298e-05\n",
      "VAL: EPOCH 53/100 | BATCH 0/8 | LOSS: 2.4765027774265036e-05\n",
      "VAL: EPOCH 53/100 | BATCH 1/8 | LOSS: 2.155957008653786e-05\n",
      "VAL: EPOCH 53/100 | BATCH 2/8 | LOSS: 2.2615361255399573e-05\n",
      "VAL: EPOCH 53/100 | BATCH 3/8 | LOSS: 2.3418447653966723e-05\n",
      "VAL: EPOCH 53/100 | BATCH 4/8 | LOSS: 2.2939316477277315e-05\n",
      "VAL: EPOCH 53/100 | BATCH 5/8 | LOSS: 2.230707256482371e-05\n",
      "VAL: EPOCH 53/100 | BATCH 6/8 | LOSS: 2.2753035279622835e-05\n",
      "VAL: EPOCH 53/100 | BATCH 7/8 | LOSS: 2.2763815650250763e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 0/71 | LOSS: 1.8801900296239182e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 1/71 | LOSS: 1.9907729438273236e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 2/71 | LOSS: 2.2633449286028433e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 3/71 | LOSS: 2.4378057332796743e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 4/71 | LOSS: 2.5331808865303172e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 5/71 | LOSS: 2.5409522701617487e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 6/71 | LOSS: 2.4948129846182254e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 7/71 | LOSS: 2.409026774330414e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 8/71 | LOSS: 2.4575384385469886e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 9/71 | LOSS: 2.392678361502476e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 10/71 | LOSS: 2.3666027118451893e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 11/71 | LOSS: 2.394638446882406e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 12/71 | LOSS: 2.3691380192758515e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 13/71 | LOSS: 2.324971579322924e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 14/71 | LOSS: 2.3373510581829276e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 15/71 | LOSS: 2.358521396672586e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 16/71 | LOSS: 2.368361781597795e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 17/71 | LOSS: 2.378235371401792e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 18/71 | LOSS: 2.386736263073736e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 19/71 | LOSS: 2.392010956100421e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 20/71 | LOSS: 2.3746271749370774e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 21/71 | LOSS: 2.3800158735618673e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 22/71 | LOSS: 2.3680803006636385e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 23/71 | LOSS: 2.382988160813208e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 24/71 | LOSS: 2.48196048050886e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 25/71 | LOSS: 2.4636714022982724e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 26/71 | LOSS: 2.4503895541851375e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 27/71 | LOSS: 2.437823559375829e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 28/71 | LOSS: 2.415604122406562e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 29/71 | LOSS: 2.399223042933348e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 30/71 | LOSS: 2.38468711036295e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 31/71 | LOSS: 2.3843822248181823e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 32/71 | LOSS: 2.3666353985335622e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 33/71 | LOSS: 2.3597182221505244e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 34/71 | LOSS: 2.3449356896370383e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 35/71 | LOSS: 2.3612725928817075e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 36/71 | LOSS: 2.3481756117591333e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 37/71 | LOSS: 2.3326807949113e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 38/71 | LOSS: 2.326879019584829e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 39/71 | LOSS: 2.3164520325735793e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 40/71 | LOSS: 2.352564865205062e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 41/71 | LOSS: 2.3593353034812026e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 42/71 | LOSS: 2.372270200582936e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 43/71 | LOSS: 2.3671736182057597e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 44/71 | LOSS: 2.3628458479328807e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 45/71 | LOSS: 2.3643122601598684e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 46/71 | LOSS: 2.351382434116726e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 47/71 | LOSS: 2.3492384608895616e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 48/71 | LOSS: 2.3442508846732826e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 49/71 | LOSS: 2.3494970992032905e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 50/71 | LOSS: 2.3597300812481062e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 51/71 | LOSS: 2.356557801109515e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 52/71 | LOSS: 2.3455083619238685e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 53/71 | LOSS: 2.346668191387801e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 54/71 | LOSS: 2.3530498550495724e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 55/71 | LOSS: 2.3432145261332543e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 56/71 | LOSS: 2.341278249922764e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 57/71 | LOSS: 2.3467427691402606e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 58/71 | LOSS: 2.351233860728394e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 59/71 | LOSS: 2.3496181893278844e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 60/71 | LOSS: 2.3510711813645177e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 61/71 | LOSS: 2.3495447485108726e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 62/71 | LOSS: 2.347862882454628e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 63/71 | LOSS: 2.3383491026152115e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 64/71 | LOSS: 2.3396914772232638e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 65/71 | LOSS: 2.3433416434713624e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 66/71 | LOSS: 2.3479184192047567e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 67/71 | LOSS: 2.344363800897126e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 68/71 | LOSS: 2.3524319618920107e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 69/71 | LOSS: 2.3470718328358737e-05\n",
      "TRAIN: EPOCH 54/100 | BATCH 70/71 | LOSS: 2.3337928723776713e-05\n",
      "VAL: EPOCH 54/100 | BATCH 0/8 | LOSS: 2.5738318072399125e-05\n",
      "VAL: EPOCH 54/100 | BATCH 1/8 | LOSS: 2.1473122615134344e-05\n",
      "VAL: EPOCH 54/100 | BATCH 2/8 | LOSS: 2.285656288828856e-05\n",
      "VAL: EPOCH 54/100 | BATCH 3/8 | LOSS: 2.4303695226990385e-05\n",
      "VAL: EPOCH 54/100 | BATCH 4/8 | LOSS: 2.4031718930928035e-05\n",
      "VAL: EPOCH 54/100 | BATCH 5/8 | LOSS: 2.3120688031970833e-05\n",
      "VAL: EPOCH 54/100 | BATCH 6/8 | LOSS: 2.3805481240352882e-05\n",
      "VAL: EPOCH 54/100 | BATCH 7/8 | LOSS: 2.3728732003291952e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 0/71 | LOSS: 2.1477673726622015e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 1/71 | LOSS: 2.192671126977075e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 2/71 | LOSS: 2.149119851916718e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 3/71 | LOSS: 2.1396923330030404e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 4/71 | LOSS: 2.149135361833032e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 5/71 | LOSS: 2.339067790065504e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 6/71 | LOSS: 2.346587565885524e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 7/71 | LOSS: 2.3047412241794518e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 8/71 | LOSS: 2.256645873583491e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 9/71 | LOSS: 2.2823089238954708e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 10/71 | LOSS: 2.2820267821027134e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 11/71 | LOSS: 2.2296320215294447e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 12/71 | LOSS: 2.1966585056641354e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 13/71 | LOSS: 2.1465348774134846e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 14/71 | LOSS: 2.1305820822211294e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 15/71 | LOSS: 2.1624008809340012e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 16/71 | LOSS: 2.17511805043186e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 17/71 | LOSS: 2.2044177765362998e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 18/71 | LOSS: 2.252465090254861e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 19/71 | LOSS: 2.2794561482442076e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 20/71 | LOSS: 2.2676758817397058e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 21/71 | LOSS: 2.2525043856479567e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 22/71 | LOSS: 2.2295931320632402e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 23/71 | LOSS: 2.2080322575372218e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 24/71 | LOSS: 2.1955358315608466e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 25/71 | LOSS: 2.2078808237543293e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 26/71 | LOSS: 2.2188874262846418e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 27/71 | LOSS: 2.2583387038009407e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 28/71 | LOSS: 2.281782445638312e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 29/71 | LOSS: 2.3144532315200193e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 30/71 | LOSS: 2.337060551883863e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 31/71 | LOSS: 2.3293522247058718e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 32/71 | LOSS: 2.314587530263728e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 33/71 | LOSS: 2.3253731663263514e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 34/71 | LOSS: 2.3278277304988088e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 35/71 | LOSS: 2.3197190178810462e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 36/71 | LOSS: 2.3191678534400324e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 37/71 | LOSS: 2.320370128329255e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 38/71 | LOSS: 2.3309257019905206e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 39/71 | LOSS: 2.342723050787754e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 40/71 | LOSS: 2.3399976271321066e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 41/71 | LOSS: 2.338952059030167e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 42/71 | LOSS: 2.339225986445875e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 43/71 | LOSS: 2.345874000556333e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 44/71 | LOSS: 2.3557212908700522e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 45/71 | LOSS: 2.348756551446454e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 46/71 | LOSS: 2.335500272586595e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 47/71 | LOSS: 2.3602138639944314e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 48/71 | LOSS: 2.3549349156828427e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 49/71 | LOSS: 2.3572881873406003e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 50/71 | LOSS: 2.3542318041447788e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 51/71 | LOSS: 2.354305969185169e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 52/71 | LOSS: 2.35305728463847e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 53/71 | LOSS: 2.3595198029027045e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 54/71 | LOSS: 2.355880503521555e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 55/71 | LOSS: 2.363101108520433e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 56/71 | LOSS: 2.351793550029438e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 57/71 | LOSS: 2.3682402758728632e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 58/71 | LOSS: 2.3550905529715485e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 59/71 | LOSS: 2.3536512132219892e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 60/71 | LOSS: 2.353310291717386e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 61/71 | LOSS: 2.3527567082642384e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 62/71 | LOSS: 2.35471887006295e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 63/71 | LOSS: 2.345486481658554e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 64/71 | LOSS: 2.345320703730417e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 65/71 | LOSS: 2.336836273157546e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 66/71 | LOSS: 2.3312391225663847e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 67/71 | LOSS: 2.324083494814284e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 68/71 | LOSS: 2.3264773214274708e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 69/71 | LOSS: 2.315851049518512e-05\n",
      "TRAIN: EPOCH 55/100 | BATCH 70/71 | LOSS: 2.306055041476035e-05\n",
      "VAL: EPOCH 55/100 | BATCH 0/8 | LOSS: 2.2884338250150904e-05\n",
      "VAL: EPOCH 55/100 | BATCH 1/8 | LOSS: 1.912957122840453e-05\n",
      "VAL: EPOCH 55/100 | BATCH 2/8 | LOSS: 2.1397453868606437e-05\n",
      "VAL: EPOCH 55/100 | BATCH 3/8 | LOSS: 2.2772060219722334e-05\n",
      "VAL: EPOCH 55/100 | BATCH 4/8 | LOSS: 2.2358914429787545e-05\n",
      "VAL: EPOCH 55/100 | BATCH 5/8 | LOSS: 2.164648503821809e-05\n",
      "VAL: EPOCH 55/100 | BATCH 6/8 | LOSS: 2.2362833728298676e-05\n",
      "VAL: EPOCH 55/100 | BATCH 7/8 | LOSS: 2.235353736068646e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 0/71 | LOSS: 2.0105630028410815e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 1/71 | LOSS: 2.2038811948732473e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 2/71 | LOSS: 2.1672793081961572e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 3/71 | LOSS: 2.2717745196132455e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 4/71 | LOSS: 2.2133431775728242e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 5/71 | LOSS: 2.08911651498056e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 6/71 | LOSS: 2.2526043851809974e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 7/71 | LOSS: 2.2946802118894993e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 8/71 | LOSS: 2.293607576575596e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 9/71 | LOSS: 2.2960414753470104e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 10/71 | LOSS: 2.2441730338455685e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 11/71 | LOSS: 2.2639908062653074e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 12/71 | LOSS: 2.2293186143192892e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 13/71 | LOSS: 2.2298441893196597e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 14/71 | LOSS: 2.226347908920919e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 15/71 | LOSS: 2.1961488982924493e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 16/71 | LOSS: 2.2271638633409404e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 17/71 | LOSS: 2.2599132838270936e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 18/71 | LOSS: 2.2516086621755293e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 19/71 | LOSS: 2.2460640138888266e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 20/71 | LOSS: 2.252406764975084e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 21/71 | LOSS: 2.2507114243291486e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 22/71 | LOSS: 2.2903504229717605e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 23/71 | LOSS: 2.2659827967193753e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 24/71 | LOSS: 2.2910548286745323e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 25/71 | LOSS: 2.2879309444061408e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 26/71 | LOSS: 2.292183516719551e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 27/71 | LOSS: 2.287726614927773e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 28/71 | LOSS: 2.3017807224873808e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 29/71 | LOSS: 2.3281962239707354e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 30/71 | LOSS: 2.3100735732793596e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 31/71 | LOSS: 2.3210933761674823e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 32/71 | LOSS: 2.3224125415612377e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 33/71 | LOSS: 2.3315452809209307e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 34/71 | LOSS: 2.3261730945835424e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 35/71 | LOSS: 2.316642318570909e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 36/71 | LOSS: 2.3263897919851178e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 37/71 | LOSS: 2.3118330270670795e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 38/71 | LOSS: 2.3038857948566332e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 39/71 | LOSS: 2.3094357766240135e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 40/71 | LOSS: 2.3163610532675392e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 41/71 | LOSS: 2.298537548908609e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 42/71 | LOSS: 2.307773371116085e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 43/71 | LOSS: 2.301120362476054e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 44/71 | LOSS: 2.3000018134350992e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 45/71 | LOSS: 2.2981619701528437e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 46/71 | LOSS: 2.28503284891671e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 47/71 | LOSS: 2.2839445174819655e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 48/71 | LOSS: 2.2915301830222714e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 49/71 | LOSS: 2.2970705285842995e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 50/71 | LOSS: 2.288991124135232e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 51/71 | LOSS: 2.2724603380853434e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 52/71 | LOSS: 2.2655153616313947e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 53/71 | LOSS: 2.2635346963566177e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 54/71 | LOSS: 2.265438306494616e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 55/71 | LOSS: 2.2665187251342494e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 56/71 | LOSS: 2.2542562650765797e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 57/71 | LOSS: 2.2405820251653244e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 58/71 | LOSS: 2.2428379051527402e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 59/71 | LOSS: 2.236462608683117e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 60/71 | LOSS: 2.2331159329280394e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 61/71 | LOSS: 2.2358236697806617e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 62/71 | LOSS: 2.2530511665132104e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 63/71 | LOSS: 2.250101942991023e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 64/71 | LOSS: 2.2397588998921074e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 65/71 | LOSS: 2.2382040642145455e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 66/71 | LOSS: 2.2404806707106502e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 67/71 | LOSS: 2.2360652752361013e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 68/71 | LOSS: 2.230775978692902e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 69/71 | LOSS: 2.2415826060750986e-05\n",
      "TRAIN: EPOCH 56/100 | BATCH 70/71 | LOSS: 2.2322143015860003e-05\n",
      "VAL: EPOCH 56/100 | BATCH 0/8 | LOSS: 2.2841453755972907e-05\n",
      "VAL: EPOCH 56/100 | BATCH 1/8 | LOSS: 1.9597766367951408e-05\n",
      "VAL: EPOCH 56/100 | BATCH 2/8 | LOSS: 2.047000877306952e-05\n",
      "VAL: EPOCH 56/100 | BATCH 3/8 | LOSS: 2.1771579667984042e-05\n",
      "VAL: EPOCH 56/100 | BATCH 4/8 | LOSS: 2.162182609026786e-05\n",
      "VAL: EPOCH 56/100 | BATCH 5/8 | LOSS: 2.0847861681734987e-05\n",
      "VAL: EPOCH 56/100 | BATCH 6/8 | LOSS: 2.1405337715155577e-05\n",
      "VAL: EPOCH 56/100 | BATCH 7/8 | LOSS: 2.1131601670276723e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 0/71 | LOSS: 1.6477875760756433e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 1/71 | LOSS: 1.9190005332347937e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 2/71 | LOSS: 2.1620541398685116e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 3/71 | LOSS: 2.2012922272551805e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 4/71 | LOSS: 2.0488668451434932e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 5/71 | LOSS: 1.934750404567846e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 6/71 | LOSS: 1.9257997077407447e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 7/71 | LOSS: 1.904314990497369e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 8/71 | LOSS: 1.9495521377797963e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 9/71 | LOSS: 1.934967986017e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 10/71 | LOSS: 1.940021237797654e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 11/71 | LOSS: 1.94005944346524e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 12/71 | LOSS: 1.9106619639877372e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 13/71 | LOSS: 2.0276578652556054e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 14/71 | LOSS: 2.046140677218015e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 15/71 | LOSS: 2.01343621029082e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 16/71 | LOSS: 2.0238525929230757e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 17/71 | LOSS: 2.0411645992781916e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 18/71 | LOSS: 2.0419819089798502e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 19/71 | LOSS: 2.0565849899867318e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 20/71 | LOSS: 2.0471636508730062e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 21/71 | LOSS: 2.0499900859828234e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 22/71 | LOSS: 2.0406237561711233e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 23/71 | LOSS: 2.0212629957920097e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 24/71 | LOSS: 2.02231781440787e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 25/71 | LOSS: 2.0238066729059658e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 26/71 | LOSS: 2.0415352318431594e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 27/71 | LOSS: 2.0528617564455738e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 28/71 | LOSS: 2.084583458530813e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 29/71 | LOSS: 2.0992809246915083e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 30/71 | LOSS: 2.1577756049209123e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 31/71 | LOSS: 2.1544204628298758e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 32/71 | LOSS: 2.1604502928146925e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 33/71 | LOSS: 2.159427009139429e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 34/71 | LOSS: 2.1582824514812922e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 35/71 | LOSS: 2.1546738935285248e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 36/71 | LOSS: 2.1646804885381858e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 37/71 | LOSS: 2.1634505556951472e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 38/71 | LOSS: 2.152132965887013e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 39/71 | LOSS: 2.1371802085923262e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 40/71 | LOSS: 2.1379678156979313e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 41/71 | LOSS: 2.138879976666344e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 42/71 | LOSS: 2.1440397642098022e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 43/71 | LOSS: 2.1473880637627602e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 44/71 | LOSS: 2.1596510446720965e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 45/71 | LOSS: 2.165850321983706e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 46/71 | LOSS: 2.178917740356484e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 47/71 | LOSS: 2.1648720178291114e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 48/71 | LOSS: 2.1677997786039724e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 49/71 | LOSS: 2.165002655601711e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 50/71 | LOSS: 2.1581963754251062e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 51/71 | LOSS: 2.1442050200885904e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 52/71 | LOSS: 2.1424724781234477e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 53/71 | LOSS: 2.138052559530479e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 54/71 | LOSS: 2.128109913015082e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 55/71 | LOSS: 2.138073466539936e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 56/71 | LOSS: 2.1390843645157904e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 57/71 | LOSS: 2.1426610287719464e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 58/71 | LOSS: 2.1437907673964095e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 59/71 | LOSS: 2.1394408592338246e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 60/71 | LOSS: 2.1489150163151713e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 61/71 | LOSS: 2.137189531206405e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 62/71 | LOSS: 2.1281867009834276e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 63/71 | LOSS: 2.12742126421972e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 64/71 | LOSS: 2.122938913089456e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 65/71 | LOSS: 2.134826143612207e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 66/71 | LOSS: 2.132849545996035e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 67/71 | LOSS: 2.1323898382059485e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 68/71 | LOSS: 2.1307490488402106e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 69/71 | LOSS: 2.126809664072685e-05\n",
      "TRAIN: EPOCH 57/100 | BATCH 70/71 | LOSS: 2.117034424552169e-05\n",
      "VAL: EPOCH 57/100 | BATCH 0/8 | LOSS: 2.391306588833686e-05\n",
      "VAL: EPOCH 57/100 | BATCH 1/8 | LOSS: 2.1016526261519175e-05\n",
      "VAL: EPOCH 57/100 | BATCH 2/8 | LOSS: 2.2037277328005683e-05\n",
      "VAL: EPOCH 57/100 | BATCH 3/8 | LOSS: 2.2357899069902487e-05\n",
      "VAL: EPOCH 57/100 | BATCH 4/8 | LOSS: 2.1753381952294147e-05\n",
      "VAL: EPOCH 57/100 | BATCH 5/8 | LOSS: 2.1098671822983306e-05\n",
      "VAL: EPOCH 57/100 | BATCH 6/8 | LOSS: 2.12878889474918e-05\n",
      "VAL: EPOCH 57/100 | BATCH 7/8 | LOSS: 2.1082189959997777e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 0/71 | LOSS: 2.549748751334846e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 1/71 | LOSS: 2.1836813175468706e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 2/71 | LOSS: 2.013522680499591e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 3/71 | LOSS: 1.9982172489108052e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 4/71 | LOSS: 2.0093554485356436e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 5/71 | LOSS: 1.973367943719495e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 6/71 | LOSS: 2.0518997968922902e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 7/71 | LOSS: 2.009760669352545e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 8/71 | LOSS: 2.0398348472857226e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 9/71 | LOSS: 2.0355952983663883e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 10/71 | LOSS: 2.075250317416662e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 11/71 | LOSS: 2.0609160856110975e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 12/71 | LOSS: 2.068314727064437e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 13/71 | LOSS: 2.0501812936605086e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 14/71 | LOSS: 2.033266597815479e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 15/71 | LOSS: 2.0145229655099683e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 16/71 | LOSS: 1.990288557649097e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 17/71 | LOSS: 1.991635478285995e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 18/71 | LOSS: 2.029018928863623e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 19/71 | LOSS: 2.035933221122832e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 20/71 | LOSS: 2.0542424646832625e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 21/71 | LOSS: 2.0516199749016035e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 22/71 | LOSS: 2.0375893259567775e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 23/71 | LOSS: 2.038897734261506e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 24/71 | LOSS: 2.029817333095707e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 25/71 | LOSS: 2.0344513709110637e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 26/71 | LOSS: 2.0401355127601334e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 27/71 | LOSS: 2.0254226650909652e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 28/71 | LOSS: 2.0132011960758345e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 29/71 | LOSS: 2.010211416442568e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 30/71 | LOSS: 2.0235633729175936e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 31/71 | LOSS: 2.018217185195681e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 32/71 | LOSS: 2.027156270659444e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 33/71 | LOSS: 2.0238354623228696e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 34/71 | LOSS: 2.020186761261097e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 35/71 | LOSS: 2.0197047180974754e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 36/71 | LOSS: 2.0271400284695414e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 37/71 | LOSS: 2.0165992138668355e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 38/71 | LOSS: 2.0190956722497223e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 39/71 | LOSS: 2.0135791373832035e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 40/71 | LOSS: 2.0163008496349837e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 41/71 | LOSS: 2.0266454157535918e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 42/71 | LOSS: 2.013716189245569e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 43/71 | LOSS: 2.010826117591271e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 44/71 | LOSS: 2.0224775067213665e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 45/71 | LOSS: 2.0145956291937342e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 46/71 | LOSS: 2.0073496284083452e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 47/71 | LOSS: 2.003769397636764e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 48/71 | LOSS: 2.001727315640595e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 49/71 | LOSS: 1.9996975552203367e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 50/71 | LOSS: 2.017297463359736e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 51/71 | LOSS: 2.0354967038936313e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 52/71 | LOSS: 2.0344631216090112e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 53/71 | LOSS: 2.0317065971337594e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 54/71 | LOSS: 2.0356317641430492e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 55/71 | LOSS: 2.029960454105354e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 56/71 | LOSS: 2.038402267077656e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 57/71 | LOSS: 2.0587133255095727e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 58/71 | LOSS: 2.051174464737661e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 59/71 | LOSS: 2.0483994391421825e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 60/71 | LOSS: 2.048929895192755e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 61/71 | LOSS: 2.0435326478623358e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 62/71 | LOSS: 2.042849713948921e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 63/71 | LOSS: 2.0443135028358483e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 64/71 | LOSS: 2.043193606088887e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 65/71 | LOSS: 2.032762664715516e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 66/71 | LOSS: 2.0305973534891902e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 67/71 | LOSS: 2.0424678910678587e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 68/71 | LOSS: 2.0459277276012987e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 69/71 | LOSS: 2.0447573777866117e-05\n",
      "TRAIN: EPOCH 58/100 | BATCH 70/71 | LOSS: 2.037657184342273e-05\n",
      "VAL: EPOCH 58/100 | BATCH 0/8 | LOSS: 2.2394679035642184e-05\n",
      "VAL: EPOCH 58/100 | BATCH 1/8 | LOSS: 1.8871211977966595e-05\n",
      "VAL: EPOCH 58/100 | BATCH 2/8 | LOSS: 1.986850960141358e-05\n",
      "VAL: EPOCH 58/100 | BATCH 3/8 | LOSS: 2.077440285574994e-05\n",
      "VAL: EPOCH 58/100 | BATCH 4/8 | LOSS: 2.0583588411682286e-05\n",
      "VAL: EPOCH 58/100 | BATCH 5/8 | LOSS: 1.983249573337768e-05\n",
      "VAL: EPOCH 58/100 | BATCH 6/8 | LOSS: 2.0251323803677224e-05\n",
      "VAL: EPOCH 58/100 | BATCH 7/8 | LOSS: 2.0145793769188458e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 0/71 | LOSS: 1.427335064363433e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 1/71 | LOSS: 1.5455119410034968e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 2/71 | LOSS: 1.5896175985593192e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 3/71 | LOSS: 1.704073406472162e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 4/71 | LOSS: 1.8159165665565525e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 5/71 | LOSS: 1.8702202851272887e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 6/71 | LOSS: 1.817870127394729e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 7/71 | LOSS: 1.8180381630372722e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 8/71 | LOSS: 1.8255270737831273e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 9/71 | LOSS: 1.813679100450827e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 10/71 | LOSS: 1.782483261856462e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 11/71 | LOSS: 1.778075132582065e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 12/71 | LOSS: 1.812319337309768e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 13/71 | LOSS: 1.8336953871767037e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 14/71 | LOSS: 1.8190463500407834e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 15/71 | LOSS: 1.8498656572774053e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 16/71 | LOSS: 1.8575907768750124e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 17/71 | LOSS: 1.8753810258608104e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 18/71 | LOSS: 1.889798008242475e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 19/71 | LOSS: 1.880454519778141e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 20/71 | LOSS: 1.877065109770878e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 21/71 | LOSS: 1.886240385250527e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 22/71 | LOSS: 1.881938695315393e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 23/71 | LOSS: 1.8777003560899175e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 24/71 | LOSS: 1.8774748750729485e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 25/71 | LOSS: 1.898907640469798e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 26/71 | LOSS: 1.8942242469940403e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 27/71 | LOSS: 1.8934426147357692e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 28/71 | LOSS: 1.901360720412099e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 29/71 | LOSS: 1.9082328799413516e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 30/71 | LOSS: 1.9213790848528247e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 31/71 | LOSS: 1.9315242013817624e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 32/71 | LOSS: 1.9296139285098903e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 33/71 | LOSS: 1.93685637521626e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 34/71 | LOSS: 1.9333212756984203e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 35/71 | LOSS: 1.9742239298163138e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 36/71 | LOSS: 1.9768837552652946e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 37/71 | LOSS: 1.9803844416425197e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 38/71 | LOSS: 1.9776573166242226e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 39/71 | LOSS: 1.974386591427901e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 40/71 | LOSS: 1.974147740040236e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 41/71 | LOSS: 1.981364312086953e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 42/71 | LOSS: 1.9734889868887813e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 43/71 | LOSS: 1.977666695231826e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 44/71 | LOSS: 1.9689533413232615e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 45/71 | LOSS: 1.9605078705353662e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 46/71 | LOSS: 1.9685862695196386e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 47/71 | LOSS: 1.98279732330775e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 48/71 | LOSS: 1.9688433820526687e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 49/71 | LOSS: 1.9875942671205847e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 50/71 | LOSS: 1.9909399520210902e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 51/71 | LOSS: 1.9953673857414218e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 52/71 | LOSS: 1.992085983044301e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 53/71 | LOSS: 1.9913335833094875e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 54/71 | LOSS: 2.0061344350159e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 55/71 | LOSS: 2.0122218757023802e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 56/71 | LOSS: 2.0051255680236702e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 57/71 | LOSS: 2.0138253728810954e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 58/71 | LOSS: 2.0095042924305928e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 59/71 | LOSS: 2.0074733887061786e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 60/71 | LOSS: 2.0070408548936478e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 61/71 | LOSS: 2.008842552787692e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 62/71 | LOSS: 2.0056197634345615e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 63/71 | LOSS: 2.0010555971339272e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 64/71 | LOSS: 2.000409790175931e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 65/71 | LOSS: 2.0086242853501585e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 66/71 | LOSS: 2.011961384881427e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 67/71 | LOSS: 2.0125125475340826e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 68/71 | LOSS: 2.009657873713877e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 69/71 | LOSS: 2.0080748773969908e-05\n",
      "TRAIN: EPOCH 59/100 | BATCH 70/71 | LOSS: 2.0133791468639097e-05\n",
      "VAL: EPOCH 59/100 | BATCH 0/8 | LOSS: 2.6210516807623208e-05\n",
      "VAL: EPOCH 59/100 | BATCH 1/8 | LOSS: 2.2778340280638076e-05\n",
      "VAL: EPOCH 59/100 | BATCH 2/8 | LOSS: 2.253739027461658e-05\n",
      "VAL: EPOCH 59/100 | BATCH 3/8 | LOSS: 2.296542470503482e-05\n",
      "VAL: EPOCH 59/100 | BATCH 4/8 | LOSS: 2.2632292893831617e-05\n",
      "VAL: EPOCH 59/100 | BATCH 5/8 | LOSS: 2.196556154861658e-05\n",
      "VAL: EPOCH 59/100 | BATCH 6/8 | LOSS: 2.2066750066837164e-05\n",
      "VAL: EPOCH 59/100 | BATCH 7/8 | LOSS: 2.1917974436291843e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 0/71 | LOSS: 2.1767034922959283e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 1/71 | LOSS: 1.9654164134408347e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 2/71 | LOSS: 1.8452259003728006e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 3/71 | LOSS: 1.9857008737744763e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 4/71 | LOSS: 2.0822979422518983e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 5/71 | LOSS: 2.0886148983360425e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 6/71 | LOSS: 2.1495798396894577e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 7/71 | LOSS: 2.2160859316500137e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 8/71 | LOSS: 2.2147384798800987e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 9/71 | LOSS: 2.1528229990508407e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 10/71 | LOSS: 2.1679761630366556e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 11/71 | LOSS: 2.206104939735572e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 12/71 | LOSS: 2.206658735723557e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 13/71 | LOSS: 2.201404341966346e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 14/71 | LOSS: 2.1703546735807322e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 15/71 | LOSS: 2.1768757164863928e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 16/71 | LOSS: 2.167159672193092e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 17/71 | LOSS: 2.142063779804933e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 18/71 | LOSS: 2.1113785758069226e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 19/71 | LOSS: 2.0974728795408736e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 20/71 | LOSS: 2.111407645445849e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 21/71 | LOSS: 2.139980675896037e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 22/71 | LOSS: 2.1405266718078487e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 23/71 | LOSS: 2.1378521523729432e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 24/71 | LOSS: 2.127292304066941e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 25/71 | LOSS: 2.1321383218701856e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 26/71 | LOSS: 2.1147529305717735e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 27/71 | LOSS: 2.119209589831631e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 28/71 | LOSS: 2.110471817512258e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 29/71 | LOSS: 2.100771574381118e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 30/71 | LOSS: 2.11742554975283e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 31/71 | LOSS: 2.1013342234255106e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 32/71 | LOSS: 2.0895344994531833e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 33/71 | LOSS: 2.0794668029445937e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 34/71 | LOSS: 2.0618597825107697e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 35/71 | LOSS: 2.0744933509478062e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 36/71 | LOSS: 2.0784137066928797e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 37/71 | LOSS: 2.0777163291840178e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 38/71 | LOSS: 2.0874448707148145e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 39/71 | LOSS: 2.0923787315041408e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 40/71 | LOSS: 2.0934235069988773e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 41/71 | LOSS: 2.0944657080690376e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 42/71 | LOSS: 2.090679676215121e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 43/71 | LOSS: 2.0912198000256797e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 44/71 | LOSS: 2.0976644939057424e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 45/71 | LOSS: 2.0923455849697348e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 46/71 | LOSS: 2.0899247055182214e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 47/71 | LOSS: 2.0860408123250334e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 48/71 | LOSS: 2.0809480627910806e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 49/71 | LOSS: 2.0718310479423964e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 50/71 | LOSS: 2.0648298196451208e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 51/71 | LOSS: 2.0664746968507712e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 52/71 | LOSS: 2.058290532281631e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 53/71 | LOSS: 2.0602716075599245e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 54/71 | LOSS: 2.0621412940355102e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 55/71 | LOSS: 2.053899678295628e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 56/71 | LOSS: 2.0547088385220585e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 57/71 | LOSS: 2.0512673504056473e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 58/71 | LOSS: 2.05108216027453e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 59/71 | LOSS: 2.0598039160783325e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 60/71 | LOSS: 2.049399793830788e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 61/71 | LOSS: 2.0376363805083992e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 62/71 | LOSS: 2.036209598517171e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 63/71 | LOSS: 2.0406780635084942e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 64/71 | LOSS: 2.0331068299128674e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 65/71 | LOSS: 2.0314986668932818e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 66/71 | LOSS: 2.0304613650269085e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 67/71 | LOSS: 2.0293715565725614e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 68/71 | LOSS: 2.0264931030713182e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 69/71 | LOSS: 2.0246145816469965e-05\n",
      "TRAIN: EPOCH 60/100 | BATCH 70/71 | LOSS: 2.0205049584316366e-05\n",
      "VAL: EPOCH 60/100 | BATCH 0/8 | LOSS: 2.2671893020742573e-05\n",
      "VAL: EPOCH 60/100 | BATCH 1/8 | LOSS: 1.9105787032458466e-05\n",
      "VAL: EPOCH 60/100 | BATCH 2/8 | LOSS: 1.9531225916580297e-05\n",
      "VAL: EPOCH 60/100 | BATCH 3/8 | LOSS: 2.008788851526333e-05\n",
      "VAL: EPOCH 60/100 | BATCH 4/8 | LOSS: 1.9772635278059168e-05\n",
      "VAL: EPOCH 60/100 | BATCH 5/8 | LOSS: 1.9100119364641916e-05\n",
      "VAL: EPOCH 60/100 | BATCH 6/8 | LOSS: 1.944240544357204e-05\n",
      "VAL: EPOCH 60/100 | BATCH 7/8 | LOSS: 1.9281789946035133e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 0/71 | LOSS: 1.580084608576726e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 1/71 | LOSS: 1.7718811250233557e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 2/71 | LOSS: 1.9065074108463403e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 3/71 | LOSS: 1.8983212157763774e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 4/71 | LOSS: 2.0098386085010133e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 5/71 | LOSS: 1.9640440162523493e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 6/71 | LOSS: 2.1747861475367764e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 7/71 | LOSS: 2.150281056856329e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 8/71 | LOSS: 2.132885189591131e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 9/71 | LOSS: 2.143595429515699e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 10/71 | LOSS: 2.0746938802899834e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 11/71 | LOSS: 2.032418531901688e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 12/71 | LOSS: 1.9891054972294325e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 13/71 | LOSS: 1.9736426436013842e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 14/71 | LOSS: 1.9780156981141773e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 15/71 | LOSS: 1.9443231337845646e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 16/71 | LOSS: 1.9269523363618646e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 17/71 | LOSS: 1.9467454724993633e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 18/71 | LOSS: 1.942123579187625e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 19/71 | LOSS: 1.9261845363871543e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 20/71 | LOSS: 1.9091112465373173e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 21/71 | LOSS: 1.920257549996181e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 22/71 | LOSS: 1.920520816898004e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 23/71 | LOSS: 1.9270776457839627e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 24/71 | LOSS: 1.905951812659623e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 25/71 | LOSS: 1.9009869307550478e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 26/71 | LOSS: 1.9004473701729524e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 27/71 | LOSS: 1.902897088257305e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 28/71 | LOSS: 1.8991873573746677e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 29/71 | LOSS: 1.9010969632896984e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 30/71 | LOSS: 1.8795996472744585e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 31/71 | LOSS: 1.868736902110868e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 32/71 | LOSS: 1.883589569236695e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 33/71 | LOSS: 1.8718356707669207e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 34/71 | LOSS: 1.862744853237278e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 35/71 | LOSS: 1.8488440193525297e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 36/71 | LOSS: 1.845581049860197e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 37/71 | LOSS: 1.8443869353895454e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 38/71 | LOSS: 1.8511622026077628e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 39/71 | LOSS: 1.854153422300442e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 40/71 | LOSS: 1.8558723041212766e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 41/71 | LOSS: 1.8683809360352044e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 42/71 | LOSS: 1.8748776055026398e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 43/71 | LOSS: 1.872644069904213e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 44/71 | LOSS: 1.8762833840608236e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 45/71 | LOSS: 1.8787990069066165e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 46/71 | LOSS: 1.891313991592974e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 47/71 | LOSS: 1.8859432335223875e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 48/71 | LOSS: 1.8810311878635786e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 49/71 | LOSS: 1.8850067444873275e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 50/71 | LOSS: 1.8815071117013064e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 51/71 | LOSS: 1.8795542560977854e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 52/71 | LOSS: 1.8819720952586e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 53/71 | LOSS: 1.8861593886227906e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 54/71 | LOSS: 1.8910945089539218e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 55/71 | LOSS: 1.8913421545870994e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 56/71 | LOSS: 1.888293916221173e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 57/71 | LOSS: 1.8993045363584253e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 58/71 | LOSS: 1.9134802996477908e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 59/71 | LOSS: 1.9182617825208582e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 60/71 | LOSS: 1.9240880781163453e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 61/71 | LOSS: 1.9219664761382568e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 62/71 | LOSS: 1.9163775173111057e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 63/71 | LOSS: 1.9135946772053103e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 64/71 | LOSS: 1.9164459347218955e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 65/71 | LOSS: 1.913259597351884e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 66/71 | LOSS: 1.9025414279786306e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 67/71 | LOSS: 1.8982898662965026e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 68/71 | LOSS: 1.902059824899787e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 69/71 | LOSS: 1.9034035007540037e-05\n",
      "TRAIN: EPOCH 61/100 | BATCH 70/71 | LOSS: 1.9008488879349694e-05\n",
      "VAL: EPOCH 61/100 | BATCH 0/8 | LOSS: 2.0128565665800124e-05\n",
      "VAL: EPOCH 61/100 | BATCH 1/8 | LOSS: 1.6980395230348222e-05\n",
      "VAL: EPOCH 61/100 | BATCH 2/8 | LOSS: 1.8164286302635446e-05\n",
      "VAL: EPOCH 61/100 | BATCH 3/8 | LOSS: 1.8735812318482203e-05\n",
      "VAL: EPOCH 61/100 | BATCH 4/8 | LOSS: 1.8451360301696695e-05\n",
      "VAL: EPOCH 61/100 | BATCH 5/8 | LOSS: 1.7928414308698848e-05\n",
      "VAL: EPOCH 61/100 | BATCH 6/8 | LOSS: 1.82038067058394e-05\n",
      "VAL: EPOCH 61/100 | BATCH 7/8 | LOSS: 1.8025937151833205e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 0/71 | LOSS: 2.3175205569714308e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 1/71 | LOSS: 2.0950765247107483e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 2/71 | LOSS: 1.9313520776146714e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 3/71 | LOSS: 2.105682460751268e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 4/71 | LOSS: 2.00783117179526e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 5/71 | LOSS: 1.9461118730153732e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 6/71 | LOSS: 1.8868915243988988e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 7/71 | LOSS: 1.9335698198119644e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 8/71 | LOSS: 1.9040110095779204e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 9/71 | LOSS: 1.9233895545767155e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 10/71 | LOSS: 1.886297848779412e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 11/71 | LOSS: 1.9241257480947144e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 12/71 | LOSS: 1.8852937500923872e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 13/71 | LOSS: 1.883456596780369e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 14/71 | LOSS: 1.8709866465845457e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 15/71 | LOSS: 1.926236279814475e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 16/71 | LOSS: 1.9158418581355363e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 17/71 | LOSS: 1.9029415044416157e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 18/71 | LOSS: 1.9066386142328968e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 19/71 | LOSS: 1.904899127112003e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 20/71 | LOSS: 1.87348251052927e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 21/71 | LOSS: 1.8875924690126506e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 22/71 | LOSS: 1.8820659098982223e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 23/71 | LOSS: 1.883136652243896e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 24/71 | LOSS: 1.8676302388485056e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 25/71 | LOSS: 1.8606071290168285e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 26/71 | LOSS: 1.869563537318889e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 27/71 | LOSS: 1.8681464065853754e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 28/71 | LOSS: 1.8611299279216147e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 29/71 | LOSS: 1.8542820695680953e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 30/71 | LOSS: 1.845174615629085e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 31/71 | LOSS: 1.8593561577517903e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 32/71 | LOSS: 1.8746023063916557e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 33/71 | LOSS: 1.8910032930066602e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 34/71 | LOSS: 1.8868268177487022e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 35/71 | LOSS: 1.8897463961467212e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 36/71 | LOSS: 1.8838031792256515e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 37/71 | LOSS: 1.870676463735627e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 38/71 | LOSS: 1.8643189949402884e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 39/71 | LOSS: 1.8629802139003004e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 40/71 | LOSS: 1.8542603622957292e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 41/71 | LOSS: 1.853473921593312e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 42/71 | LOSS: 1.843528964453198e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 43/71 | LOSS: 1.852229568364485e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 44/71 | LOSS: 1.8625914183050758e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 45/71 | LOSS: 1.8602287780041756e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 46/71 | LOSS: 1.8612461896369264e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 47/71 | LOSS: 1.8599769437817788e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 48/71 | LOSS: 1.859121081851095e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 49/71 | LOSS: 1.8526312505855457e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 50/71 | LOSS: 1.8455323947373292e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 51/71 | LOSS: 1.8429057578172294e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 52/71 | LOSS: 1.847539336284961e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 53/71 | LOSS: 1.8594040419109796e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 54/71 | LOSS: 1.8577964659595058e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 55/71 | LOSS: 1.853242372362729e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 56/71 | LOSS: 1.8425780549398296e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 57/71 | LOSS: 1.8392303618070558e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 58/71 | LOSS: 1.8400748512388003e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 59/71 | LOSS: 1.834763640241969e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 60/71 | LOSS: 1.837975015986871e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 61/71 | LOSS: 1.8381728119904084e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 62/71 | LOSS: 1.8340319435256313e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 63/71 | LOSS: 1.829370856398782e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 64/71 | LOSS: 1.8268577444429795e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 65/71 | LOSS: 1.8176990193530923e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 66/71 | LOSS: 1.8180795409738385e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 67/71 | LOSS: 1.8332443219418672e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 68/71 | LOSS: 1.8451301068547554e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 69/71 | LOSS: 1.852225438727016e-05\n",
      "TRAIN: EPOCH 62/100 | BATCH 70/71 | LOSS: 1.844974674024573e-05\n",
      "VAL: EPOCH 62/100 | BATCH 0/8 | LOSS: 2.1184321667533368e-05\n",
      "VAL: EPOCH 62/100 | BATCH 1/8 | LOSS: 1.8178479876951315e-05\n",
      "VAL: EPOCH 62/100 | BATCH 2/8 | LOSS: 1.8645418094820343e-05\n",
      "VAL: EPOCH 62/100 | BATCH 3/8 | LOSS: 1.914900349220261e-05\n",
      "VAL: EPOCH 62/100 | BATCH 4/8 | LOSS: 1.880102281575091e-05\n",
      "VAL: EPOCH 62/100 | BATCH 5/8 | LOSS: 1.8315232409804594e-05\n",
      "VAL: EPOCH 62/100 | BATCH 6/8 | LOSS: 1.8495060430723242e-05\n",
      "VAL: EPOCH 62/100 | BATCH 7/8 | LOSS: 1.8245573301101103e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 0/71 | LOSS: 1.6695328667992726e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 1/71 | LOSS: 1.552880166855175e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 2/71 | LOSS: 1.6719316893916886e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 3/71 | LOSS: 1.7805642983148573e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 4/71 | LOSS: 1.7706148355500773e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 5/71 | LOSS: 1.9344793448302273e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 6/71 | LOSS: 1.9790972146438435e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 7/71 | LOSS: 2.0298087747505633e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 8/71 | LOSS: 2.009286233967739e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 9/71 | LOSS: 2.0236771706549917e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 10/71 | LOSS: 2.0443093903436832e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 11/71 | LOSS: 2.0290806181340788e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 12/71 | LOSS: 2.0077580792041353e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 13/71 | LOSS: 1.9986335246358067e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 14/71 | LOSS: 2.017341491106587e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 15/71 | LOSS: 1.996439698359609e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 16/71 | LOSS: 1.9811712584615796e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 17/71 | LOSS: 1.9849991986120585e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 18/71 | LOSS: 2.02062741376913e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 19/71 | LOSS: 2.007856319323764e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 20/71 | LOSS: 2.0148267053411385e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 21/71 | LOSS: 2.019442085425412e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 22/71 | LOSS: 2.0275600431689426e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 23/71 | LOSS: 2.0277220452650607e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 24/71 | LOSS: 2.0324866782175376e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 25/71 | LOSS: 2.0291401317361026e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 26/71 | LOSS: 2.0173930655194756e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 27/71 | LOSS: 2.038478526498823e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 28/71 | LOSS: 2.0318668750154884e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 29/71 | LOSS: 2.0258823800152943e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 30/71 | LOSS: 2.0284363047047067e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 31/71 | LOSS: 2.0171333062535268e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 32/71 | LOSS: 2.0196082032546684e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 33/71 | LOSS: 2.0103669050834415e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 34/71 | LOSS: 1.9943207943080258e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 35/71 | LOSS: 1.9984967315799117e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 36/71 | LOSS: 1.9763966618582442e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 37/71 | LOSS: 1.9710185441860966e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 38/71 | LOSS: 1.9688667168501968e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 39/71 | LOSS: 1.9708139257090805e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 40/71 | LOSS: 1.9656669570483757e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 41/71 | LOSS: 1.9584258704223128e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 42/71 | LOSS: 1.9586746020650025e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 43/71 | LOSS: 1.9518837461873773e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 44/71 | LOSS: 1.9560007401903728e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 45/71 | LOSS: 1.9566001336225664e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 46/71 | LOSS: 1.9497912615677588e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 47/71 | LOSS: 1.9523235190869553e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 48/71 | LOSS: 1.9470789789475205e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 49/71 | LOSS: 1.942673030498554e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 50/71 | LOSS: 1.9408676373988035e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 51/71 | LOSS: 1.9320137828169838e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 52/71 | LOSS: 1.930561326312358e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 53/71 | LOSS: 1.9283886070264055e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 54/71 | LOSS: 1.923145775270479e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 55/71 | LOSS: 1.918219306519729e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 56/71 | LOSS: 1.9283035534930316e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 57/71 | LOSS: 1.927786430816672e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 58/71 | LOSS: 1.9169124767605524e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 59/71 | LOSS: 1.916836564911743e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 60/71 | LOSS: 1.9258228274368583e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 61/71 | LOSS: 1.9187381088070857e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 62/71 | LOSS: 1.9158914761242664e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 63/71 | LOSS: 1.913815069087832e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 64/71 | LOSS: 1.908810106844892e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 65/71 | LOSS: 1.9090087307356015e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 66/71 | LOSS: 1.9052083501522777e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 67/71 | LOSS: 1.9006001420005786e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 68/71 | LOSS: 1.9034476518636044e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 69/71 | LOSS: 1.9009970492334106e-05\n",
      "TRAIN: EPOCH 63/100 | BATCH 70/71 | LOSS: 1.8924941440685948e-05\n",
      "VAL: EPOCH 63/100 | BATCH 0/8 | LOSS: 1.9531562429619953e-05\n",
      "VAL: EPOCH 63/100 | BATCH 1/8 | LOSS: 1.581447577336803e-05\n",
      "VAL: EPOCH 63/100 | BATCH 2/8 | LOSS: 1.6826446881168522e-05\n",
      "VAL: EPOCH 63/100 | BATCH 3/8 | LOSS: 1.7833852780313464e-05\n",
      "VAL: EPOCH 63/100 | BATCH 4/8 | LOSS: 1.772476716723759e-05\n",
      "VAL: EPOCH 63/100 | BATCH 5/8 | LOSS: 1.7030483528894063e-05\n",
      "VAL: EPOCH 63/100 | BATCH 6/8 | LOSS: 1.7489085751419354e-05\n",
      "VAL: EPOCH 63/100 | BATCH 7/8 | LOSS: 1.721111675578868e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 0/71 | LOSS: 1.864605837909039e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 1/71 | LOSS: 2.0433183635759633e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 2/71 | LOSS: 1.982144628224584e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 3/71 | LOSS: 1.8509949313738616e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 4/71 | LOSS: 1.8593226923258043e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 5/71 | LOSS: 1.819903930784979e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 6/71 | LOSS: 1.8503114915802144e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 7/71 | LOSS: 1.906805891849217e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 8/71 | LOSS: 1.8696039559371355e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 9/71 | LOSS: 1.8938675202662127e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 10/71 | LOSS: 1.8769938006616112e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 11/71 | LOSS: 1.8586152388403814e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 12/71 | LOSS: 1.8393616511065586e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 13/71 | LOSS: 1.8004900864200734e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 14/71 | LOSS: 1.8249560647139636e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 15/71 | LOSS: 1.8197597341895744e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 16/71 | LOSS: 1.8079418784027975e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 17/71 | LOSS: 1.8350922800891567e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 18/71 | LOSS: 1.8290022509111287e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 19/71 | LOSS: 1.8315181296202355e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 20/71 | LOSS: 1.8480704533257742e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 21/71 | LOSS: 1.8734833660842426e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 22/71 | LOSS: 1.852962020126617e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 23/71 | LOSS: 1.8405518024640816e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 24/71 | LOSS: 1.842008212406654e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 25/71 | LOSS: 1.8344996393492554e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 26/71 | LOSS: 1.832732009966599e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 27/71 | LOSS: 1.8166812554097434e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 28/71 | LOSS: 1.819419964610053e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 29/71 | LOSS: 1.804671470987766e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 30/71 | LOSS: 1.7874181227647745e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 31/71 | LOSS: 1.780995776812233e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 32/71 | LOSS: 1.77805146229344e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 33/71 | LOSS: 1.7630654570786966e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 34/71 | LOSS: 1.7621516703262126e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 35/71 | LOSS: 1.760975202260953e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 36/71 | LOSS: 1.7744903497923343e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 37/71 | LOSS: 1.7635094417001757e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 38/71 | LOSS: 1.763524386870603e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 39/71 | LOSS: 1.7548621076457494e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 40/71 | LOSS: 1.7607567936399237e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 41/71 | LOSS: 1.759081154280769e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 42/71 | LOSS: 1.7566974028616028e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 43/71 | LOSS: 1.746042531439426e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 44/71 | LOSS: 1.7414340679048716e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 45/71 | LOSS: 1.7490921713349803e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 46/71 | LOSS: 1.7624308210097878e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 47/71 | LOSS: 1.7700285354749212e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 48/71 | LOSS: 1.772011097384574e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 49/71 | LOSS: 1.768033898770227e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 50/71 | LOSS: 1.7639816677746793e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 51/71 | LOSS: 1.7593315253459597e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 52/71 | LOSS: 1.7651487312077563e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 53/71 | LOSS: 1.776274078750888e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 54/71 | LOSS: 1.7821682757295837e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 55/71 | LOSS: 1.7848116515649182e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 56/71 | LOSS: 1.7803740966452385e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 57/71 | LOSS: 1.7731549319407727e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 58/71 | LOSS: 1.776899366863506e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 59/71 | LOSS: 1.7714763331847885e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 60/71 | LOSS: 1.7738411054883503e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 61/71 | LOSS: 1.7795276870592423e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 62/71 | LOSS: 1.7791596191172073e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 63/71 | LOSS: 1.7834435624308753e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 64/71 | LOSS: 1.781552304097344e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 65/71 | LOSS: 1.7872675540054456e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 66/71 | LOSS: 1.7875860582684417e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 67/71 | LOSS: 1.7862478216557065e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 68/71 | LOSS: 1.7874856881014463e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 69/71 | LOSS: 1.790469596016919e-05\n",
      "TRAIN: EPOCH 64/100 | BATCH 70/71 | LOSS: 1.7841014621962143e-05\n",
      "VAL: EPOCH 64/100 | BATCH 0/8 | LOSS: 1.972491190826986e-05\n",
      "VAL: EPOCH 64/100 | BATCH 1/8 | LOSS: 1.5854516277613584e-05\n",
      "VAL: EPOCH 64/100 | BATCH 2/8 | LOSS: 1.6861299930800062e-05\n",
      "VAL: EPOCH 64/100 | BATCH 3/8 | LOSS: 1.827133883125498e-05\n",
      "VAL: EPOCH 64/100 | BATCH 4/8 | LOSS: 1.8388626267551443e-05\n",
      "VAL: EPOCH 64/100 | BATCH 5/8 | LOSS: 1.769358990107624e-05\n",
      "VAL: EPOCH 64/100 | BATCH 6/8 | LOSS: 1.8145763143755695e-05\n",
      "VAL: EPOCH 64/100 | BATCH 7/8 | LOSS: 1.7782666418497683e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 0/71 | LOSS: 1.8557522707851604e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 1/71 | LOSS: 1.864213299995754e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 2/71 | LOSS: 1.6845766367623582e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 3/71 | LOSS: 1.8660179193830118e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 4/71 | LOSS: 1.874895897344686e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 5/71 | LOSS: 1.852534023782937e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 6/71 | LOSS: 1.9636321667348966e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 7/71 | LOSS: 1.871586778179335e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 8/71 | LOSS: 1.866955042512725e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 9/71 | LOSS: 1.904930468299426e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 10/71 | LOSS: 1.8841103321227077e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 11/71 | LOSS: 1.898485652418458e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 12/71 | LOSS: 1.889585431714435e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 13/71 | LOSS: 1.8594432406514117e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 14/71 | LOSS: 1.8693162140455873e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 15/71 | LOSS: 1.8493432833111e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 16/71 | LOSS: 1.8212185001603383e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 17/71 | LOSS: 1.821373714645031e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 18/71 | LOSS: 1.819535856683567e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 19/71 | LOSS: 1.80875807927805e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 20/71 | LOSS: 1.7955468138097785e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 21/71 | LOSS: 1.7870759785926733e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 22/71 | LOSS: 1.7841178582191628e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 23/71 | LOSS: 1.785610447768704e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 24/71 | LOSS: 1.806709813536145e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 25/71 | LOSS: 1.7888330638435527e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 26/71 | LOSS: 1.8069346912886465e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 27/71 | LOSS: 1.7935231491199894e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 28/71 | LOSS: 1.7975980442535166e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 29/71 | LOSS: 1.7918855034319372e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 30/71 | LOSS: 1.7944775355581734e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 31/71 | LOSS: 1.801117014110787e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 32/71 | LOSS: 1.801427209681027e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 33/71 | LOSS: 1.7945614351500646e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 34/71 | LOSS: 1.78611719872736e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 35/71 | LOSS: 1.785614879029001e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 36/71 | LOSS: 1.784455083464776e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 37/71 | LOSS: 1.784779459213544e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 38/71 | LOSS: 1.787109902533046e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 39/71 | LOSS: 1.782330587047909e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 40/71 | LOSS: 1.7809314912727397e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 41/71 | LOSS: 1.7840416581921525e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 42/71 | LOSS: 1.774221180862379e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 43/71 | LOSS: 1.78214977908779e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 44/71 | LOSS: 1.7789039743042345e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 45/71 | LOSS: 1.7816510317852195e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 46/71 | LOSS: 1.785878370394633e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 47/71 | LOSS: 1.796001280733132e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 48/71 | LOSS: 1.789722662170807e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 49/71 | LOSS: 1.7900893290061504e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 50/71 | LOSS: 1.7891343304654583e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 51/71 | LOSS: 1.7887706655108978e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 52/71 | LOSS: 1.7850423181750198e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 53/71 | LOSS: 1.778391749582995e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 54/71 | LOSS: 1.786201441559983e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 55/71 | LOSS: 1.784581140132754e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 56/71 | LOSS: 1.7811215672592977e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 57/71 | LOSS: 1.7835404393955633e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 58/71 | LOSS: 1.7804970627628914e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 59/71 | LOSS: 1.7686094740080686e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 60/71 | LOSS: 1.7686348104182456e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 61/71 | LOSS: 1.7646274999048815e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 62/71 | LOSS: 1.770680766697744e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 63/71 | LOSS: 1.7725822402780977e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 64/71 | LOSS: 1.7699902169764615e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 65/71 | LOSS: 1.7711092699424956e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 66/71 | LOSS: 1.7620007804622736e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 67/71 | LOSS: 1.7546794919433203e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 68/71 | LOSS: 1.74463609347594e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 69/71 | LOSS: 1.747870094602279e-05\n",
      "TRAIN: EPOCH 65/100 | BATCH 70/71 | LOSS: 1.737394354100795e-05\n",
      "VAL: EPOCH 65/100 | BATCH 0/8 | LOSS: 1.9002269254997373e-05\n",
      "VAL: EPOCH 65/100 | BATCH 1/8 | LOSS: 1.545260784041602e-05\n",
      "VAL: EPOCH 65/100 | BATCH 2/8 | LOSS: 1.605060485114033e-05\n",
      "VAL: EPOCH 65/100 | BATCH 3/8 | LOSS: 1.7006743291858584e-05\n",
      "VAL: EPOCH 65/100 | BATCH 4/8 | LOSS: 1.6899370530154555e-05\n",
      "VAL: EPOCH 65/100 | BATCH 5/8 | LOSS: 1.6270423657260835e-05\n",
      "VAL: EPOCH 65/100 | BATCH 6/8 | LOSS: 1.6633445868917208e-05\n",
      "VAL: EPOCH 65/100 | BATCH 7/8 | LOSS: 1.6351864815078443e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 0/71 | LOSS: 1.3975784895592369e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 1/71 | LOSS: 1.3626874078909168e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 2/71 | LOSS: 1.4358883163367864e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 3/71 | LOSS: 1.4912011238266132e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 4/71 | LOSS: 1.5737786998215598e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 5/71 | LOSS: 1.5740318455452023e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 6/71 | LOSS: 1.602723062075841e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 7/71 | LOSS: 1.5649895317437768e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 8/71 | LOSS: 1.5925314376848593e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 9/71 | LOSS: 1.5977434395608724e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 10/71 | LOSS: 1.6309022198997397e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 11/71 | LOSS: 1.6298425255930244e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 12/71 | LOSS: 1.656063816951176e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 13/71 | LOSS: 1.6543843555284964e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 14/71 | LOSS: 1.6610895363555757e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 15/71 | LOSS: 1.6648399707719364e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 16/71 | LOSS: 1.6643781985775978e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 17/71 | LOSS: 1.6391600234379035e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 18/71 | LOSS: 1.6469532008494226e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 19/71 | LOSS: 1.653639378673688e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 20/71 | LOSS: 1.669345619163886e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 21/71 | LOSS: 1.6548270526651653e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 22/71 | LOSS: 1.657710009053319e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 23/71 | LOSS: 1.641823765415514e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 24/71 | LOSS: 1.660805497522233e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 25/71 | LOSS: 1.6537832484923107e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 26/71 | LOSS: 1.6587192004846185e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 27/71 | LOSS: 1.660604263114302e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 28/71 | LOSS: 1.6648208656988572e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 29/71 | LOSS: 1.6838861574797194e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 30/71 | LOSS: 1.6761265332236015e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 31/71 | LOSS: 1.695532813528189e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 32/71 | LOSS: 1.6968298124001276e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 33/71 | LOSS: 1.6964350780814557e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 34/71 | LOSS: 1.697462160206799e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 35/71 | LOSS: 1.711304028301836e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 36/71 | LOSS: 1.7151251577652637e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 37/71 | LOSS: 1.7051142860605307e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 38/71 | LOSS: 1.7203683175746566e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 39/71 | LOSS: 1.705415097603691e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 40/71 | LOSS: 1.7107438907260075e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 41/71 | LOSS: 1.7283925358025866e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 42/71 | LOSS: 1.7367334209645146e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 43/71 | LOSS: 1.7315237962314065e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 44/71 | LOSS: 1.73096833047263e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 45/71 | LOSS: 1.7210012055843634e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 46/71 | LOSS: 1.720135950311855e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 47/71 | LOSS: 1.7158568293022352e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 48/71 | LOSS: 1.7174037324997587e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 49/71 | LOSS: 1.7252629022550535e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 50/71 | LOSS: 1.7316061138105807e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 51/71 | LOSS: 1.729416710706853e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 52/71 | LOSS: 1.7366800798385464e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 53/71 | LOSS: 1.73906090889499e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 54/71 | LOSS: 1.741504656820325e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 55/71 | LOSS: 1.7373880268678477e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 56/71 | LOSS: 1.7432656964126836e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 57/71 | LOSS: 1.746763191163311e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 58/71 | LOSS: 1.747747004844576e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 59/71 | LOSS: 1.7449149784927915e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 60/71 | LOSS: 1.7392699937474508e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 61/71 | LOSS: 1.7434478606094585e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 62/71 | LOSS: 1.7485893583816794e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 63/71 | LOSS: 1.748034419790656e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 64/71 | LOSS: 1.7483964018958012e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 65/71 | LOSS: 1.7431352895955293e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 66/71 | LOSS: 1.7524984436185246e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 67/71 | LOSS: 1.751898048496514e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 68/71 | LOSS: 1.7518251798469134e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 69/71 | LOSS: 1.7520573640337846e-05\n",
      "TRAIN: EPOCH 66/100 | BATCH 70/71 | LOSS: 1.740214307753543e-05\n",
      "VAL: EPOCH 66/100 | BATCH 0/8 | LOSS: 1.777804754965473e-05\n",
      "VAL: EPOCH 66/100 | BATCH 1/8 | LOSS: 1.4852015283395303e-05\n",
      "VAL: EPOCH 66/100 | BATCH 2/8 | LOSS: 1.5971600987541024e-05\n",
      "VAL: EPOCH 66/100 | BATCH 3/8 | LOSS: 1.6953648582784808e-05\n",
      "VAL: EPOCH 66/100 | BATCH 4/8 | LOSS: 1.6791379130154384e-05\n",
      "VAL: EPOCH 66/100 | BATCH 5/8 | LOSS: 1.6269607537348445e-05\n",
      "VAL: EPOCH 66/100 | BATCH 6/8 | LOSS: 1.665171034151821e-05\n",
      "VAL: EPOCH 66/100 | BATCH 7/8 | LOSS: 1.6262258782262506e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 0/71 | LOSS: 1.2170369700470474e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 1/71 | LOSS: 1.5858031929383287e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 2/71 | LOSS: 1.7141495542697765e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 3/71 | LOSS: 1.6266763623207225e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 4/71 | LOSS: 1.7198806563101244e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 5/71 | LOSS: 1.7331763198550714e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 6/71 | LOSS: 1.7545324616159113e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 7/71 | LOSS: 1.7087715605157427e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 8/71 | LOSS: 1.7329167248034435e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 9/71 | LOSS: 1.729041450744262e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 10/71 | LOSS: 1.7125081946935758e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 11/71 | LOSS: 1.706112789179315e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 12/71 | LOSS: 1.7212182613618028e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 13/71 | LOSS: 1.6939850476254442e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 14/71 | LOSS: 1.71142859471729e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 15/71 | LOSS: 1.692725538759987e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 16/71 | LOSS: 1.7001612457748063e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 17/71 | LOSS: 1.6938455701165367e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 18/71 | LOSS: 1.695173858407591e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 19/71 | LOSS: 1.6979474094114266e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 20/71 | LOSS: 1.7059234302169423e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 21/71 | LOSS: 1.694676591961814e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 22/71 | LOSS: 1.695805819864567e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 23/71 | LOSS: 1.6979214933598996e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 24/71 | LOSS: 1.703765316051431e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 25/71 | LOSS: 1.698247405539195e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 26/71 | LOSS: 1.6881342095008154e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 27/71 | LOSS: 1.688038219072041e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 28/71 | LOSS: 1.7140083055610063e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 29/71 | LOSS: 1.7127309062440568e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 30/71 | LOSS: 1.7049668097188848e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 31/71 | LOSS: 1.720015114869966e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 32/71 | LOSS: 1.709209545603671e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 33/71 | LOSS: 1.7118282170593055e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 34/71 | LOSS: 1.709507762695596e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 35/71 | LOSS: 1.7046695474063097e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 36/71 | LOSS: 1.702369047199829e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 37/71 | LOSS: 1.7043645991514878e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 38/71 | LOSS: 1.7033992779220287e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 39/71 | LOSS: 1.6940311684265908e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 40/71 | LOSS: 1.7033340399637647e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 41/71 | LOSS: 1.6982095638966366e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 42/71 | LOSS: 1.697889143373644e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 43/71 | LOSS: 1.7025595546080943e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 44/71 | LOSS: 1.697746855724189e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 45/71 | LOSS: 1.6995195833062652e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 46/71 | LOSS: 1.7042541506445096e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 47/71 | LOSS: 1.700189811041734e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 48/71 | LOSS: 1.7023236811225187e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 49/71 | LOSS: 1.720786724035861e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 50/71 | LOSS: 1.710726193029105e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 51/71 | LOSS: 1.702716115057858e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 52/71 | LOSS: 1.6973312874983576e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 53/71 | LOSS: 1.6995017075613658e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 54/71 | LOSS: 1.7082858738087285e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 55/71 | LOSS: 1.705445697552932e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 56/71 | LOSS: 1.7144159197382497e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 57/71 | LOSS: 1.7117428138464736e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 58/71 | LOSS: 1.7117115560922992e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 59/71 | LOSS: 1.7054612771971734e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 60/71 | LOSS: 1.709437550684403e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 61/71 | LOSS: 1.7055579754073293e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 62/71 | LOSS: 1.7079791942951342e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 63/71 | LOSS: 1.7072767917625242e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 64/71 | LOSS: 1.702597658620037e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 65/71 | LOSS: 1.6991841110928252e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 66/71 | LOSS: 1.7009406645283618e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 67/71 | LOSS: 1.7022783381167648e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 68/71 | LOSS: 1.705876876139487e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 69/71 | LOSS: 1.7018772450683172e-05\n",
      "TRAIN: EPOCH 67/100 | BATCH 70/71 | LOSS: 1.7065163287760453e-05\n",
      "VAL: EPOCH 67/100 | BATCH 0/8 | LOSS: 2.309335832251236e-05\n",
      "VAL: EPOCH 67/100 | BATCH 1/8 | LOSS: 1.9364779291208833e-05\n",
      "VAL: EPOCH 67/100 | BATCH 2/8 | LOSS: 1.9072169986126635e-05\n",
      "VAL: EPOCH 67/100 | BATCH 3/8 | LOSS: 1.9682520814967575e-05\n",
      "VAL: EPOCH 67/100 | BATCH 4/8 | LOSS: 1.953809078258928e-05\n",
      "VAL: EPOCH 67/100 | BATCH 5/8 | LOSS: 1.882998033882662e-05\n",
      "VAL: EPOCH 67/100 | BATCH 6/8 | LOSS: 1.8982589088929152e-05\n",
      "VAL: EPOCH 67/100 | BATCH 7/8 | LOSS: 1.8704235344557674e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 0/71 | LOSS: 1.577954571985174e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 1/71 | LOSS: 1.6743296328058932e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 2/71 | LOSS: 1.607620712699524e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 3/71 | LOSS: 1.5952867670421256e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 4/71 | LOSS: 1.6641587353660726e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 5/71 | LOSS: 1.6347986578087632e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 6/71 | LOSS: 1.600782785057423e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 7/71 | LOSS: 1.6199752735701622e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 8/71 | LOSS: 1.6553838375127976e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 9/71 | LOSS: 1.6550613509025425e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 10/71 | LOSS: 1.6602776502788235e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 11/71 | LOSS: 1.683780358992711e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 12/71 | LOSS: 1.653228733070696e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 13/71 | LOSS: 1.6586975107202306e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 14/71 | LOSS: 1.6617656122737874e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 15/71 | LOSS: 1.6437179851891415e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 16/71 | LOSS: 1.6475311520694315e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 17/71 | LOSS: 1.6274630626058853e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 18/71 | LOSS: 1.646147848077817e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 19/71 | LOSS: 1.647169488023792e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 20/71 | LOSS: 1.644307333966329e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 21/71 | LOSS: 1.6488320025845578e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 22/71 | LOSS: 1.6343017547193956e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 23/71 | LOSS: 1.6326736158589483e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 24/71 | LOSS: 1.6296409085043707e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 25/71 | LOSS: 1.6261907218834564e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 26/71 | LOSS: 1.643980498624e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 27/71 | LOSS: 1.6458896295392022e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 28/71 | LOSS: 1.6463128460976616e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 29/71 | LOSS: 1.6370067472356218e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 30/71 | LOSS: 1.633970585375679e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 31/71 | LOSS: 1.6388297694902576e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 32/71 | LOSS: 1.6253815539874434e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 33/71 | LOSS: 1.6204648599180955e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 34/71 | LOSS: 1.626694801026523e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 35/71 | LOSS: 1.622609619921099e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 36/71 | LOSS: 1.6233291692490615e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 37/71 | LOSS: 1.6272368348405796e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 38/71 | LOSS: 1.6329359846750798e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 39/71 | LOSS: 1.6324270927725592e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 40/71 | LOSS: 1.624717825928578e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 41/71 | LOSS: 1.6365252598743175e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 42/71 | LOSS: 1.6391078408856247e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 43/71 | LOSS: 1.636408801046359e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 44/71 | LOSS: 1.6450738985440694e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 45/71 | LOSS: 1.6378958859960498e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 46/71 | LOSS: 1.6475672674324473e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 47/71 | LOSS: 1.650079508408453e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 48/71 | LOSS: 1.6534039081752596e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 49/71 | LOSS: 1.6526224699191515e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 50/71 | LOSS: 1.6443668933860345e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 51/71 | LOSS: 1.6409926607033973e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 52/71 | LOSS: 1.6422749261969244e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 53/71 | LOSS: 1.6421674512525055e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 54/71 | LOSS: 1.6490206855285745e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 55/71 | LOSS: 1.645918733369659e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 56/71 | LOSS: 1.644052223893691e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 57/71 | LOSS: 1.6412815057265107e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 58/71 | LOSS: 1.6402868314506092e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 59/71 | LOSS: 1.6439222872577374e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 60/71 | LOSS: 1.6520596903722092e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 61/71 | LOSS: 1.6594111002632417e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 62/71 | LOSS: 1.659099461181858e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 63/71 | LOSS: 1.6579032973140784e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 64/71 | LOSS: 1.6645828341447318e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 65/71 | LOSS: 1.6622128110942537e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 66/71 | LOSS: 1.6618434814002135e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 67/71 | LOSS: 1.6613403485119788e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 68/71 | LOSS: 1.660155787934512e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 69/71 | LOSS: 1.669756910424829e-05\n",
      "TRAIN: EPOCH 68/100 | BATCH 70/71 | LOSS: 1.658503912603663e-05\n",
      "VAL: EPOCH 68/100 | BATCH 0/8 | LOSS: 2.0639570720959455e-05\n",
      "VAL: EPOCH 68/100 | BATCH 1/8 | LOSS: 1.6760022845119238e-05\n",
      "VAL: EPOCH 68/100 | BATCH 2/8 | LOSS: 1.723476574018908e-05\n",
      "VAL: EPOCH 68/100 | BATCH 3/8 | LOSS: 1.762632518875762e-05\n",
      "VAL: EPOCH 68/100 | BATCH 4/8 | LOSS: 1.7290262985625304e-05\n",
      "VAL: EPOCH 68/100 | BATCH 5/8 | LOSS: 1.6534526366740465e-05\n",
      "VAL: EPOCH 68/100 | BATCH 6/8 | LOSS: 1.6843280328820193e-05\n",
      "VAL: EPOCH 68/100 | BATCH 7/8 | LOSS: 1.6586405877205834e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 0/71 | LOSS: 1.2699992112175096e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 1/71 | LOSS: 1.4463923889707075e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 2/71 | LOSS: 1.4323549294203985e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 3/71 | LOSS: 1.3728172007176909e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 4/71 | LOSS: 1.4959672080294695e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 5/71 | LOSS: 1.4932615007031321e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 6/71 | LOSS: 1.56463895889049e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 7/71 | LOSS: 1.6147346855177602e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 8/71 | LOSS: 1.6562979504265564e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 9/71 | LOSS: 1.612711730558658e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 10/71 | LOSS: 1.6322423768790692e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 11/71 | LOSS: 1.6292901970397605e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 12/71 | LOSS: 1.621471514786558e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 13/71 | LOSS: 1.6370578383170402e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 14/71 | LOSS: 1.678168688764951e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 15/71 | LOSS: 1.6852414205459354e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 16/71 | LOSS: 1.6717946998730494e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 17/71 | LOSS: 1.6719945102118396e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 18/71 | LOSS: 1.6766222583292372e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 19/71 | LOSS: 1.6728858145143023e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 20/71 | LOSS: 1.657026747298438e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 21/71 | LOSS: 1.6720041995457986e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 22/71 | LOSS: 1.6648862623222396e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 23/71 | LOSS: 1.6589590093947965e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 24/71 | LOSS: 1.661835896811681e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 25/71 | LOSS: 1.640736693913753e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 26/71 | LOSS: 1.636189824273094e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 27/71 | LOSS: 1.6326681134160026e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 28/71 | LOSS: 1.6177019209988368e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 29/71 | LOSS: 1.608861287725934e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 30/71 | LOSS: 1.6048110645046578e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 31/71 | LOSS: 1.607066826636583e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 32/71 | LOSS: 1.60378851818898e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 33/71 | LOSS: 1.595077519061092e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 34/71 | LOSS: 1.5875409917498474e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 35/71 | LOSS: 1.577106869869264e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 36/71 | LOSS: 1.5714600792577857e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 37/71 | LOSS: 1.5791731153800165e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 38/71 | LOSS: 1.5716706002422143e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 39/71 | LOSS: 1.570101760535181e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 40/71 | LOSS: 1.5711453027218274e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 41/71 | LOSS: 1.5765366199394477e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 42/71 | LOSS: 1.57986956807246e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 43/71 | LOSS: 1.5832332068880945e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 44/71 | LOSS: 1.580000569245183e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 45/71 | LOSS: 1.5787771328335943e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 46/71 | LOSS: 1.5792667503038947e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 47/71 | LOSS: 1.579328384574789e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 48/71 | LOSS: 1.582254776157966e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 49/71 | LOSS: 1.5853841268835822e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 50/71 | LOSS: 1.587311226995e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 51/71 | LOSS: 1.5946200252213974e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 52/71 | LOSS: 1.6077287257775305e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 53/71 | LOSS: 1.61111149989463e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 54/71 | LOSS: 1.6111328212983527e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 55/71 | LOSS: 1.6136575514532785e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 56/71 | LOSS: 1.6186601131171063e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 57/71 | LOSS: 1.6287596061939158e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 58/71 | LOSS: 1.627926673590683e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 59/71 | LOSS: 1.6315311601526142e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 60/71 | LOSS: 1.6312342968823764e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 61/71 | LOSS: 1.6350234971234328e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 62/71 | LOSS: 1.632848614549619e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 63/71 | LOSS: 1.6381715738589264e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 64/71 | LOSS: 1.6411980770779057e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 65/71 | LOSS: 1.6368938538631262e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 66/71 | LOSS: 1.633102584125495e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 67/71 | LOSS: 1.628485487719231e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 68/71 | LOSS: 1.6274925946018428e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 69/71 | LOSS: 1.6209430629844843e-05\n",
      "TRAIN: EPOCH 69/100 | BATCH 70/71 | LOSS: 1.6291183757703935e-05\n",
      "VAL: EPOCH 69/100 | BATCH 0/8 | LOSS: 1.9079310732195154e-05\n",
      "VAL: EPOCH 69/100 | BATCH 1/8 | LOSS: 1.6386325114581268e-05\n",
      "VAL: EPOCH 69/100 | BATCH 2/8 | LOSS: 1.7125244388201583e-05\n",
      "VAL: EPOCH 69/100 | BATCH 3/8 | LOSS: 1.7777741959434934e-05\n",
      "VAL: EPOCH 69/100 | BATCH 4/8 | LOSS: 1.7562000721227378e-05\n",
      "VAL: EPOCH 69/100 | BATCH 5/8 | LOSS: 1.6975343442027224e-05\n",
      "VAL: EPOCH 69/100 | BATCH 6/8 | LOSS: 1.717564854126457e-05\n",
      "VAL: EPOCH 69/100 | BATCH 7/8 | LOSS: 1.677707086855662e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 0/71 | LOSS: 1.638755202293396e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 1/71 | LOSS: 1.720872023724951e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 2/71 | LOSS: 1.7705579011817463e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 3/71 | LOSS: 1.7157148249680176e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 4/71 | LOSS: 1.6799907098175026e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 5/71 | LOSS: 1.692466806465139e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 6/71 | LOSS: 1.697311095735391e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 7/71 | LOSS: 1.693409353720199e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 8/71 | LOSS: 1.729714272692541e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 9/71 | LOSS: 1.6985869478958193e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 10/71 | LOSS: 1.6493400173865005e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 11/71 | LOSS: 1.6424069599452196e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 12/71 | LOSS: 1.633506382439429e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 13/71 | LOSS: 1.640810920175032e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 14/71 | LOSS: 1.6470735984815596e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 15/71 | LOSS: 1.651485229103855e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 16/71 | LOSS: 1.626609242949224e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 17/71 | LOSS: 1.633644074495856e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 18/71 | LOSS: 1.6366067173198406e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 19/71 | LOSS: 1.6467124169139424e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 20/71 | LOSS: 1.6460980380846497e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 21/71 | LOSS: 1.6356876668313898e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 22/71 | LOSS: 1.6237699642458566e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 23/71 | LOSS: 1.6098117688064424e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 24/71 | LOSS: 1.613673462998122e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 25/71 | LOSS: 1.6290406095392358e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 26/71 | LOSS: 1.6352844694564636e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 27/71 | LOSS: 1.6302298393254334e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 28/71 | LOSS: 1.6283595461322093e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 29/71 | LOSS: 1.6214777618491403e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 30/71 | LOSS: 1.6168315215186488e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 31/71 | LOSS: 1.6082956733498577e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 32/71 | LOSS: 1.6015554137993604e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 33/71 | LOSS: 1.60753964283846e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 34/71 | LOSS: 1.614073137586404e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 35/71 | LOSS: 1.6079446898705403e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 36/71 | LOSS: 1.5985036141599364e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 37/71 | LOSS: 1.608881841189271e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 38/71 | LOSS: 1.6019421496028557e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 39/71 | LOSS: 1.5955175308590695e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 40/71 | LOSS: 1.592906968786648e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 41/71 | LOSS: 1.5849971424315646e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 42/71 | LOSS: 1.5857154446779914e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 43/71 | LOSS: 1.5866620267181794e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 44/71 | LOSS: 1.5782055521008766e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 45/71 | LOSS: 1.5694984026397478e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 46/71 | LOSS: 1.571311034414511e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 47/71 | LOSS: 1.5734406398829986e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 48/71 | LOSS: 1.5676357086667107e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 49/71 | LOSS: 1.5654315338906598e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 50/71 | LOSS: 1.5659857411924795e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 51/71 | LOSS: 1.572335099808697e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 52/71 | LOSS: 1.573393354810146e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 53/71 | LOSS: 1.5736814711325912e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 54/71 | LOSS: 1.575604522010756e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 55/71 | LOSS: 1.5756272286385604e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 56/71 | LOSS: 1.569568998922733e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 57/71 | LOSS: 1.5738521330510185e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 58/71 | LOSS: 1.5781152534931978e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 59/71 | LOSS: 1.5757753286986067e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 60/71 | LOSS: 1.576354457700022e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 61/71 | LOSS: 1.5778950558254315e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 62/71 | LOSS: 1.5795924449025374e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 63/71 | LOSS: 1.5800502268348282e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 64/71 | LOSS: 1.58298883504288e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 65/71 | LOSS: 1.578784215808176e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 66/71 | LOSS: 1.5736869635705925e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 67/71 | LOSS: 1.572056810479861e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 68/71 | LOSS: 1.5759072928209417e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 69/71 | LOSS: 1.574260676144539e-05\n",
      "TRAIN: EPOCH 70/100 | BATCH 70/71 | LOSS: 1.5635857879984338e-05\n",
      "VAL: EPOCH 70/100 | BATCH 0/8 | LOSS: 1.9136054106638767e-05\n",
      "VAL: EPOCH 70/100 | BATCH 1/8 | LOSS: 1.5630834241164848e-05\n",
      "VAL: EPOCH 70/100 | BATCH 2/8 | LOSS: 1.616703229956329e-05\n",
      "VAL: EPOCH 70/100 | BATCH 3/8 | LOSS: 1.6645599316689186e-05\n",
      "VAL: EPOCH 70/100 | BATCH 4/8 | LOSS: 1.6294086526613684e-05\n",
      "VAL: EPOCH 70/100 | BATCH 5/8 | LOSS: 1.56573820883447e-05\n",
      "VAL: EPOCH 70/100 | BATCH 6/8 | LOSS: 1.59241881192429e-05\n",
      "VAL: EPOCH 70/100 | BATCH 7/8 | LOSS: 1.5665389355490333e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 0/71 | LOSS: 1.2081929526175372e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 1/71 | LOSS: 1.2936420262121828e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 2/71 | LOSS: 1.2822814824176021e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 3/71 | LOSS: 1.3747940101893619e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 4/71 | LOSS: 1.3265609049994964e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 5/71 | LOSS: 1.3313358673864665e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 6/71 | LOSS: 1.3546494561264158e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 7/71 | LOSS: 1.3434129982670129e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 8/71 | LOSS: 1.3927695161126192e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 9/71 | LOSS: 1.4397695031220792e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 10/71 | LOSS: 1.4318162109041374e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 11/71 | LOSS: 1.4808889848912562e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 12/71 | LOSS: 1.4617558848000885e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 13/71 | LOSS: 1.5041185114179306e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 14/71 | LOSS: 1.4978908014503152e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 15/71 | LOSS: 1.5153603783346625e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 16/71 | LOSS: 1.5391735627524802e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 17/71 | LOSS: 1.5298360014842023e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 18/71 | LOSS: 1.5119691086415274e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 19/71 | LOSS: 1.5136501997403683e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 20/71 | LOSS: 1.5187095197602285e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 21/71 | LOSS: 1.5194559430462753e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 22/71 | LOSS: 1.5183851721066395e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 23/71 | LOSS: 1.505553082855234e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 24/71 | LOSS: 1.4981446147430688e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 25/71 | LOSS: 1.4929391489623902e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 26/71 | LOSS: 1.5150657485786791e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 27/71 | LOSS: 1.5172343475049794e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 28/71 | LOSS: 1.5187778287860244e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 29/71 | LOSS: 1.5081524422081808e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 30/71 | LOSS: 1.5109516454920654e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 31/71 | LOSS: 1.505052475181401e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 32/71 | LOSS: 1.491358662778194e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 33/71 | LOSS: 1.4885296209018526e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 34/71 | LOSS: 1.494574735261267e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 35/71 | LOSS: 1.4939288626515514e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 36/71 | LOSS: 1.5085788962440691e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 37/71 | LOSS: 1.5102602446859237e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 38/71 | LOSS: 1.5006556331615848e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 39/71 | LOSS: 1.4931128907846869e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 40/71 | LOSS: 1.5066448013829182e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 41/71 | LOSS: 1.5036280837237081e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 42/71 | LOSS: 1.5024784430394457e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 43/71 | LOSS: 1.5063195178299793e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 44/71 | LOSS: 1.4950738902067922e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 45/71 | LOSS: 1.503486923273e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 46/71 | LOSS: 1.4994791815703873e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 47/71 | LOSS: 1.498646234191862e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 48/71 | LOSS: 1.4988929121630332e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 49/71 | LOSS: 1.5016718225524529e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 50/71 | LOSS: 1.5157853500996306e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 51/71 | LOSS: 1.5240131145989066e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 52/71 | LOSS: 1.5350395033409477e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 53/71 | LOSS: 1.5374485462450173e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 54/71 | LOSS: 1.5368116410999475e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 55/71 | LOSS: 1.5385456760798532e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 56/71 | LOSS: 1.537126258642131e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 57/71 | LOSS: 1.537037165878152e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 58/71 | LOSS: 1.5325293118667877e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 59/71 | LOSS: 1.539384722188212e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 60/71 | LOSS: 1.542449459878701e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 61/71 | LOSS: 1.547012155447925e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 62/71 | LOSS: 1.5471217935440122e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 63/71 | LOSS: 1.55114464774897e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 64/71 | LOSS: 1.5528547303872003e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 65/71 | LOSS: 1.5534685727206206e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 66/71 | LOSS: 1.5525065925851493e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 67/71 | LOSS: 1.5521262733219535e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 68/71 | LOSS: 1.554877591210559e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 69/71 | LOSS: 1.556227801172229e-05\n",
      "TRAIN: EPOCH 71/100 | BATCH 70/71 | LOSS: 1.553209745958464e-05\n",
      "VAL: EPOCH 71/100 | BATCH 0/8 | LOSS: 1.848954525485169e-05\n",
      "VAL: EPOCH 71/100 | BATCH 1/8 | LOSS: 1.5030283975647762e-05\n",
      "VAL: EPOCH 71/100 | BATCH 2/8 | LOSS: 1.5293586329789832e-05\n",
      "VAL: EPOCH 71/100 | BATCH 3/8 | LOSS: 1.5917808013909962e-05\n",
      "VAL: EPOCH 71/100 | BATCH 4/8 | LOSS: 1.5697562957939226e-05\n",
      "VAL: EPOCH 71/100 | BATCH 5/8 | LOSS: 1.5058871213113889e-05\n",
      "VAL: EPOCH 71/100 | BATCH 6/8 | LOSS: 1.532354040786491e-05\n",
      "VAL: EPOCH 71/100 | BATCH 7/8 | LOSS: 1.4972983876759827e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 0/71 | LOSS: 1.4025707059772685e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 1/71 | LOSS: 1.4330929843708873e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 2/71 | LOSS: 1.485358370700851e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 3/71 | LOSS: 1.4726916560903192e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 4/71 | LOSS: 1.4150618881103583e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 5/71 | LOSS: 1.3782664003277508e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 6/71 | LOSS: 1.4173386132045249e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 7/71 | LOSS: 1.4565699984814273e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 8/71 | LOSS: 1.4949277909989985e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 9/71 | LOSS: 1.470063007218414e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 10/71 | LOSS: 1.5033464809345208e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 11/71 | LOSS: 1.5295694159552415e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 12/71 | LOSS: 1.5186136037160535e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 13/71 | LOSS: 1.4897811654788842e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 14/71 | LOSS: 1.4822997036390006e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 15/71 | LOSS: 1.493319200562837e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 16/71 | LOSS: 1.492818452842424e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 17/71 | LOSS: 1.4816907019444949e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 18/71 | LOSS: 1.4864803808067287e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 19/71 | LOSS: 1.5114032976271119e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 20/71 | LOSS: 1.514875406255236e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 21/71 | LOSS: 1.5390712922618892e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 22/71 | LOSS: 1.536929286648165e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 23/71 | LOSS: 1.5361013879555685e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 24/71 | LOSS: 1.5587675625283738e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 25/71 | LOSS: 1.5428951988164605e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 26/71 | LOSS: 1.5311532849780094e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 27/71 | LOSS: 1.540236309795416e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 28/71 | LOSS: 1.5346760815661802e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 29/71 | LOSS: 1.542833200195067e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 30/71 | LOSS: 1.5545326981913193e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 31/71 | LOSS: 1.5699135161639788e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 32/71 | LOSS: 1.5914355682263814e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 33/71 | LOSS: 1.5978061636654756e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 34/71 | LOSS: 1.593603044187018e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 35/71 | LOSS: 1.6001841711638714e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 36/71 | LOSS: 1.6039572983572725e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 37/71 | LOSS: 1.5970309864075572e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 38/71 | LOSS: 1.594791135572357e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 39/71 | LOSS: 1.5995000399016136e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 40/71 | LOSS: 1.5938158845174036e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 41/71 | LOSS: 1.6004446739056482e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 42/71 | LOSS: 1.5960372450557093e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 43/71 | LOSS: 1.5819417926236795e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 44/71 | LOSS: 1.576694396337391e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 45/71 | LOSS: 1.573721008595464e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 46/71 | LOSS: 1.574950178148859e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 47/71 | LOSS: 1.572285130653957e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 48/71 | LOSS: 1.5698667511412855e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 49/71 | LOSS: 1.5623375020368258e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 50/71 | LOSS: 1.5645908045852967e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 51/71 | LOSS: 1.5707346025150262e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 52/71 | LOSS: 1.5823743129692718e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 53/71 | LOSS: 1.582145400620734e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 54/71 | LOSS: 1.5767411010032944e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 55/71 | LOSS: 1.5748190467742722e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 56/71 | LOSS: 1.573023923571638e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 57/71 | LOSS: 1.5716713764488994e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 58/71 | LOSS: 1.565414710304309e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 59/71 | LOSS: 1.559934768617192e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 60/71 | LOSS: 1.5673570681533195e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 61/71 | LOSS: 1.5696391528763343e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 62/71 | LOSS: 1.5692842261780735e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 63/71 | LOSS: 1.571130076172267e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 64/71 | LOSS: 1.571892059403651e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 65/71 | LOSS: 1.5694352786147537e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 66/71 | LOSS: 1.5718245519035835e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 67/71 | LOSS: 1.5753800158456535e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 68/71 | LOSS: 1.571479250465611e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 69/71 | LOSS: 1.5669516545001117e-05\n",
      "TRAIN: EPOCH 72/100 | BATCH 70/71 | LOSS: 1.5681871176844064e-05\n",
      "VAL: EPOCH 72/100 | BATCH 0/8 | LOSS: 1.8528040527598932e-05\n",
      "VAL: EPOCH 72/100 | BATCH 1/8 | LOSS: 1.4998970982560422e-05\n",
      "VAL: EPOCH 72/100 | BATCH 2/8 | LOSS: 1.5077360634071132e-05\n",
      "VAL: EPOCH 72/100 | BATCH 3/8 | LOSS: 1.5889640962996054e-05\n",
      "VAL: EPOCH 72/100 | BATCH 4/8 | LOSS: 1.5630163761670702e-05\n",
      "VAL: EPOCH 72/100 | BATCH 5/8 | LOSS: 1.496382431772266e-05\n",
      "VAL: EPOCH 72/100 | BATCH 6/8 | LOSS: 1.5226054659121604e-05\n",
      "VAL: EPOCH 72/100 | BATCH 7/8 | LOSS: 1.4780605056330387e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 0/71 | LOSS: 1.4408024071599357e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 1/71 | LOSS: 1.4291610568761826e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 2/71 | LOSS: 1.4379468060117992e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 3/71 | LOSS: 1.4278072740125936e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 4/71 | LOSS: 1.3382359247771091e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 5/71 | LOSS: 1.431646705896128e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 6/71 | LOSS: 1.5059779148681887e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 7/71 | LOSS: 1.5708006685599685e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 8/71 | LOSS: 1.573153207977561e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 9/71 | LOSS: 1.6181101455003954e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 10/71 | LOSS: 1.6096847254878163e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 11/71 | LOSS: 1.603164491825737e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 12/71 | LOSS: 1.5874580290423743e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 13/71 | LOSS: 1.6076182938767098e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 14/71 | LOSS: 1.6622080086866237e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 15/71 | LOSS: 1.6648786242967617e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 16/71 | LOSS: 1.6899395364709882e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 17/71 | LOSS: 1.6712303426579132e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 18/71 | LOSS: 1.6696913151367642e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 19/71 | LOSS: 1.6730292782085598e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 20/71 | LOSS: 1.6920677071972196e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 21/71 | LOSS: 1.669728164250211e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 22/71 | LOSS: 1.65378872660095e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 23/71 | LOSS: 1.6826163611464533e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 24/71 | LOSS: 1.6739633101678918e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 25/71 | LOSS: 1.6642414056360183e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 26/71 | LOSS: 1.6768660956316767e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 27/71 | LOSS: 1.66462991728622e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 28/71 | LOSS: 1.6534726047870335e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 29/71 | LOSS: 1.6494400703474336e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 30/71 | LOSS: 1.6478207315583036e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 31/71 | LOSS: 1.6433088063649848e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 32/71 | LOSS: 1.6416707694588695e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 33/71 | LOSS: 1.6653720168131474e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 34/71 | LOSS: 1.6496161515533457e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 35/71 | LOSS: 1.6388262666724688e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 36/71 | LOSS: 1.6365671768112776e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 37/71 | LOSS: 1.6255947062554217e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 38/71 | LOSS: 1.615196922499215e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 39/71 | LOSS: 1.611221284747444e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 40/71 | LOSS: 1.614777956114622e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 41/71 | LOSS: 1.6109860011749245e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 42/71 | LOSS: 1.6083301192719415e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 43/71 | LOSS: 1.5970090254830378e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 44/71 | LOSS: 1.589642656148903e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 45/71 | LOSS: 1.587411733176534e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 46/71 | LOSS: 1.5866949844224863e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 47/71 | LOSS: 1.5903834366781666e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 48/71 | LOSS: 1.5849630917542454e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 49/71 | LOSS: 1.574122616148088e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 50/71 | LOSS: 1.577384967936714e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 51/71 | LOSS: 1.5726965487444362e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 52/71 | LOSS: 1.5681875615796945e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 53/71 | LOSS: 1.5711824053815456e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 54/71 | LOSS: 1.573210196180629e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 55/71 | LOSS: 1.5705331065873906e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 56/71 | LOSS: 1.5583200477346525e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 57/71 | LOSS: 1.5570484312524186e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 58/71 | LOSS: 1.553671800292697e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 59/71 | LOSS: 1.549199914734345e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 60/71 | LOSS: 1.5509407424614154e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 61/71 | LOSS: 1.5498623431466834e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 62/71 | LOSS: 1.5496842428951317e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 63/71 | LOSS: 1.5496627582933797e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 64/71 | LOSS: 1.547094437298186e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 65/71 | LOSS: 1.5438429396784738e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 66/71 | LOSS: 1.540172698737275e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 67/71 | LOSS: 1.5337376653940953e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 68/71 | LOSS: 1.5311109656476468e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 69/71 | LOSS: 1.531084386182816e-05\n",
      "TRAIN: EPOCH 73/100 | BATCH 70/71 | LOSS: 1.5343654294937693e-05\n",
      "VAL: EPOCH 73/100 | BATCH 0/8 | LOSS: 1.65828678291291e-05\n",
      "VAL: EPOCH 73/100 | BATCH 1/8 | LOSS: 1.3648617823491804e-05\n",
      "VAL: EPOCH 73/100 | BATCH 2/8 | LOSS: 1.4507132921911156e-05\n",
      "VAL: EPOCH 73/100 | BATCH 3/8 | LOSS: 1.5296169749490218e-05\n",
      "VAL: EPOCH 73/100 | BATCH 4/8 | LOSS: 1.5056521078804508e-05\n",
      "VAL: EPOCH 73/100 | BATCH 5/8 | LOSS: 1.4510062101180665e-05\n",
      "VAL: EPOCH 73/100 | BATCH 6/8 | LOSS: 1.4893416976389875e-05\n",
      "VAL: EPOCH 73/100 | BATCH 7/8 | LOSS: 1.4539179915118439e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 0/71 | LOSS: 1.4919326531526167e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 1/71 | LOSS: 1.7039518297679024e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 2/71 | LOSS: 1.7035441184513427e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 3/71 | LOSS: 1.659902432038507e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 4/71 | LOSS: 1.6733048505557234e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 5/71 | LOSS: 1.701305230502233e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 6/71 | LOSS: 1.6252046797100255e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 7/71 | LOSS: 1.594090190337738e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 8/71 | LOSS: 1.588239279549776e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 9/71 | LOSS: 1.5647192230971996e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 10/71 | LOSS: 1.5581496964759108e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 11/71 | LOSS: 1.5356486225452198e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 12/71 | LOSS: 1.5281720572182585e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 13/71 | LOSS: 1.565483288296881e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 14/71 | LOSS: 1.585261531242092e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 15/71 | LOSS: 1.562299922852617e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 16/71 | LOSS: 1.5794927468745943e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 17/71 | LOSS: 1.5716167935251076e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 18/71 | LOSS: 1.5673472932432657e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 19/71 | LOSS: 1.5443176016560757e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 20/71 | LOSS: 1.5641780919395387e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 21/71 | LOSS: 1.56423649803566e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 22/71 | LOSS: 1.5644873992246374e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 23/71 | LOSS: 1.5581415330719512e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 24/71 | LOSS: 1.5556056569039354e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 25/71 | LOSS: 1.5377950183812154e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 26/71 | LOSS: 1.527239227369945e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 27/71 | LOSS: 1.5310746480931163e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 28/71 | LOSS: 1.5366289015469975e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 29/71 | LOSS: 1.5294967488443946e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 30/71 | LOSS: 1.529914392918844e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 31/71 | LOSS: 1.5243845439272263e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 32/71 | LOSS: 1.5196909082464337e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 33/71 | LOSS: 1.5082828828299706e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 34/71 | LOSS: 1.5100295084786402e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 35/71 | LOSS: 1.5121187794243775e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 36/71 | LOSS: 1.5013993327778047e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 37/71 | LOSS: 1.5039112799728938e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 38/71 | LOSS: 1.5102845864954035e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 39/71 | LOSS: 1.5117466068659268e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 40/71 | LOSS: 1.5224677487666913e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 41/71 | LOSS: 1.533574363964449e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 42/71 | LOSS: 1.5400827028632953e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 43/71 | LOSS: 1.5349704633329317e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 44/71 | LOSS: 1.5315784124444085e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 45/71 | LOSS: 1.5445034023287008e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 46/71 | LOSS: 1.5519053414073882e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 47/71 | LOSS: 1.5476415361111624e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 48/71 | LOSS: 1.545805148174511e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 49/71 | LOSS: 1.540207857033238e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 50/71 | LOSS: 1.5311834467152664e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 51/71 | LOSS: 1.5293520916132435e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 52/71 | LOSS: 1.537444511069345e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 53/71 | LOSS: 1.5347938187915133e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 54/71 | LOSS: 1.5314226055290253e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 55/71 | LOSS: 1.5391058582281403e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 56/71 | LOSS: 1.5407927381410875e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 57/71 | LOSS: 1.536988772919546e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 58/71 | LOSS: 1.5313294925825607e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 59/71 | LOSS: 1.5298232953379434e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 60/71 | LOSS: 1.5366737599953146e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 61/71 | LOSS: 1.5355485937752454e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 62/71 | LOSS: 1.541597467867033e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 63/71 | LOSS: 1.5414203872410326e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 64/71 | LOSS: 1.542219684219722e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 65/71 | LOSS: 1.5389789496111916e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 66/71 | LOSS: 1.5361300816650002e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 67/71 | LOSS: 1.538124067391276e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 68/71 | LOSS: 1.536230332061203e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 69/71 | LOSS: 1.545740591869357e-05\n",
      "TRAIN: EPOCH 74/100 | BATCH 70/71 | LOSS: 1.5500960216741078e-05\n",
      "VAL: EPOCH 74/100 | BATCH 0/8 | LOSS: 1.8073495084536262e-05\n",
      "VAL: EPOCH 74/100 | BATCH 1/8 | LOSS: 1.4593998457712587e-05\n",
      "VAL: EPOCH 74/100 | BATCH 2/8 | LOSS: 1.48067935394162e-05\n",
      "VAL: EPOCH 74/100 | BATCH 3/8 | LOSS: 1.563806495141762e-05\n",
      "VAL: EPOCH 74/100 | BATCH 4/8 | LOSS: 1.542846075608395e-05\n",
      "VAL: EPOCH 74/100 | BATCH 5/8 | LOSS: 1.4756827567907749e-05\n",
      "VAL: EPOCH 74/100 | BATCH 6/8 | LOSS: 1.5092040874670992e-05\n",
      "VAL: EPOCH 74/100 | BATCH 7/8 | LOSS: 1.4694217043142999e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 0/71 | LOSS: 1.4091952834860422e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 1/71 | LOSS: 1.6050099475251045e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 2/71 | LOSS: 1.714374472309525e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 3/71 | LOSS: 1.6726195099181496e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 4/71 | LOSS: 1.7197141642100177e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 5/71 | LOSS: 1.657820500137556e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 6/71 | LOSS: 1.6420425708929542e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 7/71 | LOSS: 1.6344932987522043e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 8/71 | LOSS: 1.6144752407853957e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 9/71 | LOSS: 1.6354841591237346e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 10/71 | LOSS: 1.6158752094435673e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 11/71 | LOSS: 1.5858475383841626e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 12/71 | LOSS: 1.5702049005575154e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 13/71 | LOSS: 1.591579835543858e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 14/71 | LOSS: 1.592565574052666e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 15/71 | LOSS: 1.5825161597149418e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 16/71 | LOSS: 1.5698499148985482e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 17/71 | LOSS: 1.5748931117640394e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 18/71 | LOSS: 1.568019376320176e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 19/71 | LOSS: 1.5570120922347996e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 20/71 | LOSS: 1.5529677901733002e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 21/71 | LOSS: 1.550165106064841e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 22/71 | LOSS: 1.55313343740367e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 23/71 | LOSS: 1.5500353015340806e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 24/71 | LOSS: 1.5277336788130925e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 25/71 | LOSS: 1.5318887003209406e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 26/71 | LOSS: 1.5231729540337929e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 27/71 | LOSS: 1.5229155841162928e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 28/71 | LOSS: 1.5235610182320795e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 29/71 | LOSS: 1.5183354086426941e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 30/71 | LOSS: 1.5111433170996832e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 31/71 | LOSS: 1.4916673961806737e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 32/71 | LOSS: 1.492093176211435e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 33/71 | LOSS: 1.4896309012863853e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 34/71 | LOSS: 1.4932600424799603e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 35/71 | LOSS: 1.4955657686894281e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 36/71 | LOSS: 1.4961058474688577e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 37/71 | LOSS: 1.4936903545802985e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 38/71 | LOSS: 1.4930055467396951e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 39/71 | LOSS: 1.4888880832586437e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 40/71 | LOSS: 1.4834364486798622e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 41/71 | LOSS: 1.4792599077260266e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 42/71 | LOSS: 1.4765578250029817e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 43/71 | LOSS: 1.4788957659303295e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 44/71 | LOSS: 1.4712298070662656e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 45/71 | LOSS: 1.4661507817515446e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 46/71 | LOSS: 1.4726560522753874e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 47/71 | LOSS: 1.4668559951284502e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 48/71 | LOSS: 1.4632920970236264e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 49/71 | LOSS: 1.470247972974903e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 50/71 | LOSS: 1.472385757355654e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 51/71 | LOSS: 1.4669543833616235e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 52/71 | LOSS: 1.4637165341415346e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 53/71 | LOSS: 1.4607223790705945e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 54/71 | LOSS: 1.4705836160415361e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 55/71 | LOSS: 1.471829880626631e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 56/71 | LOSS: 1.4683954919236017e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 57/71 | LOSS: 1.4761575443869867e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 58/71 | LOSS: 1.4796515889598996e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 59/71 | LOSS: 1.4749019222411637e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 60/71 | LOSS: 1.4815828196378043e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 61/71 | LOSS: 1.4802005851731426e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 62/71 | LOSS: 1.4789414149524259e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 63/71 | LOSS: 1.4746046886671138e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 64/71 | LOSS: 1.4746044213937308e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 65/71 | LOSS: 1.474897073256381e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 66/71 | LOSS: 1.475207668658756e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 67/71 | LOSS: 1.4702427814186462e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 68/71 | LOSS: 1.4720454653771425e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 69/71 | LOSS: 1.4757694888040922e-05\n",
      "TRAIN: EPOCH 75/100 | BATCH 70/71 | LOSS: 1.4758404971179914e-05\n",
      "VAL: EPOCH 75/100 | BATCH 0/8 | LOSS: 1.8883230950450525e-05\n",
      "VAL: EPOCH 75/100 | BATCH 1/8 | LOSS: 1.5243392226693686e-05\n",
      "VAL: EPOCH 75/100 | BATCH 2/8 | LOSS: 1.51679896589485e-05\n",
      "VAL: EPOCH 75/100 | BATCH 3/8 | LOSS: 1.5738947467980324e-05\n",
      "VAL: EPOCH 75/100 | BATCH 4/8 | LOSS: 1.5367966625490226e-05\n",
      "VAL: EPOCH 75/100 | BATCH 5/8 | LOSS: 1.4730746443092357e-05\n",
      "VAL: EPOCH 75/100 | BATCH 6/8 | LOSS: 1.4954756677200618e-05\n",
      "VAL: EPOCH 75/100 | BATCH 7/8 | LOSS: 1.4562223327629908e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 0/71 | LOSS: 1.4225859558791853e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 1/71 | LOSS: 1.3208109976403648e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 2/71 | LOSS: 1.3372531005491814e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 3/71 | LOSS: 1.3386485989030916e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 4/71 | LOSS: 1.3590811977337581e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 5/71 | LOSS: 1.3593296595596863e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 6/71 | LOSS: 1.4055002923539308e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 7/71 | LOSS: 1.4352418475027662e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 8/71 | LOSS: 1.4630159866000111e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 9/71 | LOSS: 1.4905211901350413e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 10/71 | LOSS: 1.4925507044112733e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 11/71 | LOSS: 1.4733780138461347e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 12/71 | LOSS: 1.4619273945125035e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 13/71 | LOSS: 1.4784021004743408e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 14/71 | LOSS: 1.4796816867601591e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 15/71 | LOSS: 1.4655935444807255e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 16/71 | LOSS: 1.4881108815162032e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 17/71 | LOSS: 1.467647724792995e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 18/71 | LOSS: 1.4552624299157238e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 19/71 | LOSS: 1.4476553496933775e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 20/71 | LOSS: 1.4708356978926098e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 21/71 | LOSS: 1.4646306226495653e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 22/71 | LOSS: 1.4509438820507215e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 23/71 | LOSS: 1.4409302442193924e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 24/71 | LOSS: 1.4432109601330012e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 25/71 | LOSS: 1.4435768697204856e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 26/71 | LOSS: 1.4384447947830065e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 27/71 | LOSS: 1.4364538336459581e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 28/71 | LOSS: 1.4345891721514536e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 29/71 | LOSS: 1.4433782234846149e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 30/71 | LOSS: 1.4482330844921601e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 31/71 | LOSS: 1.4519654598643683e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 32/71 | LOSS: 1.442463138973284e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 33/71 | LOSS: 1.4274678131401101e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 34/71 | LOSS: 1.4405044099216217e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 35/71 | LOSS: 1.442654956917977e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 36/71 | LOSS: 1.4366959181348352e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 37/71 | LOSS: 1.44152022585331e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 38/71 | LOSS: 1.4348306276354915e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 39/71 | LOSS: 1.4407279786610161e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 40/71 | LOSS: 1.4421382419219831e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 41/71 | LOSS: 1.4474494881661875e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 42/71 | LOSS: 1.4444392438108903e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 43/71 | LOSS: 1.4389923721451355e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 44/71 | LOSS: 1.4389171216559286e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 45/71 | LOSS: 1.4436744727983909e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 46/71 | LOSS: 1.4424724290717115e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 47/71 | LOSS: 1.437260440676861e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 48/71 | LOSS: 1.439240210801268e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 49/71 | LOSS: 1.4420894767681603e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 50/71 | LOSS: 1.439102336863631e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 51/71 | LOSS: 1.4350113430667148e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 52/71 | LOSS: 1.43994081708853e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 53/71 | LOSS: 1.4395516414138169e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 54/71 | LOSS: 1.4350257267572239e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 55/71 | LOSS: 1.440563466660803e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 56/71 | LOSS: 1.437384213332144e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 57/71 | LOSS: 1.4346048499583989e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 58/71 | LOSS: 1.434131012255251e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 59/71 | LOSS: 1.434912160220847e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 60/71 | LOSS: 1.4335282431885822e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 61/71 | LOSS: 1.430305698499245e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 62/71 | LOSS: 1.4308835979757113e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 63/71 | LOSS: 1.4326102458994683e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 64/71 | LOSS: 1.4377456948120828e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 65/71 | LOSS: 1.4404472839294503e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 66/71 | LOSS: 1.4366192351475032e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 67/71 | LOSS: 1.4397236354317797e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 68/71 | LOSS: 1.4373254234142486e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 69/71 | LOSS: 1.4352727742204608e-05\n",
      "TRAIN: EPOCH 76/100 | BATCH 70/71 | LOSS: 1.4340721643005144e-05\n",
      "VAL: EPOCH 76/100 | BATCH 0/8 | LOSS: 2.0945455617038533e-05\n",
      "VAL: EPOCH 76/100 | BATCH 1/8 | LOSS: 1.6747911104175728e-05\n",
      "VAL: EPOCH 76/100 | BATCH 2/8 | LOSS: 1.5966267104280025e-05\n",
      "VAL: EPOCH 76/100 | BATCH 3/8 | LOSS: 1.7061477819879656e-05\n",
      "VAL: EPOCH 76/100 | BATCH 4/8 | LOSS: 1.6866696932993362e-05\n",
      "VAL: EPOCH 76/100 | BATCH 5/8 | LOSS: 1.6168789443327114e-05\n",
      "VAL: EPOCH 76/100 | BATCH 6/8 | LOSS: 1.634253515346375e-05\n",
      "VAL: EPOCH 76/100 | BATCH 7/8 | LOSS: 1.5767080867590266e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 0/71 | LOSS: 1.769574373611249e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 1/71 | LOSS: 1.6687259630998597e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 2/71 | LOSS: 1.5651439146798413e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 3/71 | LOSS: 1.5153599179029698e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 4/71 | LOSS: 1.5860350686125457e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 5/71 | LOSS: 1.6487532775499858e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 6/71 | LOSS: 1.574058010841587e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 7/71 | LOSS: 1.5763164810778107e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 8/71 | LOSS: 1.5590643670293503e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 9/71 | LOSS: 1.5195370542642194e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 10/71 | LOSS: 1.5345490696331993e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 11/71 | LOSS: 1.5314025783178902e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 12/71 | LOSS: 1.551487121804922e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 13/71 | LOSS: 1.5493849657234803e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 14/71 | LOSS: 1.5192494659762209e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 15/71 | LOSS: 1.5172795997386856e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 16/71 | LOSS: 1.4914610801497474e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 17/71 | LOSS: 1.482065969563943e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 18/71 | LOSS: 1.4731261264961703e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 19/71 | LOSS: 1.4696509106215672e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 20/71 | LOSS: 1.4589375972545462e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 21/71 | LOSS: 1.444918040182198e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 22/71 | LOSS: 1.4426501168244336e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 23/71 | LOSS: 1.4422250728785002e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 24/71 | LOSS: 1.4391334079846274e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 25/71 | LOSS: 1.4251826437080475e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 26/71 | LOSS: 1.4171570708438392e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 27/71 | LOSS: 1.4187886888196641e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 28/71 | LOSS: 1.4303849011725859e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 29/71 | LOSS: 1.4407118972788643e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 30/71 | LOSS: 1.4408863887271558e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 31/71 | LOSS: 1.4460715902941956e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 32/71 | LOSS: 1.4510464786849987e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 33/71 | LOSS: 1.4494712960346506e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 34/71 | LOSS: 1.4692019742921859e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 35/71 | LOSS: 1.4621520146344361e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 36/71 | LOSS: 1.4556826548918301e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 37/71 | LOSS: 1.4540402892299953e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 38/71 | LOSS: 1.4670855540689678e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 39/71 | LOSS: 1.4598834559365059e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 40/71 | LOSS: 1.4579629750209634e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 41/71 | LOSS: 1.46451868983734e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 42/71 | LOSS: 1.4681625797556779e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 43/71 | LOSS: 1.4623113408809083e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 44/71 | LOSS: 1.462005517775348e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 45/71 | LOSS: 1.4654791377087468e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 46/71 | LOSS: 1.4664625713850639e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 47/71 | LOSS: 1.4688210910662747e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 48/71 | LOSS: 1.4745849586354464e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 49/71 | LOSS: 1.479202654081746e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 50/71 | LOSS: 1.4826005955161664e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 51/71 | LOSS: 1.4790343199750238e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 52/71 | LOSS: 1.4831080663051975e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 53/71 | LOSS: 1.4845104437376398e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 54/71 | LOSS: 1.4796377879313447e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 55/71 | LOSS: 1.4731836163913872e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 56/71 | LOSS: 1.4791404027435642e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 57/71 | LOSS: 1.490328353109613e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 58/71 | LOSS: 1.4904866675659817e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 59/71 | LOSS: 1.5021607866098445e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 60/71 | LOSS: 1.5113586787116278e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 61/71 | LOSS: 1.5133578618852265e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 62/71 | LOSS: 1.5286445107319325e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 63/71 | LOSS: 1.530100503543963e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 64/71 | LOSS: 1.5277176204947934e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 65/71 | LOSS: 1.528843860355802e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 66/71 | LOSS: 1.533259014582084e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 67/71 | LOSS: 1.5342518549112036e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 68/71 | LOSS: 1.5400785636355042e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 69/71 | LOSS: 1.544825672681327e-05\n",
      "TRAIN: EPOCH 77/100 | BATCH 70/71 | LOSS: 1.551888193793736e-05\n",
      "VAL: EPOCH 77/100 | BATCH 0/8 | LOSS: 1.5921683370834216e-05\n",
      "VAL: EPOCH 77/100 | BATCH 1/8 | LOSS: 1.3168988971301587e-05\n",
      "VAL: EPOCH 77/100 | BATCH 2/8 | LOSS: 1.3899184523324948e-05\n",
      "VAL: EPOCH 77/100 | BATCH 3/8 | LOSS: 1.5139910601646989e-05\n",
      "VAL: EPOCH 77/100 | BATCH 4/8 | LOSS: 1.5045014151837677e-05\n",
      "VAL: EPOCH 77/100 | BATCH 5/8 | LOSS: 1.4658756754215574e-05\n",
      "VAL: EPOCH 77/100 | BATCH 6/8 | LOSS: 1.503816058954856e-05\n",
      "VAL: EPOCH 77/100 | BATCH 7/8 | LOSS: 1.4526486097565794e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 0/71 | LOSS: 1.6301737559842877e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 1/71 | LOSS: 1.8202000319433864e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 2/71 | LOSS: 1.803269090790612e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 3/71 | LOSS: 1.756081655912567e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 4/71 | LOSS: 1.6175381097127685e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 5/71 | LOSS: 1.8037335091018274e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 6/71 | LOSS: 1.822842724712765e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 7/71 | LOSS: 1.8150887171941577e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 8/71 | LOSS: 1.8611062437735705e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 9/71 | LOSS: 1.8702762827160768e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 10/71 | LOSS: 1.8210507236786228e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 11/71 | LOSS: 1.8125948599845287e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 12/71 | LOSS: 1.8811008465910654e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 13/71 | LOSS: 1.8365199593972648e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 14/71 | LOSS: 1.826419872183275e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 15/71 | LOSS: 1.827425279543604e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 16/71 | LOSS: 1.8247600029794473e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 17/71 | LOSS: 1.7996248667865682e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 18/71 | LOSS: 1.8060308272987114e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 19/71 | LOSS: 1.7796032398109674e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 20/71 | LOSS: 1.76677978656482e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 21/71 | LOSS: 1.751032579539407e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 22/71 | LOSS: 1.745482800090584e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 23/71 | LOSS: 1.7138205104553588e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 24/71 | LOSS: 1.700729877484264e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 25/71 | LOSS: 1.6829428659548284e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 26/71 | LOSS: 1.6671557407082422e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 27/71 | LOSS: 1.6432341323837007e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 28/71 | LOSS: 1.6215619325521402e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 29/71 | LOSS: 1.6068993075653756e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 30/71 | LOSS: 1.6088290067273192e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 31/71 | LOSS: 1.6036010322295624e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 32/71 | LOSS: 1.5918788173594017e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 33/71 | LOSS: 1.5896485793371755e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 34/71 | LOSS: 1.5848900797469207e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 35/71 | LOSS: 1.575939321204108e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 36/71 | LOSS: 1.577548113886764e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 37/71 | LOSS: 1.5796497145378164e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 38/71 | LOSS: 1.5744311895105056e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 39/71 | LOSS: 1.5648414046154357e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 40/71 | LOSS: 1.5506739367904117e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 41/71 | LOSS: 1.5406758436819517e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 42/71 | LOSS: 1.5389346307804158e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 43/71 | LOSS: 1.5296898110054e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 44/71 | LOSS: 1.5271663424856443e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 45/71 | LOSS: 1.5216852874721592e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 46/71 | LOSS: 1.5189464589793909e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 47/71 | LOSS: 1.5092875154702293e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 48/71 | LOSS: 1.5032827516789643e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 49/71 | LOSS: 1.5007144029368646e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 50/71 | LOSS: 1.4954696744811658e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 51/71 | LOSS: 1.4926560355049263e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 52/71 | LOSS: 1.4845716835242336e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 53/71 | LOSS: 1.4801251701875892e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 54/71 | LOSS: 1.4766165771685667e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 55/71 | LOSS: 1.4759600357397826e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 56/71 | LOSS: 1.4777613869866177e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 57/71 | LOSS: 1.4762979248942053e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 58/71 | LOSS: 1.476699863984723e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 59/71 | LOSS: 1.4780862632809052e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 60/71 | LOSS: 1.4758823364572294e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 61/71 | LOSS: 1.4765393438386625e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 62/71 | LOSS: 1.478251137346628e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 63/71 | LOSS: 1.4841072129456734e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 64/71 | LOSS: 1.4842381516735678e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 65/71 | LOSS: 1.481290211923999e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 66/71 | LOSS: 1.4832770504463993e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 67/71 | LOSS: 1.4813338673454436e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 68/71 | LOSS: 1.4774037863653444e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 69/71 | LOSS: 1.4746677853898811e-05\n",
      "TRAIN: EPOCH 78/100 | BATCH 70/71 | LOSS: 1.467366650436995e-05\n",
      "VAL: EPOCH 78/100 | BATCH 0/8 | LOSS: 2.1037158148828894e-05\n",
      "VAL: EPOCH 78/100 | BATCH 1/8 | LOSS: 1.7782784198061563e-05\n",
      "VAL: EPOCH 78/100 | BATCH 2/8 | LOSS: 1.734766919980757e-05\n",
      "VAL: EPOCH 78/100 | BATCH 3/8 | LOSS: 1.7930273315869272e-05\n",
      "VAL: EPOCH 78/100 | BATCH 4/8 | LOSS: 1.756162710080389e-05\n",
      "VAL: EPOCH 78/100 | BATCH 5/8 | LOSS: 1.6833913226340275e-05\n",
      "VAL: EPOCH 78/100 | BATCH 6/8 | LOSS: 1.6971124361069606e-05\n",
      "VAL: EPOCH 78/100 | BATCH 7/8 | LOSS: 1.6581230397605395e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 0/71 | LOSS: 1.453771437809337e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 1/71 | LOSS: 1.4699200619361363e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 2/71 | LOSS: 1.4224711776478216e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 3/71 | LOSS: 1.4758082215848844e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 4/71 | LOSS: 1.5097626601345837e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 5/71 | LOSS: 1.5046319276734721e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 6/71 | LOSS: 1.4597415949018406e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 7/71 | LOSS: 1.5262120541592594e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 8/71 | LOSS: 1.5349802399416352e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 9/71 | LOSS: 1.5349355089711024e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 10/71 | LOSS: 1.5217748610170515e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 11/71 | LOSS: 1.501807120500113e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 12/71 | LOSS: 1.553196901883124e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 13/71 | LOSS: 1.574600504942022e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 14/71 | LOSS: 1.5532560973952057e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 15/71 | LOSS: 1.5408020715312887e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 16/71 | LOSS: 1.5461309297226014e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 17/71 | LOSS: 1.5561755440608573e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 18/71 | LOSS: 1.5678924922356503e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 19/71 | LOSS: 1.5367311061709187e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 20/71 | LOSS: 1.5174238363951667e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 21/71 | LOSS: 1.541056694391459e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 22/71 | LOSS: 1.5535077577426463e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 23/71 | LOSS: 1.551246214148705e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 24/71 | LOSS: 1.535000999865588e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 25/71 | LOSS: 1.5206285052744743e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 26/71 | LOSS: 1.5203648082980956e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 27/71 | LOSS: 1.516349584562704e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 28/71 | LOSS: 1.5076432614948536e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 29/71 | LOSS: 1.5180502911486353e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 30/71 | LOSS: 1.5124951929582523e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 31/71 | LOSS: 1.5029106833708283e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 32/71 | LOSS: 1.4927766532116838e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 33/71 | LOSS: 1.4809144271138767e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 34/71 | LOSS: 1.4923141160189905e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 35/71 | LOSS: 1.4925793518866865e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 36/71 | LOSS: 1.4868720009454174e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 37/71 | LOSS: 1.4832637708949684e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 38/71 | LOSS: 1.4846884742161283e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 39/71 | LOSS: 1.4817594978921988e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 40/71 | LOSS: 1.478057031529649e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 41/71 | LOSS: 1.482216937022583e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 42/71 | LOSS: 1.4813836652286036e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 43/71 | LOSS: 1.4797613113676727e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 44/71 | LOSS: 1.4898420704412275e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 45/71 | LOSS: 1.4916825342009796e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 46/71 | LOSS: 1.479938548575948e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 47/71 | LOSS: 1.4730694450311907e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 48/71 | LOSS: 1.4686453085937964e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 49/71 | LOSS: 1.4722130272275535e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 50/71 | LOSS: 1.4621774247572438e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 51/71 | LOSS: 1.4616865917979829e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 52/71 | LOSS: 1.4600847604907478e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 53/71 | LOSS: 1.4650159065004792e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 54/71 | LOSS: 1.4619636815702754e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 55/71 | LOSS: 1.4689026053409699e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 56/71 | LOSS: 1.4654357360804601e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 57/71 | LOSS: 1.4600911934645285e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 58/71 | LOSS: 1.4654245005350291e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 59/71 | LOSS: 1.463459805866781e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 60/71 | LOSS: 1.4571958642785025e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 61/71 | LOSS: 1.461248825029132e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 62/71 | LOSS: 1.465274949118288e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 63/71 | LOSS: 1.4653138038056568e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 64/71 | LOSS: 1.4608065010599183e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 65/71 | LOSS: 1.4592487842133833e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 66/71 | LOSS: 1.4539929323481332e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 67/71 | LOSS: 1.4498336614612045e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 68/71 | LOSS: 1.446806523428835e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 69/71 | LOSS: 1.4473833380179712e-05\n",
      "TRAIN: EPOCH 79/100 | BATCH 70/71 | LOSS: 1.453172655960194e-05\n",
      "VAL: EPOCH 79/100 | BATCH 0/8 | LOSS: 1.7451644453103654e-05\n",
      "VAL: EPOCH 79/100 | BATCH 1/8 | LOSS: 1.4530236512655392e-05\n",
      "VAL: EPOCH 79/100 | BATCH 2/8 | LOSS: 1.482850287478262e-05\n",
      "VAL: EPOCH 79/100 | BATCH 3/8 | LOSS: 1.537091338832397e-05\n",
      "VAL: EPOCH 79/100 | BATCH 4/8 | LOSS: 1.5005127534095663e-05\n",
      "VAL: EPOCH 79/100 | BATCH 5/8 | LOSS: 1.4439342521654908e-05\n",
      "VAL: EPOCH 79/100 | BATCH 6/8 | LOSS: 1.4551388630934525e-05\n",
      "VAL: EPOCH 79/100 | BATCH 7/8 | LOSS: 1.4195759945323516e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 0/71 | LOSS: 1.1348422049195506e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 1/71 | LOSS: 1.2955323200003477e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 2/71 | LOSS: 1.3644240425492171e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 3/71 | LOSS: 1.4038573908692342e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 4/71 | LOSS: 1.4123402797849849e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 5/71 | LOSS: 1.4675542540013945e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 6/71 | LOSS: 1.4699262984712342e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 7/71 | LOSS: 1.469944027121528e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 8/71 | LOSS: 1.4564958392939945e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 9/71 | LOSS: 1.4613813254982233e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 10/71 | LOSS: 1.4639519411668351e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 11/71 | LOSS: 1.4602169054948414e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 12/71 | LOSS: 1.4777488663989621e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 13/71 | LOSS: 1.469355629524216e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 14/71 | LOSS: 1.4647721218352672e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 15/71 | LOSS: 1.4809099013746163e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 16/71 | LOSS: 1.454021789600922e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 17/71 | LOSS: 1.4533636961762872e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 18/71 | LOSS: 1.4329846874859772e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 19/71 | LOSS: 1.4355298026202945e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 20/71 | LOSS: 1.4390401529020719e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 21/71 | LOSS: 1.4423527318285778e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 22/71 | LOSS: 1.4415049720454313e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 23/71 | LOSS: 1.441778264658448e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 24/71 | LOSS: 1.4295338369265664e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 25/71 | LOSS: 1.4205576252816872e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 26/71 | LOSS: 1.4279783465974981e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 27/71 | LOSS: 1.4256538666554011e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 28/71 | LOSS: 1.4304856041893925e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 29/71 | LOSS: 1.4300176765876434e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 30/71 | LOSS: 1.4313262821111317e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 31/71 | LOSS: 1.4240549973010275e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 32/71 | LOSS: 1.4413967288750097e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 33/71 | LOSS: 1.4361891691591796e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 34/71 | LOSS: 1.4371838811452367e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 35/71 | LOSS: 1.4318826490327612e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 36/71 | LOSS: 1.4302204756343082e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 37/71 | LOSS: 1.4380209043003178e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 38/71 | LOSS: 1.4304090188652015e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 39/71 | LOSS: 1.4282725987868617e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 40/71 | LOSS: 1.4238047652924717e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 41/71 | LOSS: 1.42313032094015e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 42/71 | LOSS: 1.4134589946479537e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 43/71 | LOSS: 1.4114639518745456e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 44/71 | LOSS: 1.4218127681589168e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 45/71 | LOSS: 1.4252329888024732e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 46/71 | LOSS: 1.4200355353557128e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 47/71 | LOSS: 1.4182930575164695e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 48/71 | LOSS: 1.4192837977821092e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 49/71 | LOSS: 1.4174742882460122e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 50/71 | LOSS: 1.4171908065584242e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 51/71 | LOSS: 1.4233248905908953e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 52/71 | LOSS: 1.4251427805816693e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 53/71 | LOSS: 1.4265526917572262e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 54/71 | LOSS: 1.4351698353659066e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 55/71 | LOSS: 1.4336824619931576e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 56/71 | LOSS: 1.435445232856549e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 57/71 | LOSS: 1.433695430525153e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 58/71 | LOSS: 1.4373430222819432e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 59/71 | LOSS: 1.4331609054352157e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 60/71 | LOSS: 1.4306328041845414e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 61/71 | LOSS: 1.4306162293289691e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 62/71 | LOSS: 1.431329735443421e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 63/71 | LOSS: 1.4239446414876511e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 64/71 | LOSS: 1.4284639613693938e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 65/71 | LOSS: 1.4316514407806663e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 66/71 | LOSS: 1.4364000754309262e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 67/71 | LOSS: 1.4324502765649573e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 68/71 | LOSS: 1.4322076321400676e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 69/71 | LOSS: 1.4332219637123802e-05\n",
      "TRAIN: EPOCH 80/100 | BATCH 70/71 | LOSS: 1.4383742791964529e-05\n",
      "VAL: EPOCH 80/100 | BATCH 0/8 | LOSS: 1.8398444808553904e-05\n",
      "VAL: EPOCH 80/100 | BATCH 1/8 | LOSS: 1.515351323178038e-05\n",
      "VAL: EPOCH 80/100 | BATCH 2/8 | LOSS: 1.513941121326449e-05\n",
      "VAL: EPOCH 80/100 | BATCH 3/8 | LOSS: 1.573127792653395e-05\n",
      "VAL: EPOCH 80/100 | BATCH 4/8 | LOSS: 1.5444330347236247e-05\n",
      "VAL: EPOCH 80/100 | BATCH 5/8 | LOSS: 1.4874840720343249e-05\n",
      "VAL: EPOCH 80/100 | BATCH 6/8 | LOSS: 1.4850238715423205e-05\n",
      "VAL: EPOCH 80/100 | BATCH 7/8 | LOSS: 1.4389920806934242e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 0/71 | LOSS: 1.3167794350010809e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 1/71 | LOSS: 1.4720529634359991e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 2/71 | LOSS: 1.4062498545778604e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 3/71 | LOSS: 1.3451802942654467e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 4/71 | LOSS: 1.4556512724084314e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 5/71 | LOSS: 1.5109545680995021e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 6/71 | LOSS: 1.4562018675081032e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 7/71 | LOSS: 1.4967138440624694e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 8/71 | LOSS: 1.5089865579890708e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 9/71 | LOSS: 1.4870118775434094e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 10/71 | LOSS: 1.4610039133086419e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 11/71 | LOSS: 1.4927448243421773e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 12/71 | LOSS: 1.4879139878478152e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 13/71 | LOSS: 1.4782232028665021e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 14/71 | LOSS: 1.5007229618883381e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 15/71 | LOSS: 1.471652734608142e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 16/71 | LOSS: 1.4542198882469057e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 17/71 | LOSS: 1.466442189565795e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 18/71 | LOSS: 1.4557792718206705e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 19/71 | LOSS: 1.4325206529974821e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 20/71 | LOSS: 1.4439878950291868e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 21/71 | LOSS: 1.4342712058185663e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 22/71 | LOSS: 1.4257502026220722e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 23/71 | LOSS: 1.4060743941020823e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 24/71 | LOSS: 1.4235295275284443e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 25/71 | LOSS: 1.426037400821e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 26/71 | LOSS: 1.4186237422109116e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 27/71 | LOSS: 1.4292743376894837e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 28/71 | LOSS: 1.4234943372339958e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 29/71 | LOSS: 1.4099042982707032e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 30/71 | LOSS: 1.412393348507448e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 31/71 | LOSS: 1.4176136971855158e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 32/71 | LOSS: 1.4075968437916316e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 33/71 | LOSS: 1.4170943573202856e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 34/71 | LOSS: 1.4179302473036971e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 35/71 | LOSS: 1.4138693232881553e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 36/71 | LOSS: 1.4229534261520146e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 37/71 | LOSS: 1.4277156591752397e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 38/71 | LOSS: 1.4246613738158396e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 39/71 | LOSS: 1.4207791514309065e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 40/71 | LOSS: 1.4187181078319199e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 41/71 | LOSS: 1.416056548329098e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 42/71 | LOSS: 1.4203507203620935e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 43/71 | LOSS: 1.4239235394019158e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 44/71 | LOSS: 1.4188293587519891e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 45/71 | LOSS: 1.4107696781296829e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 46/71 | LOSS: 1.4104086771344845e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 47/71 | LOSS: 1.403080627445282e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 48/71 | LOSS: 1.4100062799116131e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 49/71 | LOSS: 1.4070591423660517e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 50/71 | LOSS: 1.4116995967639263e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 51/71 | LOSS: 1.4118587317240712e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 52/71 | LOSS: 1.4037166006603928e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 53/71 | LOSS: 1.4013445555368284e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 54/71 | LOSS: 1.4048721674374643e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 55/71 | LOSS: 1.3990846809974755e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 56/71 | LOSS: 1.3977862819733114e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 57/71 | LOSS: 1.390327856668215e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 58/71 | LOSS: 1.390496833926714e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 59/71 | LOSS: 1.3915767764653235e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 60/71 | LOSS: 1.3901555695290464e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 61/71 | LOSS: 1.3917059937563365e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 62/71 | LOSS: 1.388537882684475e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 63/71 | LOSS: 1.3827068997329661e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 64/71 | LOSS: 1.3866417052318306e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 65/71 | LOSS: 1.3874018432860877e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 66/71 | LOSS: 1.396713957832101e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 67/71 | LOSS: 1.3958034809119828e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 68/71 | LOSS: 1.398140189532727e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 69/71 | LOSS: 1.3949537826972248e-05\n",
      "TRAIN: EPOCH 81/100 | BATCH 70/71 | LOSS: 1.389127216280543e-05\n",
      "VAL: EPOCH 81/100 | BATCH 0/8 | LOSS: 1.6150965166161768e-05\n",
      "VAL: EPOCH 81/100 | BATCH 1/8 | LOSS: 1.3069858141534496e-05\n",
      "VAL: EPOCH 81/100 | BATCH 2/8 | LOSS: 1.364091561602739e-05\n",
      "VAL: EPOCH 81/100 | BATCH 3/8 | LOSS: 1.4321659364213701e-05\n",
      "VAL: EPOCH 81/100 | BATCH 4/8 | LOSS: 1.3992268759466241e-05\n",
      "VAL: EPOCH 81/100 | BATCH 5/8 | LOSS: 1.3452163026765144e-05\n",
      "VAL: EPOCH 81/100 | BATCH 6/8 | LOSS: 1.375443883132123e-05\n",
      "VAL: EPOCH 81/100 | BATCH 7/8 | LOSS: 1.3424537428363692e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 0/71 | LOSS: 1.3565543667937163e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 1/71 | LOSS: 1.3813941222906578e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 2/71 | LOSS: 1.3767997491716718e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 3/71 | LOSS: 1.396117022522958e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 4/71 | LOSS: 1.4469488087343052e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 5/71 | LOSS: 1.3542366862869434e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 6/71 | LOSS: 1.4380348277752222e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 7/71 | LOSS: 1.4211868005986616e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 8/71 | LOSS: 1.4100013686402235e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 9/71 | LOSS: 1.4070867564441869e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 10/71 | LOSS: 1.4238293426222464e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 11/71 | LOSS: 1.409667879670451e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 12/71 | LOSS: 1.444519616947778e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 13/71 | LOSS: 1.4742208220143635e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 14/71 | LOSS: 1.4772504497765719e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 15/71 | LOSS: 1.4814343728630774e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 16/71 | LOSS: 1.4853600987407845e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 17/71 | LOSS: 1.463354323681819e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 18/71 | LOSS: 1.446163002439885e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 19/71 | LOSS: 1.4500642100756522e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 20/71 | LOSS: 1.4344575988832817e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 21/71 | LOSS: 1.431989759029503e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 22/71 | LOSS: 1.4354462188709041e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 23/71 | LOSS: 1.4350017636388657e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 24/71 | LOSS: 1.4286421137512662e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 25/71 | LOSS: 1.4298147561930818e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 26/71 | LOSS: 1.4158908891625254e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 27/71 | LOSS: 1.4083222985001547e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 28/71 | LOSS: 1.3990204526384457e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 29/71 | LOSS: 1.3953110798562799e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 30/71 | LOSS: 1.4006238797587538e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 31/71 | LOSS: 1.3974905328950626e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 32/71 | LOSS: 1.3939064074568733e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 33/71 | LOSS: 1.3918348486186005e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 34/71 | LOSS: 1.3769357272914412e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 35/71 | LOSS: 1.3725786680475318e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 36/71 | LOSS: 1.3710533199333145e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 37/71 | LOSS: 1.364010583864384e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 38/71 | LOSS: 1.358843301437819e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 39/71 | LOSS: 1.3537645895667082e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 40/71 | LOSS: 1.3566801809668882e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 41/71 | LOSS: 1.3622730486011797e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 42/71 | LOSS: 1.3647742741531422e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 43/71 | LOSS: 1.3626716591633423e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 44/71 | LOSS: 1.3635911394279295e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 45/71 | LOSS: 1.3662119984118085e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 46/71 | LOSS: 1.3748377046369491e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 47/71 | LOSS: 1.3723054602602739e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 48/71 | LOSS: 1.3651561468058e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 49/71 | LOSS: 1.3630776957143098e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 50/71 | LOSS: 1.3659290815150028e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 51/71 | LOSS: 1.3642919192044736e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 52/71 | LOSS: 1.3597026925296786e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 53/71 | LOSS: 1.3503166714885824e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 54/71 | LOSS: 1.3485665394083215e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 55/71 | LOSS: 1.3468128664888873e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 56/71 | LOSS: 1.3402910585762329e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 57/71 | LOSS: 1.3368804016424512e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 58/71 | LOSS: 1.3341745401148574e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 59/71 | LOSS: 1.332473957518232e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 60/71 | LOSS: 1.3249946531403588e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 61/71 | LOSS: 1.3194001689219974e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 62/71 | LOSS: 1.3146539574321766e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 63/71 | LOSS: 1.3123043871132722e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 64/71 | LOSS: 1.3114816437547023e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 65/71 | LOSS: 1.3127376929112485e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 66/71 | LOSS: 1.3115377631907199e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 67/71 | LOSS: 1.314439111197049e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 68/71 | LOSS: 1.3197290054054262e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 69/71 | LOSS: 1.3196716554375599e-05\n",
      "TRAIN: EPOCH 82/100 | BATCH 70/71 | LOSS: 1.3210171077139741e-05\n",
      "VAL: EPOCH 82/100 | BATCH 0/8 | LOSS: 1.809943023545202e-05\n",
      "VAL: EPOCH 82/100 | BATCH 1/8 | LOSS: 1.4740675396751612e-05\n",
      "VAL: EPOCH 82/100 | BATCH 2/8 | LOSS: 1.5098051638536466e-05\n",
      "VAL: EPOCH 82/100 | BATCH 3/8 | LOSS: 1.5535734291916015e-05\n",
      "VAL: EPOCH 82/100 | BATCH 4/8 | LOSS: 1.5233157319016754e-05\n",
      "VAL: EPOCH 82/100 | BATCH 5/8 | LOSS: 1.4546307587200621e-05\n",
      "VAL: EPOCH 82/100 | BATCH 6/8 | LOSS: 1.478120066167321e-05\n",
      "VAL: EPOCH 82/100 | BATCH 7/8 | LOSS: 1.440506844119227e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 0/71 | LOSS: 1.7156982721644454e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 1/71 | LOSS: 1.614012080608518e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 2/71 | LOSS: 1.5757422867560916e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 3/71 | LOSS: 1.5733191730760154e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 4/71 | LOSS: 1.5138352864596527e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 5/71 | LOSS: 1.484137859127562e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 6/71 | LOSS: 1.5137170391556407e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 7/71 | LOSS: 1.5300181075872388e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 8/71 | LOSS: 1.47982440871096e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 9/71 | LOSS: 1.4571054725820431e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 10/71 | LOSS: 1.4574349155406129e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 11/71 | LOSS: 1.4576198509530514e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 12/71 | LOSS: 1.442007585585368e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 13/71 | LOSS: 1.4392947053108531e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 14/71 | LOSS: 1.4742875706967122e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 15/71 | LOSS: 1.448091694555842e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 16/71 | LOSS: 1.4468901741458789e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 17/71 | LOSS: 1.4518037409693028e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 18/71 | LOSS: 1.4357068434228973e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 19/71 | LOSS: 1.4361724288391997e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 20/71 | LOSS: 1.4424109438204185e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 21/71 | LOSS: 1.4371597444551298e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 22/71 | LOSS: 1.4350467846151846e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 23/71 | LOSS: 1.455079609513632e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 24/71 | LOSS: 1.4690425887238234e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 25/71 | LOSS: 1.4650888960414495e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 26/71 | LOSS: 1.4606939253604247e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 27/71 | LOSS: 1.4635103785362194e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 28/71 | LOSS: 1.4630262018632994e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 29/71 | LOSS: 1.4511548306472832e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 30/71 | LOSS: 1.4429725847402288e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 31/71 | LOSS: 1.4521929983857262e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 32/71 | LOSS: 1.4471360951905478e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 33/71 | LOSS: 1.4454346883747562e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 34/71 | LOSS: 1.4345163513748308e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 35/71 | LOSS: 1.4325861810801951e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 36/71 | LOSS: 1.4377668407718915e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 37/71 | LOSS: 1.4306492893841718e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 38/71 | LOSS: 1.4278797397519856e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 39/71 | LOSS: 1.4304457431535411e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 40/71 | LOSS: 1.4285766133625436e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 41/71 | LOSS: 1.4316528536472192e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 42/71 | LOSS: 1.4225703210307307e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 43/71 | LOSS: 1.420896834050919e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 44/71 | LOSS: 1.4220827829881779e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 45/71 | LOSS: 1.4178642508657346e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 46/71 | LOSS: 1.4157991845035638e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 47/71 | LOSS: 1.4139626008121317e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 48/71 | LOSS: 1.4087658683467676e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 49/71 | LOSS: 1.408729525792296e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 50/71 | LOSS: 1.3992443450215255e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 51/71 | LOSS: 1.4058461916787424e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 52/71 | LOSS: 1.406957511240968e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 53/71 | LOSS: 1.4159818655806939e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 54/71 | LOSS: 1.4193719008207237e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 55/71 | LOSS: 1.4148028136722652e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 56/71 | LOSS: 1.4157540630421544e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 57/71 | LOSS: 1.4121390747810626e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 58/71 | LOSS: 1.4146970281248195e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 59/71 | LOSS: 1.413735150587551e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 60/71 | LOSS: 1.4148801606321197e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 61/71 | LOSS: 1.4121775620878907e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 62/71 | LOSS: 1.4088839851042611e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 63/71 | LOSS: 1.404285723083376e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 64/71 | LOSS: 1.4014260397324016e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 65/71 | LOSS: 1.3997727102943111e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 66/71 | LOSS: 1.3990519592106697e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 67/71 | LOSS: 1.4025764478313195e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 68/71 | LOSS: 1.3972598118424935e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 69/71 | LOSS: 1.3915533846946866e-05\n",
      "TRAIN: EPOCH 83/100 | BATCH 70/71 | LOSS: 1.3924970248104139e-05\n",
      "VAL: EPOCH 83/100 | BATCH 0/8 | LOSS: 2.5348563212901354e-05\n",
      "VAL: EPOCH 83/100 | BATCH 1/8 | LOSS: 2.0085798496438656e-05\n",
      "VAL: EPOCH 83/100 | BATCH 2/8 | LOSS: 2.1341704875036765e-05\n",
      "VAL: EPOCH 83/100 | BATCH 3/8 | LOSS: 2.2026176793588093e-05\n",
      "VAL: EPOCH 83/100 | BATCH 4/8 | LOSS: 2.157107592211105e-05\n",
      "VAL: EPOCH 83/100 | BATCH 5/8 | LOSS: 2.0664160122881487e-05\n",
      "VAL: EPOCH 83/100 | BATCH 6/8 | LOSS: 2.113026831856197e-05\n",
      "VAL: EPOCH 83/100 | BATCH 7/8 | LOSS: 2.0832426116612623e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 0/71 | LOSS: 2.1332174583221786e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 1/71 | LOSS: 1.6087551102828e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 2/71 | LOSS: 1.6052921940475546e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 3/71 | LOSS: 1.695519335953577e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 4/71 | LOSS: 1.722205797705101e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 5/71 | LOSS: 1.667187264805155e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 6/71 | LOSS: 1.7127563556381835e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 7/71 | LOSS: 1.7016489323395945e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 8/71 | LOSS: 1.676635707149722e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 9/71 | LOSS: 1.6938489443418804e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 10/71 | LOSS: 1.6918767786674753e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 11/71 | LOSS: 1.629007359345754e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 12/71 | LOSS: 1.6195927785772186e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 13/71 | LOSS: 1.626893390493933e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 14/71 | LOSS: 1.5937045342676964e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 15/71 | LOSS: 1.5825549326109467e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 16/71 | LOSS: 1.5654031866527957e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 17/71 | LOSS: 1.5851497614170916e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 18/71 | LOSS: 1.599570355450085e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 19/71 | LOSS: 1.5795309127497604e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 20/71 | LOSS: 1.6248136576212434e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 21/71 | LOSS: 1.621614141682089e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 22/71 | LOSS: 1.6213945973899882e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 23/71 | LOSS: 1.641074857161584e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 24/71 | LOSS: 1.6434093959105665e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 25/71 | LOSS: 1.6352928399171036e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 26/71 | LOSS: 1.6465633490480724e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 27/71 | LOSS: 1.6364222769230503e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 28/71 | LOSS: 1.6197227711833467e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 29/71 | LOSS: 1.611748878834381e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 30/71 | LOSS: 1.6126993291583212e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 31/71 | LOSS: 1.5976764274228117e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 32/71 | LOSS: 1.5895177939180002e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 33/71 | LOSS: 1.5790795420317035e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 34/71 | LOSS: 1.5821502477462802e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 35/71 | LOSS: 1.573336317051144e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 36/71 | LOSS: 1.5659376618419333e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 37/71 | LOSS: 1.5617088904132526e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 38/71 | LOSS: 1.5521929992992693e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 39/71 | LOSS: 1.552909648125933e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 40/71 | LOSS: 1.5401288581459795e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 41/71 | LOSS: 1.5256288969838005e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 42/71 | LOSS: 1.5184600797087917e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 43/71 | LOSS: 1.50490259949782e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 44/71 | LOSS: 1.5000350544546058e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 45/71 | LOSS: 1.4893423390869588e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 46/71 | LOSS: 1.489661404930496e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 47/71 | LOSS: 1.4863166256873228e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 48/71 | LOSS: 1.4830436732748295e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 49/71 | LOSS: 1.4802851619606372e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 50/71 | LOSS: 1.4797912626818288e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 51/71 | LOSS: 1.4798964929678523e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 52/71 | LOSS: 1.4742655984487359e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 53/71 | LOSS: 1.4663129604741698e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 54/71 | LOSS: 1.4723441282430113e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 55/71 | LOSS: 1.474697207218664e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 56/71 | LOSS: 1.4730605966051052e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 57/71 | LOSS: 1.4734282386002477e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 58/71 | LOSS: 1.4699193882934504e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 59/71 | LOSS: 1.4725553000971559e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 60/71 | LOSS: 1.4688002607488402e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 61/71 | LOSS: 1.4617250628596653e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 62/71 | LOSS: 1.4562667346913331e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 63/71 | LOSS: 1.4499516325372497e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 64/71 | LOSS: 1.4433195288942302e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 65/71 | LOSS: 1.4397276696730511e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 66/71 | LOSS: 1.4345585278438669e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 67/71 | LOSS: 1.4311349171775338e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 68/71 | LOSS: 1.4281710415768126e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 69/71 | LOSS: 1.4237977250429269e-05\n",
      "TRAIN: EPOCH 84/100 | BATCH 70/71 | LOSS: 1.4226205538262278e-05\n",
      "VAL: EPOCH 84/100 | BATCH 0/8 | LOSS: 1.5271196389221586e-05\n",
      "VAL: EPOCH 84/100 | BATCH 1/8 | LOSS: 1.256346695299726e-05\n",
      "VAL: EPOCH 84/100 | BATCH 2/8 | LOSS: 1.2641625896018619e-05\n",
      "VAL: EPOCH 84/100 | BATCH 3/8 | LOSS: 1.374640214635292e-05\n",
      "VAL: EPOCH 84/100 | BATCH 4/8 | LOSS: 1.3447703531710431e-05\n",
      "VAL: EPOCH 84/100 | BATCH 5/8 | LOSS: 1.3029548123692317e-05\n",
      "VAL: EPOCH 84/100 | BATCH 6/8 | LOSS: 1.3273548315087932e-05\n",
      "VAL: EPOCH 84/100 | BATCH 7/8 | LOSS: 1.282422249460069e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 0/71 | LOSS: 1.2336300642346032e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 1/71 | LOSS: 1.1423300747992471e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 2/71 | LOSS: 1.1166675297621017e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 3/71 | LOSS: 1.1838467571578803e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 4/71 | LOSS: 1.2617120046343188e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 5/71 | LOSS: 1.2602162617743792e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 6/71 | LOSS: 1.2649697444950497e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 7/71 | LOSS: 1.2674374033849745e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 8/71 | LOSS: 1.2464332737888779e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 9/71 | LOSS: 1.2689028426393634e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 10/71 | LOSS: 1.3022877944670405e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 11/71 | LOSS: 1.3384353148164033e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 12/71 | LOSS: 1.3438297803356993e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 13/71 | LOSS: 1.3384006641509976e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 14/71 | LOSS: 1.3429743739834521e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 15/71 | LOSS: 1.3252837504751369e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 16/71 | LOSS: 1.3328978639037814e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 17/71 | LOSS: 1.3557512476028125e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 18/71 | LOSS: 1.3452266868721621e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 19/71 | LOSS: 1.3441121564028435e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 20/71 | LOSS: 1.3377871644999167e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 21/71 | LOSS: 1.3448842351367189e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 22/71 | LOSS: 1.3502597025565237e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 23/71 | LOSS: 1.3466413292917423e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 24/71 | LOSS: 1.3505221213563345e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 25/71 | LOSS: 1.3394333459123467e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 26/71 | LOSS: 1.3575145675093626e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 27/71 | LOSS: 1.3530216879839177e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 28/71 | LOSS: 1.3516422894036654e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 29/71 | LOSS: 1.3532253312102209e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 30/71 | LOSS: 1.3550688538910641e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 31/71 | LOSS: 1.3753617167822085e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 32/71 | LOSS: 1.3638865835453158e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 33/71 | LOSS: 1.362256144228227e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 34/71 | LOSS: 1.3632435937844484e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 35/71 | LOSS: 1.355228245200932e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 36/71 | LOSS: 1.354827720599526e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 37/71 | LOSS: 1.3475238550822005e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 38/71 | LOSS: 1.3513497647065191e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 39/71 | LOSS: 1.3443987745631602e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 40/71 | LOSS: 1.3416459762261294e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 41/71 | LOSS: 1.3391392193235724e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 42/71 | LOSS: 1.3533372830986696e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 43/71 | LOSS: 1.3500333145864054e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 44/71 | LOSS: 1.3534567187889479e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 45/71 | LOSS: 1.3494217672933187e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 46/71 | LOSS: 1.3410699292317657e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 47/71 | LOSS: 1.3348960597644085e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 48/71 | LOSS: 1.3337952753188023e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 49/71 | LOSS: 1.333974536464666e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 50/71 | LOSS: 1.331302535297308e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 51/71 | LOSS: 1.3278148274436648e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 52/71 | LOSS: 1.3276476444311897e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 53/71 | LOSS: 1.3250397842057282e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 54/71 | LOSS: 1.3207864239203362e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 55/71 | LOSS: 1.3191740549700626e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 56/71 | LOSS: 1.3137330473941882e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 57/71 | LOSS: 1.3135882407619522e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 58/71 | LOSS: 1.308241211271537e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 59/71 | LOSS: 1.3128466495497074e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 60/71 | LOSS: 1.3085109100354545e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 61/71 | LOSS: 1.3052163159309497e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 62/71 | LOSS: 1.3087591397047545e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 63/71 | LOSS: 1.3055172217946165e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 64/71 | LOSS: 1.3076136551367549e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 65/71 | LOSS: 1.3031043002256686e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 66/71 | LOSS: 1.3030274950229362e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 67/71 | LOSS: 1.3037192448952256e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 68/71 | LOSS: 1.306293591145389e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 69/71 | LOSS: 1.3087880701537609e-05\n",
      "TRAIN: EPOCH 85/100 | BATCH 70/71 | LOSS: 1.3086490173624094e-05\n",
      "VAL: EPOCH 85/100 | BATCH 0/8 | LOSS: 1.577024522703141e-05\n",
      "VAL: EPOCH 85/100 | BATCH 1/8 | LOSS: 1.3597673387266695e-05\n",
      "VAL: EPOCH 85/100 | BATCH 2/8 | LOSS: 1.4464124736453718e-05\n",
      "VAL: EPOCH 85/100 | BATCH 3/8 | LOSS: 1.574953148519853e-05\n",
      "VAL: EPOCH 85/100 | BATCH 4/8 | LOSS: 1.5519975931965746e-05\n",
      "VAL: EPOCH 85/100 | BATCH 5/8 | LOSS: 1.5170323422353249e-05\n",
      "VAL: EPOCH 85/100 | BATCH 6/8 | LOSS: 1.546353217106246e-05\n",
      "VAL: EPOCH 85/100 | BATCH 7/8 | LOSS: 1.4910878462615074e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 0/71 | LOSS: 1.4085027942201123e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 1/71 | LOSS: 1.4270012798078824e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 2/71 | LOSS: 1.4227189240045846e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 3/71 | LOSS: 1.4773583188798511e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 4/71 | LOSS: 1.5033153249532916e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 5/71 | LOSS: 1.433530906069791e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 6/71 | LOSS: 1.3791531403382709e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 7/71 | LOSS: 1.3924704376222508e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 8/71 | LOSS: 1.4020295869462037e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 9/71 | LOSS: 1.3823921653965953e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 10/71 | LOSS: 1.391313030167525e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 11/71 | LOSS: 1.3898283062493041e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 12/71 | LOSS: 1.3700119500124576e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 13/71 | LOSS: 1.3545695180385206e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 14/71 | LOSS: 1.3696315909328404e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 15/71 | LOSS: 1.3798626639527356e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 16/71 | LOSS: 1.3905009997388869e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 17/71 | LOSS: 1.3793397000780613e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 18/71 | LOSS: 1.3667485051739373e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 19/71 | LOSS: 1.3619526771435631e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 20/71 | LOSS: 1.3814094104635019e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 21/71 | LOSS: 1.372013945332252e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 22/71 | LOSS: 1.3696264155125549e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 23/71 | LOSS: 1.3735746392740111e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 24/71 | LOSS: 1.3820476779073942e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 25/71 | LOSS: 1.3947400370503041e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 26/71 | LOSS: 1.4041264101386466e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 27/71 | LOSS: 1.3969083186436495e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 28/71 | LOSS: 1.3882927336892643e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 29/71 | LOSS: 1.3809918194359246e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 30/71 | LOSS: 1.3752908730126482e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 31/71 | LOSS: 1.3797112472957451e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 32/71 | LOSS: 1.3731622967690893e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 33/71 | LOSS: 1.368486346109421e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 34/71 | LOSS: 1.364905648577925e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 35/71 | LOSS: 1.3540255597237976e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 36/71 | LOSS: 1.3451405130900958e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 37/71 | LOSS: 1.3470714915647955e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 38/71 | LOSS: 1.3451754383799524e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 39/71 | LOSS: 1.3525308941098046e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 40/71 | LOSS: 1.347075056223619e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 41/71 | LOSS: 1.349676728880565e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 42/71 | LOSS: 1.3491414540831735e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 43/71 | LOSS: 1.3455786405426228e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 44/71 | LOSS: 1.3388970486024239e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 45/71 | LOSS: 1.331213535076645e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 46/71 | LOSS: 1.3272478138381477e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 47/71 | LOSS: 1.3282274911337785e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 48/71 | LOSS: 1.3252269275895111e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 49/71 | LOSS: 1.3218587537267012e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 50/71 | LOSS: 1.320622034066165e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 51/71 | LOSS: 1.3219341623810647e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 52/71 | LOSS: 1.314479408701684e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 53/71 | LOSS: 1.3208629466303305e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 54/71 | LOSS: 1.321186419690176e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 55/71 | LOSS: 1.3182397310629312e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 56/71 | LOSS: 1.3150029876111116e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 57/71 | LOSS: 1.3125701508246932e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 58/71 | LOSS: 1.3128407186679582e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 59/71 | LOSS: 1.3125272016623057e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 60/71 | LOSS: 1.3076554557196911e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 61/71 | LOSS: 1.3010011864553821e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 62/71 | LOSS: 1.2986855056494409e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 63/71 | LOSS: 1.2970068638651355e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 64/71 | LOSS: 1.3007512545580259e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 65/71 | LOSS: 1.3016728630508155e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 66/71 | LOSS: 1.303718136792912e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 67/71 | LOSS: 1.2986922590297421e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 68/71 | LOSS: 1.2967065429066956e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 69/71 | LOSS: 1.3024699183006305e-05\n",
      "TRAIN: EPOCH 86/100 | BATCH 70/71 | LOSS: 1.29818898412722e-05\n",
      "VAL: EPOCH 86/100 | BATCH 0/8 | LOSS: 1.6361680536647327e-05\n",
      "VAL: EPOCH 86/100 | BATCH 1/8 | LOSS: 1.3219595984992338e-05\n",
      "VAL: EPOCH 86/100 | BATCH 2/8 | LOSS: 1.3272975290116543e-05\n",
      "VAL: EPOCH 86/100 | BATCH 3/8 | LOSS: 1.4302895124274073e-05\n",
      "VAL: EPOCH 86/100 | BATCH 4/8 | LOSS: 1.4060192006581928e-05\n",
      "VAL: EPOCH 86/100 | BATCH 5/8 | LOSS: 1.3493430287780939e-05\n",
      "VAL: EPOCH 86/100 | BATCH 6/8 | LOSS: 1.3745172249660495e-05\n",
      "VAL: EPOCH 86/100 | BATCH 7/8 | LOSS: 1.332685121724353e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 0/71 | LOSS: 1.1161519068991765e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 1/71 | LOSS: 1.1082798664574511e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 2/71 | LOSS: 1.2262237457131656e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 3/71 | LOSS: 1.2624088640222908e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 4/71 | LOSS: 1.234987357747741e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 5/71 | LOSS: 1.2347536843056636e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 6/71 | LOSS: 1.3498234693543054e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 7/71 | LOSS: 1.314564451604383e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 8/71 | LOSS: 1.3192682066195024e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 9/71 | LOSS: 1.3361364472075365e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 10/71 | LOSS: 1.3740444046561606e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 11/71 | LOSS: 1.3582605106421397e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 12/71 | LOSS: 1.3586322263738391e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 13/71 | LOSS: 1.3382546122946743e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 14/71 | LOSS: 1.3433450112643186e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 15/71 | LOSS: 1.3465678364354972e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 16/71 | LOSS: 1.3473240548645503e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 17/71 | LOSS: 1.3339772572685939e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 18/71 | LOSS: 1.3518241997333366e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 19/71 | LOSS: 1.3350432254810585e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 20/71 | LOSS: 1.3346428680788016e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 21/71 | LOSS: 1.3415495232038666e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 22/71 | LOSS: 1.326809749867686e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 23/71 | LOSS: 1.3266975732525074e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 24/71 | LOSS: 1.3387187718763017e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 25/71 | LOSS: 1.3383956229517935e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 26/71 | LOSS: 1.3324844749823333e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 27/71 | LOSS: 1.3421195002852723e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 28/71 | LOSS: 1.3520772129062418e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 29/71 | LOSS: 1.3497252590847589e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 30/71 | LOSS: 1.3610888315944917e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 31/71 | LOSS: 1.3579852151224259e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 32/71 | LOSS: 1.3510498898871116e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 33/71 | LOSS: 1.3434189280387192e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 34/71 | LOSS: 1.3356336005797078e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 35/71 | LOSS: 1.32569599221218e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 36/71 | LOSS: 1.3285411433890977e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 37/71 | LOSS: 1.3273156510924883e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 38/71 | LOSS: 1.3221916467442405e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 39/71 | LOSS: 1.3186739920456603e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 40/71 | LOSS: 1.3151995362266728e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 41/71 | LOSS: 1.3128079379904583e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 42/71 | LOSS: 1.30599701574389e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 43/71 | LOSS: 1.2990035532890364e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 44/71 | LOSS: 1.3003190280061163e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 45/71 | LOSS: 1.296262537479102e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 46/71 | LOSS: 1.2905921644136122e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 47/71 | LOSS: 1.2878498713083294e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 48/71 | LOSS: 1.291331979273866e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 49/71 | LOSS: 1.2856505818490405e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 50/71 | LOSS: 1.2781470960523139e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 51/71 | LOSS: 1.2788773451925059e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 52/71 | LOSS: 1.2824222773455198e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 53/71 | LOSS: 1.287116822789324e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 54/71 | LOSS: 1.2878254892711993e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 55/71 | LOSS: 1.288472073416155e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 56/71 | LOSS: 1.2896462402223782e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 57/71 | LOSS: 1.2924590150917005e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 58/71 | LOSS: 1.2930516184774283e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 59/71 | LOSS: 1.2988798259054117e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 60/71 | LOSS: 1.2941958928303259e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 61/71 | LOSS: 1.2940374202322169e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 62/71 | LOSS: 1.3004108300914855e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 63/71 | LOSS: 1.3018674337672564e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 64/71 | LOSS: 1.3031368619592215e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 65/71 | LOSS: 1.3025666551887602e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 66/71 | LOSS: 1.3028220496719891e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 67/71 | LOSS: 1.303441972060468e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 68/71 | LOSS: 1.3033467663605563e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 69/71 | LOSS: 1.2992892009476366e-05\n",
      "TRAIN: EPOCH 87/100 | BATCH 70/71 | LOSS: 1.3113438207012582e-05\n",
      "VAL: EPOCH 87/100 | BATCH 0/8 | LOSS: 1.6031404811656103e-05\n",
      "VAL: EPOCH 87/100 | BATCH 1/8 | LOSS: 1.3000684703001752e-05\n",
      "VAL: EPOCH 87/100 | BATCH 2/8 | LOSS: 1.352122429428467e-05\n",
      "VAL: EPOCH 87/100 | BATCH 3/8 | LOSS: 1.4803825706621865e-05\n",
      "VAL: EPOCH 87/100 | BATCH 4/8 | LOSS: 1.438789076928515e-05\n",
      "VAL: EPOCH 87/100 | BATCH 5/8 | LOSS: 1.3941912887579141e-05\n",
      "VAL: EPOCH 87/100 | BATCH 6/8 | LOSS: 1.4381894808528678e-05\n",
      "VAL: EPOCH 87/100 | BATCH 7/8 | LOSS: 1.398121628426452e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 0/71 | LOSS: 1.547753345221281e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 1/71 | LOSS: 1.3155537544662366e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 2/71 | LOSS: 1.383075777994236e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 3/71 | LOSS: 1.2795814200217137e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 4/71 | LOSS: 1.346408316749148e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 5/71 | LOSS: 1.3494019943512589e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 6/71 | LOSS: 1.3378239080858683e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 7/71 | LOSS: 1.342414782357082e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 8/71 | LOSS: 1.3645117986824414e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 9/71 | LOSS: 1.3517838488041889e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 10/71 | LOSS: 1.3710439154899426e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 11/71 | LOSS: 1.3531709403954059e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 12/71 | LOSS: 1.3512691461633389e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 13/71 | LOSS: 1.3556206340581411e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 14/71 | LOSS: 1.3457953900797293e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 15/71 | LOSS: 1.35077998493216e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 16/71 | LOSS: 1.3575637400636505e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 17/71 | LOSS: 1.3534976965780112e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 18/71 | LOSS: 1.3437901782852255e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 19/71 | LOSS: 1.3291795858094701e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 20/71 | LOSS: 1.3511188669889678e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 21/71 | LOSS: 1.3535812534180216e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 22/71 | LOSS: 1.364559410949764e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 23/71 | LOSS: 1.3684631047302295e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 24/71 | LOSS: 1.3674565234396141e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 25/71 | LOSS: 1.3520738775696373e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 26/71 | LOSS: 1.3581570413036928e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 27/71 | LOSS: 1.3584860295980303e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 28/71 | LOSS: 1.3480580711420143e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 29/71 | LOSS: 1.3452691503819854e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 30/71 | LOSS: 1.3374140918688593e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 31/71 | LOSS: 1.3333738479559543e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 32/71 | LOSS: 1.323734724075731e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 33/71 | LOSS: 1.3236416955752407e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 34/71 | LOSS: 1.3243802667212939e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 35/71 | LOSS: 1.3199960373337186e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 36/71 | LOSS: 1.3181101461963703e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 37/71 | LOSS: 1.3153612192246855e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 38/71 | LOSS: 1.3115639866065556e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 39/71 | LOSS: 1.304425520629593e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 40/71 | LOSS: 1.3064439150273357e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 41/71 | LOSS: 1.3008132750006293e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 42/71 | LOSS: 1.3080613191346476e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 43/71 | LOSS: 1.3095433401262694e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 44/71 | LOSS: 1.31030062321871e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 45/71 | LOSS: 1.3086193299070304e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 46/71 | LOSS: 1.3024758931002926e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 47/71 | LOSS: 1.2958643443046943e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 48/71 | LOSS: 1.293761976498621e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 49/71 | LOSS: 1.2928411888424306e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 50/71 | LOSS: 1.2905927606505643e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 51/71 | LOSS: 1.289135282324703e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 52/71 | LOSS: 1.289920175691324e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 53/71 | LOSS: 1.2920349692851434e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 54/71 | LOSS: 1.2893819885820531e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 55/71 | LOSS: 1.2852189066896763e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 56/71 | LOSS: 1.285765450263984e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 57/71 | LOSS: 1.2844033194439896e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 58/71 | LOSS: 1.2828398582114152e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 59/71 | LOSS: 1.282935207503518e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 60/71 | LOSS: 1.2807772184371925e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 61/71 | LOSS: 1.275973656975698e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 62/71 | LOSS: 1.276062200289859e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 63/71 | LOSS: 1.2804483233708197e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 64/71 | LOSS: 1.2777348078648524e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 65/71 | LOSS: 1.2746874827743424e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 66/71 | LOSS: 1.2797753521199547e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 67/71 | LOSS: 1.2757915716493325e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 68/71 | LOSS: 1.2742472777524423e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 69/71 | LOSS: 1.2710585828504658e-05\n",
      "TRAIN: EPOCH 88/100 | BATCH 70/71 | LOSS: 1.271118485355671e-05\n",
      "VAL: EPOCH 88/100 | BATCH 0/8 | LOSS: 1.7077998563763686e-05\n",
      "VAL: EPOCH 88/100 | BATCH 1/8 | LOSS: 1.4334059414977673e-05\n",
      "VAL: EPOCH 88/100 | BATCH 2/8 | LOSS: 1.3982391465106048e-05\n",
      "VAL: EPOCH 88/100 | BATCH 3/8 | LOSS: 1.4556935184373287e-05\n",
      "VAL: EPOCH 88/100 | BATCH 4/8 | LOSS: 1.4192310663929674e-05\n",
      "VAL: EPOCH 88/100 | BATCH 5/8 | LOSS: 1.3634457748897452e-05\n",
      "VAL: EPOCH 88/100 | BATCH 6/8 | LOSS: 1.3699807173647319e-05\n",
      "VAL: EPOCH 88/100 | BATCH 7/8 | LOSS: 1.3316879631020129e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 0/71 | LOSS: 1.2943839465151541e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 1/71 | LOSS: 1.2277727819309803e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 2/71 | LOSS: 1.2740806899576759e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 3/71 | LOSS: 1.1580817954381928e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 4/71 | LOSS: 1.1925480066565797e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 5/71 | LOSS: 1.2092303525908695e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 6/71 | LOSS: 1.2603572031366639e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 7/71 | LOSS: 1.2395257385833247e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 8/71 | LOSS: 1.2597015787226459e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 9/71 | LOSS: 1.2581794180732686e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 10/71 | LOSS: 1.2713341675407719e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 11/71 | LOSS: 1.2426761410703572e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 12/71 | LOSS: 1.2277555470063817e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 13/71 | LOSS: 1.2119770547412503e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 14/71 | LOSS: 1.209955019779348e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 15/71 | LOSS: 1.2114353694414604e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 16/71 | LOSS: 1.2252750393064857e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 17/71 | LOSS: 1.2364913749883675e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 18/71 | LOSS: 1.2450881896304628e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 19/71 | LOSS: 1.2528508796094684e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 20/71 | LOSS: 1.2417763519763303e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 21/71 | LOSS: 1.2374496608655052e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 22/71 | LOSS: 1.2535147106973454e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 23/71 | LOSS: 1.239593321618789e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 24/71 | LOSS: 1.235376807017019e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 25/71 | LOSS: 1.2335163669027119e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 26/71 | LOSS: 1.2447811126616284e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 27/71 | LOSS: 1.2483868139107862e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 28/71 | LOSS: 1.2432159659009154e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 29/71 | LOSS: 1.2440512576480007e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 30/71 | LOSS: 1.2350073149953519e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 31/71 | LOSS: 1.2372645358027512e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 32/71 | LOSS: 1.2342959080618158e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 33/71 | LOSS: 1.2453459395842133e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 34/71 | LOSS: 1.2446228668393036e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 35/71 | LOSS: 1.2418505169383328e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 36/71 | LOSS: 1.247472099183486e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 37/71 | LOSS: 1.2492534719249785e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 38/71 | LOSS: 1.2523748549798396e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 39/71 | LOSS: 1.2482259717216947e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 40/71 | LOSS: 1.2504372778760337e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 41/71 | LOSS: 1.2531674309727914e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 42/71 | LOSS: 1.2488561815400825e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 43/71 | LOSS: 1.2450188313034067e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 44/71 | LOSS: 1.2448282945519572e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 45/71 | LOSS: 1.248728057876376e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 46/71 | LOSS: 1.2491468236901788e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 47/71 | LOSS: 1.2530061970513392e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 48/71 | LOSS: 1.2513837182083005e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 49/71 | LOSS: 1.250656569027342e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 50/71 | LOSS: 1.2539793697571582e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 51/71 | LOSS: 1.2540931545747579e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 52/71 | LOSS: 1.2540620360287337e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 53/71 | LOSS: 1.2496457556052417e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 54/71 | LOSS: 1.2482996333512182e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 55/71 | LOSS: 1.2535213451363753e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 56/71 | LOSS: 1.2524329417430094e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 57/71 | LOSS: 1.2546520322538182e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 58/71 | LOSS: 1.2554902919657899e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 59/71 | LOSS: 1.2591194035849185e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 60/71 | LOSS: 1.2597015677888297e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 61/71 | LOSS: 1.261770296790224e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 62/71 | LOSS: 1.2570278707402943e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 63/71 | LOSS: 1.2546669793778165e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 64/71 | LOSS: 1.2574691250987459e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 65/71 | LOSS: 1.254824440944834e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 66/71 | LOSS: 1.25345998219303e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 67/71 | LOSS: 1.2518954931425361e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 68/71 | LOSS: 1.2473724645910872e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 69/71 | LOSS: 1.2450256118816989e-05\n",
      "TRAIN: EPOCH 89/100 | BATCH 70/71 | LOSS: 1.2483289192203583e-05\n",
      "VAL: EPOCH 89/100 | BATCH 0/8 | LOSS: 1.6687448805896565e-05\n",
      "VAL: EPOCH 89/100 | BATCH 1/8 | LOSS: 1.394206128679798e-05\n",
      "VAL: EPOCH 89/100 | BATCH 2/8 | LOSS: 1.3758131899521686e-05\n",
      "VAL: EPOCH 89/100 | BATCH 3/8 | LOSS: 1.4575680324924178e-05\n",
      "VAL: EPOCH 89/100 | BATCH 4/8 | LOSS: 1.4346060743264388e-05\n",
      "VAL: EPOCH 89/100 | BATCH 5/8 | LOSS: 1.3749414847552544e-05\n",
      "VAL: EPOCH 89/100 | BATCH 6/8 | LOSS: 1.386012432444009e-05\n",
      "VAL: EPOCH 89/100 | BATCH 7/8 | LOSS: 1.3402763784142735e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 0/71 | LOSS: 1.5128112863749266e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 1/71 | LOSS: 1.4387808278115699e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 2/71 | LOSS: 1.2925278194112858e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 3/71 | LOSS: 1.3184876706873183e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 4/71 | LOSS: 1.266339259018423e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 5/71 | LOSS: 1.220497400330108e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 6/71 | LOSS: 1.2689144958650494e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 7/71 | LOSS: 1.2457119851205789e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 8/71 | LOSS: 1.214836983207432e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 9/71 | LOSS: 1.2253422482899623e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 10/71 | LOSS: 1.2708653312901417e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 11/71 | LOSS: 1.2655512440081415e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 12/71 | LOSS: 1.264309314417635e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 13/71 | LOSS: 1.272075282291293e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 14/71 | LOSS: 1.2751146884208235e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 15/71 | LOSS: 1.2657465447318828e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 16/71 | LOSS: 1.2655960103181902e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 17/71 | LOSS: 1.281384720641654e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 18/71 | LOSS: 1.2753093343283245e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 19/71 | LOSS: 1.274137712243828e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 20/71 | LOSS: 1.271248094694567e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 21/71 | LOSS: 1.2685615729424171e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 22/71 | LOSS: 1.270742213028311e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 23/71 | LOSS: 1.2622593923576156e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 24/71 | LOSS: 1.2531835709523875e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 25/71 | LOSS: 1.2498789251717077e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 26/71 | LOSS: 1.2464419274625402e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 27/71 | LOSS: 1.2424118202553863e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 28/71 | LOSS: 1.2358507743117335e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 29/71 | LOSS: 1.2342954596533673e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 30/71 | LOSS: 1.2387444551296216e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 31/71 | LOSS: 1.237331909464956e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 32/71 | LOSS: 1.2344100275985317e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 33/71 | LOSS: 1.2301070642827066e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 34/71 | LOSS: 1.2395965807497434e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 35/71 | LOSS: 1.2351925736058749e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 36/71 | LOSS: 1.2406334585771342e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 37/71 | LOSS: 1.2364100065927855e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 38/71 | LOSS: 1.2354492052205993e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 39/71 | LOSS: 1.235628421909496e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 40/71 | LOSS: 1.233921348768224e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 41/71 | LOSS: 1.2410517467201099e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 42/71 | LOSS: 1.2410331459419509e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 43/71 | LOSS: 1.2374845399873182e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 44/71 | LOSS: 1.2405013856348685e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 45/71 | LOSS: 1.2460549125693353e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 46/71 | LOSS: 1.245149248093862e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 47/71 | LOSS: 1.2512195818696151e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 48/71 | LOSS: 1.2568195341707312e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 49/71 | LOSS: 1.2586439461301779e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 50/71 | LOSS: 1.2665009521964856e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 51/71 | LOSS: 1.2725338067936649e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 52/71 | LOSS: 1.2785064530551196e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 53/71 | LOSS: 1.2791863624712547e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 54/71 | LOSS: 1.2774511411738455e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 55/71 | LOSS: 1.2722563659346114e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 56/71 | LOSS: 1.2724338694967804e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 57/71 | LOSS: 1.2775963020023232e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 58/71 | LOSS: 1.276393089999871e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 59/71 | LOSS: 1.276388711630716e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 60/71 | LOSS: 1.2777390364761708e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 61/71 | LOSS: 1.27432623494608e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 62/71 | LOSS: 1.2753222252949998e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 63/71 | LOSS: 1.2742641672502941e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 64/71 | LOSS: 1.2753862588746975e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 65/71 | LOSS: 1.275200126353108e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 66/71 | LOSS: 1.2751057918431975e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 67/71 | LOSS: 1.2723227080320666e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 68/71 | LOSS: 1.2695453272275357e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 69/71 | LOSS: 1.2655564868120044e-05\n",
      "TRAIN: EPOCH 90/100 | BATCH 70/71 | LOSS: 1.2660633280613414e-05\n",
      "VAL: EPOCH 90/100 | BATCH 0/8 | LOSS: 1.4378778359969147e-05\n",
      "VAL: EPOCH 90/100 | BATCH 1/8 | LOSS: 1.1572278708626982e-05\n",
      "VAL: EPOCH 90/100 | BATCH 2/8 | LOSS: 1.2142807463533245e-05\n",
      "VAL: EPOCH 90/100 | BATCH 3/8 | LOSS: 1.304341003560694e-05\n",
      "VAL: EPOCH 90/100 | BATCH 4/8 | LOSS: 1.2646369577851147e-05\n",
      "VAL: EPOCH 90/100 | BATCH 5/8 | LOSS: 1.2068976957380073e-05\n",
      "VAL: EPOCH 90/100 | BATCH 6/8 | LOSS: 1.2407049455630062e-05\n",
      "VAL: EPOCH 90/100 | BATCH 7/8 | LOSS: 1.2063060353284527e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 0/71 | LOSS: 1.0010307050833944e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 1/71 | LOSS: 1.0223532626696397e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 2/71 | LOSS: 1.1577702328698555e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 3/71 | LOSS: 1.2510156693679164e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 4/71 | LOSS: 1.2696753583441022e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 5/71 | LOSS: 1.3331524466290526e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 6/71 | LOSS: 1.438265709501658e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 7/71 | LOSS: 1.454525988719979e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 8/71 | LOSS: 1.475713027806099e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 9/71 | LOSS: 1.4714238841406768e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 10/71 | LOSS: 1.4463005754805636e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 11/71 | LOSS: 1.4640241791615457e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 12/71 | LOSS: 1.4609362691631899e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 13/71 | LOSS: 1.4517288036586251e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 14/71 | LOSS: 1.4386076933684915e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 15/71 | LOSS: 1.4303263299098035e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 16/71 | LOSS: 1.4078161685200244e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 17/71 | LOSS: 1.4139629709259478e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 18/71 | LOSS: 1.4083321012118463e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 19/71 | LOSS: 1.40453647418326e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 20/71 | LOSS: 1.4071010679929583e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 21/71 | LOSS: 1.4114506732518997e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 22/71 | LOSS: 1.4048139667505419e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 23/71 | LOSS: 1.4032708691047446e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 24/71 | LOSS: 1.4152071344142314e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 25/71 | LOSS: 1.4218471768309917e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 26/71 | LOSS: 1.4159093586419692e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 27/71 | LOSS: 1.4105375839790213e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 28/71 | LOSS: 1.399746376811326e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 29/71 | LOSS: 1.4016572671001389e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 30/71 | LOSS: 1.3962753274104362e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 31/71 | LOSS: 1.3791093152804024e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 32/71 | LOSS: 1.3730808115558995e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 33/71 | LOSS: 1.3696773240714238e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 34/71 | LOSS: 1.3574804662701873e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 35/71 | LOSS: 1.346549290954297e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 36/71 | LOSS: 1.337692874831437e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 37/71 | LOSS: 1.3315551457612615e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 38/71 | LOSS: 1.3312878483982506e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 39/71 | LOSS: 1.3308109623721976e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 40/71 | LOSS: 1.3184384287586266e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 41/71 | LOSS: 1.314842897738258e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 42/71 | LOSS: 1.3119663509674273e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 43/71 | LOSS: 1.3169845735980992e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 44/71 | LOSS: 1.316078089277855e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 45/71 | LOSS: 1.309566548867303e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 46/71 | LOSS: 1.3055617331546989e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 47/71 | LOSS: 1.3078769219040018e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 48/71 | LOSS: 1.3077973265780553e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 49/71 | LOSS: 1.3102636621624697e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 50/71 | LOSS: 1.3032363330792435e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 51/71 | LOSS: 1.3048319068240324e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 52/71 | LOSS: 1.3062801852631207e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 53/71 | LOSS: 1.3032294326101826e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 54/71 | LOSS: 1.2993168440351093e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 55/71 | LOSS: 1.2995511886921512e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 56/71 | LOSS: 1.2973644675583815e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 57/71 | LOSS: 1.2947001182353767e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 58/71 | LOSS: 1.2919988906922577e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 59/71 | LOSS: 1.299039307317192e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 60/71 | LOSS: 1.2936851131509355e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 61/71 | LOSS: 1.2941955989639786e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 62/71 | LOSS: 1.292850599080945e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 63/71 | LOSS: 1.2904582007422505e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 64/71 | LOSS: 1.2891427062501093e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 65/71 | LOSS: 1.2856084650235116e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 66/71 | LOSS: 1.2816684734864237e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 67/71 | LOSS: 1.2877127801402821e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 68/71 | LOSS: 1.2847307661486674e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 69/71 | LOSS: 1.2853962367184328e-05\n",
      "TRAIN: EPOCH 91/100 | BATCH 70/71 | LOSS: 1.2851860511020764e-05\n",
      "VAL: EPOCH 91/100 | BATCH 0/8 | LOSS: 1.48981052916497e-05\n",
      "VAL: EPOCH 91/100 | BATCH 1/8 | LOSS: 1.2684346984315198e-05\n",
      "VAL: EPOCH 91/100 | BATCH 2/8 | LOSS: 1.3223447543471897e-05\n",
      "VAL: EPOCH 91/100 | BATCH 3/8 | LOSS: 1.4050989420866244e-05\n",
      "VAL: EPOCH 91/100 | BATCH 4/8 | LOSS: 1.3768224926025141e-05\n",
      "VAL: EPOCH 91/100 | BATCH 5/8 | LOSS: 1.3307404666799508e-05\n",
      "VAL: EPOCH 91/100 | BATCH 6/8 | LOSS: 1.3338727902529562e-05\n",
      "VAL: EPOCH 91/100 | BATCH 7/8 | LOSS: 1.2919533787680848e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 0/71 | LOSS: 1.0211575499852188e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 1/71 | LOSS: 1.0281632512487704e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 2/71 | LOSS: 1.063189029082423e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 3/71 | LOSS: 1.0966278523483197e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 4/71 | LOSS: 1.1354576054145582e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 5/71 | LOSS: 1.1208943760721013e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 6/71 | LOSS: 1.1541605610026246e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 7/71 | LOSS: 1.1807050213974435e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 8/71 | LOSS: 1.1770532687983683e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 9/71 | LOSS: 1.2225933642184827e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 10/71 | LOSS: 1.2331964667695997e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 11/71 | LOSS: 1.2354042003911067e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 12/71 | LOSS: 1.2034867722832132e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 13/71 | LOSS: 1.1978365169592767e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 14/71 | LOSS: 1.2145060160643577e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 15/71 | LOSS: 1.219927980855573e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 16/71 | LOSS: 1.2071965530220255e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 17/71 | LOSS: 1.205524621481244e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 18/71 | LOSS: 1.1928917047043797e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 19/71 | LOSS: 1.2207271220177063e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 20/71 | LOSS: 1.224018329334545e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 21/71 | LOSS: 1.2178119215439603e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 22/71 | LOSS: 1.2138658981395723e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 23/71 | LOSS: 1.2161598685149025e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 24/71 | LOSS: 1.2150243528594728e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 25/71 | LOSS: 1.2158624905658564e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 26/71 | LOSS: 1.2078461521822545e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 27/71 | LOSS: 1.2018982813190502e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 28/71 | LOSS: 1.2027904850928562e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 29/71 | LOSS: 1.2090234577044612e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 30/71 | LOSS: 1.2014695189023886e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 31/71 | LOSS: 1.2038794238833361e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 32/71 | LOSS: 1.21048880662949e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 33/71 | LOSS: 1.2134587905165972e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 34/71 | LOSS: 1.2073378924729435e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 35/71 | LOSS: 1.2064078621026258e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 36/71 | LOSS: 1.2101679031557805e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 37/71 | LOSS: 1.2076041409607952e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 38/71 | LOSS: 1.2027092009539512e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 39/71 | LOSS: 1.1958771392528433e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 40/71 | LOSS: 1.1928470529510821e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 41/71 | LOSS: 1.190234187171362e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 42/71 | LOSS: 1.1892841030203272e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 43/71 | LOSS: 1.1932944827011935e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 44/71 | LOSS: 1.1951507460455308e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 45/71 | LOSS: 1.2015956757671159e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 46/71 | LOSS: 1.2004945482600461e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 47/71 | LOSS: 1.2032385408626093e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 48/71 | LOSS: 1.2053258522391101e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 49/71 | LOSS: 1.2053413574903971e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 50/71 | LOSS: 1.202383737108903e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 51/71 | LOSS: 1.205848976636704e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 52/71 | LOSS: 1.2069403947982278e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 53/71 | LOSS: 1.2073034415354616e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 54/71 | LOSS: 1.2063922374429901e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 55/71 | LOSS: 1.2014242705455607e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 56/71 | LOSS: 1.1980538163283072e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 57/71 | LOSS: 1.1975291899630221e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 58/71 | LOSS: 1.1983827604681364e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 59/71 | LOSS: 1.1951571771836218e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 60/71 | LOSS: 1.1949606098331985e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 61/71 | LOSS: 1.194975200104959e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 62/71 | LOSS: 1.1953915100815403e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 63/71 | LOSS: 1.1947870476092248e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 64/71 | LOSS: 1.1982304945726915e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 65/71 | LOSS: 1.1970689491025757e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 66/71 | LOSS: 1.1947806125312154e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 67/71 | LOSS: 1.198701594309087e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 68/71 | LOSS: 1.2048408266771045e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 69/71 | LOSS: 1.2016462356509043e-05\n",
      "TRAIN: EPOCH 92/100 | BATCH 70/71 | LOSS: 1.2017925424951384e-05\n",
      "VAL: EPOCH 92/100 | BATCH 0/8 | LOSS: 1.4953791833249852e-05\n",
      "VAL: EPOCH 92/100 | BATCH 1/8 | LOSS: 1.1890720998053439e-05\n",
      "VAL: EPOCH 92/100 | BATCH 2/8 | LOSS: 1.2149466783739626e-05\n",
      "VAL: EPOCH 92/100 | BATCH 3/8 | LOSS: 1.3268821021483745e-05\n",
      "VAL: EPOCH 92/100 | BATCH 4/8 | LOSS: 1.3096733346174005e-05\n",
      "VAL: EPOCH 92/100 | BATCH 5/8 | LOSS: 1.2491436943188697e-05\n",
      "VAL: EPOCH 92/100 | BATCH 6/8 | LOSS: 1.275026404203215e-05\n",
      "VAL: EPOCH 92/100 | BATCH 7/8 | LOSS: 1.2360681353129621e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 0/71 | LOSS: 1.2328084267210215e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 1/71 | LOSS: 1.2444194908312056e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 2/71 | LOSS: 1.2132035408285446e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 3/71 | LOSS: 1.1516227914398769e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 4/71 | LOSS: 1.1248456212342717e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 5/71 | LOSS: 1.1433778430121796e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 6/71 | LOSS: 1.1615728518726038e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 7/71 | LOSS: 1.1326633284625132e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 8/71 | LOSS: 1.146742761193309e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 9/71 | LOSS: 1.14043350549764e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 10/71 | LOSS: 1.1578313288654582e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 11/71 | LOSS: 1.1462039992693462e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 12/71 | LOSS: 1.1578017410311777e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 13/71 | LOSS: 1.1562909224137132e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 14/71 | LOSS: 1.1701703382035097e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 15/71 | LOSS: 1.1965469866481726e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 16/71 | LOSS: 1.1933943984759561e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 17/71 | LOSS: 1.1883126565711185e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 18/71 | LOSS: 1.1786953731835207e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 19/71 | LOSS: 1.181093907689501e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 20/71 | LOSS: 1.1746585620303882e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 21/71 | LOSS: 1.1688465912646445e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 22/71 | LOSS: 1.1549216414214639e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 23/71 | LOSS: 1.150638634802211e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 24/71 | LOSS: 1.1556257159099913e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 25/71 | LOSS: 1.1572967405999616e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 26/71 | LOSS: 1.1624380325277647e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 27/71 | LOSS: 1.162790480131142e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 28/71 | LOSS: 1.1752375500910947e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 29/71 | LOSS: 1.1664891098916997e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 30/71 | LOSS: 1.1655908301397557e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 31/71 | LOSS: 1.1793989472153044e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 32/71 | LOSS: 1.188739544445663e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 33/71 | LOSS: 1.1939537377175465e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 34/71 | LOSS: 1.201391873369825e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 35/71 | LOSS: 1.2145241195563964e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 36/71 | LOSS: 1.2099943592762134e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 37/71 | LOSS: 1.2175045364864146e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 38/71 | LOSS: 1.220939618808468e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 39/71 | LOSS: 1.225489211265085e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 40/71 | LOSS: 1.2351012029706376e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 41/71 | LOSS: 1.2341229843774012e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 42/71 | LOSS: 1.2266974091437796e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 43/71 | LOSS: 1.2334582503502713e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 44/71 | LOSS: 1.2468801873587331e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 45/71 | LOSS: 1.255218053895236e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 46/71 | LOSS: 1.2609658098997588e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 47/71 | LOSS: 1.2701248692792433e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 48/71 | LOSS: 1.2716984944132974e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 49/71 | LOSS: 1.2727139946946408e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 50/71 | LOSS: 1.2703571526656684e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 51/71 | LOSS: 1.2749691117838105e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 52/71 | LOSS: 1.2802677427419628e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 53/71 | LOSS: 1.2875845152814657e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 54/71 | LOSS: 1.2911025822342542e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 55/71 | LOSS: 1.293745218130685e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 56/71 | LOSS: 1.3016512240395522e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 57/71 | LOSS: 1.3110655188562494e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 58/71 | LOSS: 1.3103883078255714e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 59/71 | LOSS: 1.3138002471653938e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 60/71 | LOSS: 1.3208090780101541e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 61/71 | LOSS: 1.3191950294581588e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 62/71 | LOSS: 1.3165007845918087e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 63/71 | LOSS: 1.3168060348789368e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 64/71 | LOSS: 1.3266198864305177e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 65/71 | LOSS: 1.3294811320850965e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 66/71 | LOSS: 1.3297267395867479e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 67/71 | LOSS: 1.3296395081852097e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 68/71 | LOSS: 1.3331887664431976e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 69/71 | LOSS: 1.3415816595495146e-05\n",
      "TRAIN: EPOCH 93/100 | BATCH 70/71 | LOSS: 1.3422007828973018e-05\n",
      "VAL: EPOCH 93/100 | BATCH 0/8 | LOSS: 1.8335302229388617e-05\n",
      "VAL: EPOCH 93/100 | BATCH 1/8 | LOSS: 1.6371714991691988e-05\n",
      "VAL: EPOCH 93/100 | BATCH 2/8 | LOSS: 1.6789497143084493e-05\n",
      "VAL: EPOCH 93/100 | BATCH 3/8 | LOSS: 1.7990803371503716e-05\n",
      "VAL: EPOCH 93/100 | BATCH 4/8 | LOSS: 1.778195764927659e-05\n",
      "VAL: EPOCH 93/100 | BATCH 5/8 | LOSS: 1.7407283242694877e-05\n",
      "VAL: EPOCH 93/100 | BATCH 6/8 | LOSS: 1.727261120062654e-05\n",
      "VAL: EPOCH 93/100 | BATCH 7/8 | LOSS: 1.6768908380981884e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 0/71 | LOSS: 1.4662080502603203e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 1/71 | LOSS: 1.3816752471029758e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 2/71 | LOSS: 1.4190583897288889e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 3/71 | LOSS: 1.4988706425356213e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 4/71 | LOSS: 1.556624993099831e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 5/71 | LOSS: 1.4958826795918867e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 6/71 | LOSS: 1.480303879881311e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 7/71 | LOSS: 1.498746212291735e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 8/71 | LOSS: 1.453924485556652e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 9/71 | LOSS: 1.4150285915093264e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 10/71 | LOSS: 1.4629642877976453e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 11/71 | LOSS: 1.4634017513041423e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 12/71 | LOSS: 1.4351995559991337e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 13/71 | LOSS: 1.4398112983014602e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 14/71 | LOSS: 1.418862466380233e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 15/71 | LOSS: 1.4086583121297735e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 16/71 | LOSS: 1.3900510620099821e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 17/71 | LOSS: 1.3713001610287594e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 18/71 | LOSS: 1.3683261481913322e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 19/71 | LOSS: 1.3734664526054985e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 20/71 | LOSS: 1.3652003699029419e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 21/71 | LOSS: 1.3474280752813105e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 22/71 | LOSS: 1.3562655310480572e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 23/71 | LOSS: 1.3409305590054524e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 24/71 | LOSS: 1.3416368092293852e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 25/71 | LOSS: 1.342769040163078e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 26/71 | LOSS: 1.3270955369753884e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 27/71 | LOSS: 1.320560740428586e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 28/71 | LOSS: 1.3124524606421895e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 29/71 | LOSS: 1.3069627160196736e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 30/71 | LOSS: 1.3054100940456508e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 31/71 | LOSS: 1.307916440396184e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 32/71 | LOSS: 1.2984502697663354e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 33/71 | LOSS: 1.2936911469510318e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 34/71 | LOSS: 1.2914940348959395e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 35/71 | LOSS: 1.2893612620246131e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 36/71 | LOSS: 1.287621671508532e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 37/71 | LOSS: 1.2755712305079214e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 38/71 | LOSS: 1.2770108393548677e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 39/71 | LOSS: 1.2740210922856932e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 40/71 | LOSS: 1.2752397751461192e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 41/71 | LOSS: 1.2850662382593977e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 42/71 | LOSS: 1.287170646740611e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 43/71 | LOSS: 1.2850005863252921e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 44/71 | LOSS: 1.2768742959047408e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 45/71 | LOSS: 1.2797552679567952e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 46/71 | LOSS: 1.2733958720537944e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 47/71 | LOSS: 1.2704651965123048e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 48/71 | LOSS: 1.2768949295141097e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 49/71 | LOSS: 1.2697084184765117e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 50/71 | LOSS: 1.2665364563729905e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 51/71 | LOSS: 1.267143849103237e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 52/71 | LOSS: 1.266139567959263e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 53/71 | LOSS: 1.265250197051231e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 54/71 | LOSS: 1.2626048258177682e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 55/71 | LOSS: 1.2650456369556196e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 56/71 | LOSS: 1.2602671998600973e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 57/71 | LOSS: 1.2642969350367665e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 58/71 | LOSS: 1.2638050466810425e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 59/71 | LOSS: 1.2642132454251016e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 60/71 | LOSS: 1.2640425889823417e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 61/71 | LOSS: 1.261630469314461e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 62/71 | LOSS: 1.2567790907355847e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 63/71 | LOSS: 1.2525636819304964e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 64/71 | LOSS: 1.2506350363904718e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 65/71 | LOSS: 1.2445900390020777e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 66/71 | LOSS: 1.2395760405972884e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 67/71 | LOSS: 1.2397216430733405e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 68/71 | LOSS: 1.2409421801946692e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 69/71 | LOSS: 1.23676480143331e-05\n",
      "TRAIN: EPOCH 94/100 | BATCH 70/71 | LOSS: 1.2356426427878675e-05\n",
      "VAL: EPOCH 94/100 | BATCH 0/8 | LOSS: 1.4088187526795082e-05\n",
      "VAL: EPOCH 94/100 | BATCH 1/8 | LOSS: 1.1738372450054158e-05\n",
      "VAL: EPOCH 94/100 | BATCH 2/8 | LOSS: 1.1688214423581181e-05\n",
      "VAL: EPOCH 94/100 | BATCH 3/8 | LOSS: 1.255752658835263e-05\n",
      "VAL: EPOCH 94/100 | BATCH 4/8 | LOSS: 1.2250452346052044e-05\n",
      "VAL: EPOCH 94/100 | BATCH 5/8 | LOSS: 1.1696470513318976e-05\n",
      "VAL: EPOCH 94/100 | BATCH 6/8 | LOSS: 1.176434553989176e-05\n",
      "VAL: EPOCH 94/100 | BATCH 7/8 | LOSS: 1.137796448347217e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 0/71 | LOSS: 9.86680606729351e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 1/71 | LOSS: 1.0177597687288653e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 2/71 | LOSS: 1.055558732332429e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 3/71 | LOSS: 9.756229019330931e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 4/71 | LOSS: 9.550867980578915e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 5/71 | LOSS: 9.171068465244995e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 6/71 | LOSS: 9.414607445380949e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 7/71 | LOSS: 9.684350800398533e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 8/71 | LOSS: 9.776068559909214e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 9/71 | LOSS: 1.0045090266430635e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 10/71 | LOSS: 9.977401408501795e-06\n",
      "TRAIN: EPOCH 95/100 | BATCH 11/71 | LOSS: 1.0270718841335716e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 12/71 | LOSS: 1.0391979965508934e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 13/71 | LOSS: 1.0515472532850773e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 14/71 | LOSS: 1.0610838489810704e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 15/71 | LOSS: 1.0834179107632735e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 16/71 | LOSS: 1.0869128105054295e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 17/71 | LOSS: 1.0991845227989768e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 18/71 | LOSS: 1.100953271529043e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 19/71 | LOSS: 1.1201119627912704e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 20/71 | LOSS: 1.117575220632716e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 21/71 | LOSS: 1.1238642876130392e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 22/71 | LOSS: 1.1150684278381169e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 23/71 | LOSS: 1.1105796128655735e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 24/71 | LOSS: 1.1180511046404718e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 25/71 | LOSS: 1.1197140845699603e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 26/71 | LOSS: 1.117242540944803e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 27/71 | LOSS: 1.111412223053776e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 28/71 | LOSS: 1.1077327442386566e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 29/71 | LOSS: 1.1327493772720724e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 30/71 | LOSS: 1.1306872091799666e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 31/71 | LOSS: 1.130800619364436e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 32/71 | LOSS: 1.1296935374985244e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 33/71 | LOSS: 1.1328389425269801e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 34/71 | LOSS: 1.1298888694000198e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 35/71 | LOSS: 1.1328994850777639e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 36/71 | LOSS: 1.130260501225633e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 37/71 | LOSS: 1.1312331659945285e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 38/71 | LOSS: 1.1369501326277177e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 39/71 | LOSS: 1.141933540793616e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 40/71 | LOSS: 1.147485934824345e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 41/71 | LOSS: 1.1601758604315581e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 42/71 | LOSS: 1.1620554867180925e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 43/71 | LOSS: 1.1587253941963189e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 44/71 | LOSS: 1.1580524884872527e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 45/71 | LOSS: 1.1638317682413996e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 46/71 | LOSS: 1.1655667481620107e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 47/71 | LOSS: 1.1697677763322645e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 48/71 | LOSS: 1.1668943137251796e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 49/71 | LOSS: 1.1688617969412007e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 50/71 | LOSS: 1.1721700689924868e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 51/71 | LOSS: 1.1706573900482908e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 52/71 | LOSS: 1.1661162587634468e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 53/71 | LOSS: 1.162061708041788e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 54/71 | LOSS: 1.1614018322606254e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 55/71 | LOSS: 1.1630240879055367e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 56/71 | LOSS: 1.1655284979177962e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 57/71 | LOSS: 1.163189960669993e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 58/71 | LOSS: 1.1575726888221945e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 59/71 | LOSS: 1.1591420108440312e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 60/71 | LOSS: 1.1574161071919542e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 61/71 | LOSS: 1.1531649343235205e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 62/71 | LOSS: 1.153776861789834e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 63/71 | LOSS: 1.1523000814861462e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 64/71 | LOSS: 1.151514953352699e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 65/71 | LOSS: 1.1550764838830219e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 66/71 | LOSS: 1.1533521005934636e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 67/71 | LOSS: 1.1550431662833622e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 68/71 | LOSS: 1.156110860445525e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 69/71 | LOSS: 1.1532741473274654e-05\n",
      "TRAIN: EPOCH 95/100 | BATCH 70/71 | LOSS: 1.1537853840988173e-05\n",
      "VAL: EPOCH 95/100 | BATCH 0/8 | LOSS: 1.4423798347706906e-05\n",
      "VAL: EPOCH 95/100 | BATCH 1/8 | LOSS: 1.2035018698952626e-05\n",
      "VAL: EPOCH 95/100 | BATCH 2/8 | LOSS: 1.2123611971522527e-05\n",
      "VAL: EPOCH 95/100 | BATCH 3/8 | LOSS: 1.2879151654487941e-05\n",
      "VAL: EPOCH 95/100 | BATCH 4/8 | LOSS: 1.2534982124634552e-05\n",
      "VAL: EPOCH 95/100 | BATCH 5/8 | LOSS: 1.1854490063948711e-05\n",
      "VAL: EPOCH 95/100 | BATCH 6/8 | LOSS: 1.1993916164751031e-05\n",
      "VAL: EPOCH 95/100 | BATCH 7/8 | LOSS: 1.162058856607473e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 0/71 | LOSS: 9.779907486517914e-06\n",
      "TRAIN: EPOCH 96/100 | BATCH 1/71 | LOSS: 1.2877610970463138e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 2/71 | LOSS: 1.2446000861624876e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 3/71 | LOSS: 1.2008974863420008e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 4/71 | LOSS: 1.1547422036528588e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 5/71 | LOSS: 1.1771598262081776e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 6/71 | LOSS: 1.1586773195761322e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 7/71 | LOSS: 1.138307300152519e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 8/71 | LOSS: 1.1667927133708468e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 9/71 | LOSS: 1.1618839016591664e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 10/71 | LOSS: 1.1690270184772089e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 11/71 | LOSS: 1.188378064398421e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 12/71 | LOSS: 1.1711475893277497e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 13/71 | LOSS: 1.1569347212311446e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 14/71 | LOSS: 1.161998025054345e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 15/71 | LOSS: 1.1585986328555009e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 16/71 | LOSS: 1.167844809114125e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 17/71 | LOSS: 1.1701482789147929e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 18/71 | LOSS: 1.151992704475752e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 19/71 | LOSS: 1.1445634254414471e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 20/71 | LOSS: 1.1439401359460871e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 21/71 | LOSS: 1.1245960753182192e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 22/71 | LOSS: 1.119188577238516e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 23/71 | LOSS: 1.1248161494374168e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 24/71 | LOSS: 1.1299550551484572e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 25/71 | LOSS: 1.1238451459296182e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 26/71 | LOSS: 1.1172282551409876e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 27/71 | LOSS: 1.1285736377431022e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 28/71 | LOSS: 1.1315552518649853e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 29/71 | LOSS: 1.1262695276551918e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 30/71 | LOSS: 1.1277444627921795e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 31/71 | LOSS: 1.133844644130022e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 32/71 | LOSS: 1.1279687138691466e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 33/71 | LOSS: 1.1271366086957106e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 34/71 | LOSS: 1.1254594560991142e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 35/71 | LOSS: 1.1318303108964756e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 36/71 | LOSS: 1.1304954187909135e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 37/71 | LOSS: 1.1332561742069389e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 38/71 | LOSS: 1.134339463066844e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 39/71 | LOSS: 1.1329162805395754e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 40/71 | LOSS: 1.1315390632417558e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 41/71 | LOSS: 1.1303248194443377e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 42/71 | LOSS: 1.1378096249160968e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 43/71 | LOSS: 1.1430821742082612e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 44/71 | LOSS: 1.1433526308084968e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 45/71 | LOSS: 1.1436905137800394e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 46/71 | LOSS: 1.1450531106442832e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 47/71 | LOSS: 1.1405820686150037e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 48/71 | LOSS: 1.1456831302615571e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 49/71 | LOSS: 1.1514977168189943e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 50/71 | LOSS: 1.150985562002033e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 51/71 | LOSS: 1.1500145293351894e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 52/71 | LOSS: 1.1501988161776439e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 53/71 | LOSS: 1.1494066096996971e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 54/71 | LOSS: 1.1469688607493563e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 55/71 | LOSS: 1.1482874299417745e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 56/71 | LOSS: 1.150907949347145e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 57/71 | LOSS: 1.1515826427371238e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 58/71 | LOSS: 1.1519692102642e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 59/71 | LOSS: 1.1546292012099001e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 60/71 | LOSS: 1.1536082139009225e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 61/71 | LOSS: 1.154578849456166e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 62/71 | LOSS: 1.1529924182786278e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 63/71 | LOSS: 1.1508407901317241e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 64/71 | LOSS: 1.1543217673394024e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 65/71 | LOSS: 1.1567529871613122e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 66/71 | LOSS: 1.1553926782001856e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 67/71 | LOSS: 1.1541738016978129e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 68/71 | LOSS: 1.151399756976837e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 69/71 | LOSS: 1.1502126907154369e-05\n",
      "TRAIN: EPOCH 96/100 | BATCH 70/71 | LOSS: 1.1555910423596006e-05\n",
      "VAL: EPOCH 96/100 | BATCH 0/8 | LOSS: 1.4708757589687593e-05\n",
      "VAL: EPOCH 96/100 | BATCH 1/8 | LOSS: 1.2013850664516212e-05\n",
      "VAL: EPOCH 96/100 | BATCH 2/8 | LOSS: 1.1773629315333286e-05\n",
      "VAL: EPOCH 96/100 | BATCH 3/8 | LOSS: 1.2615947980521014e-05\n",
      "VAL: EPOCH 96/100 | BATCH 4/8 | LOSS: 1.2405784946167842e-05\n",
      "VAL: EPOCH 96/100 | BATCH 5/8 | LOSS: 1.1851956969621824e-05\n",
      "VAL: EPOCH 96/100 | BATCH 6/8 | LOSS: 1.1922754440872399e-05\n",
      "VAL: EPOCH 96/100 | BATCH 7/8 | LOSS: 1.149246190834674e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 0/71 | LOSS: 1.1203364010725636e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 1/71 | LOSS: 1.2068408977938816e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 2/71 | LOSS: 1.1733091923815664e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 3/71 | LOSS: 1.159025055130769e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 4/71 | LOSS: 1.1284154061286244e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 5/71 | LOSS: 1.135787366971878e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 6/71 | LOSS: 1.2305206187842746e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 7/71 | LOSS: 1.2274510595489119e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 8/71 | LOSS: 1.213134318176243e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 9/71 | LOSS: 1.202651983476244e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 10/71 | LOSS: 1.2077033798877066e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 11/71 | LOSS: 1.2036778192244432e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 12/71 | LOSS: 1.2004186338834608e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 13/71 | LOSS: 1.1889786297355645e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 14/71 | LOSS: 1.2097919473793202e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 15/71 | LOSS: 1.1977974565979821e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 16/71 | LOSS: 1.186342428006944e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 17/71 | LOSS: 1.2006904802951289e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 18/71 | LOSS: 1.2178225772099962e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 19/71 | LOSS: 1.216409855260281e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 20/71 | LOSS: 1.222997681387434e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 21/71 | LOSS: 1.22040096357523e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 22/71 | LOSS: 1.2268995147757739e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 23/71 | LOSS: 1.211887073774657e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 24/71 | LOSS: 1.2024722127534915e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 25/71 | LOSS: 1.2142844997679977e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 26/71 | LOSS: 1.217642248876574e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 27/71 | LOSS: 1.2168306869040992e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 28/71 | LOSS: 1.2169477987958214e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 29/71 | LOSS: 1.2146330088095661e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 30/71 | LOSS: 1.2128528602085182e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 31/71 | LOSS: 1.2030025885678697e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 32/71 | LOSS: 1.1919899379515272e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 33/71 | LOSS: 1.1826803449152708e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 34/71 | LOSS: 1.1875522016323104e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 35/71 | LOSS: 1.1843703936796777e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 36/71 | LOSS: 1.1845780603392553e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 37/71 | LOSS: 1.1857599785894548e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 38/71 | LOSS: 1.1835488877318896e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 39/71 | LOSS: 1.190159321140527e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 40/71 | LOSS: 1.1887607975151651e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 41/71 | LOSS: 1.1886111007611145e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 42/71 | LOSS: 1.1887138498423155e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 43/71 | LOSS: 1.1847081395899295e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 44/71 | LOSS: 1.189075468977939e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 45/71 | LOSS: 1.179465075635918e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 46/71 | LOSS: 1.1811687340439753e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 47/71 | LOSS: 1.17920243193718e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 48/71 | LOSS: 1.1751982667043863e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 49/71 | LOSS: 1.1753864018828608e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 50/71 | LOSS: 1.1738989461959778e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 51/71 | LOSS: 1.1721293591248328e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 52/71 | LOSS: 1.1729708637845624e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 53/71 | LOSS: 1.1703551552744574e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 54/71 | LOSS: 1.1644405177238778e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 55/71 | LOSS: 1.1626157840640059e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 56/71 | LOSS: 1.1583985341483066e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 57/71 | LOSS: 1.159731008838521e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 58/71 | LOSS: 1.161376720551358e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 59/71 | LOSS: 1.1653306000880547e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 60/71 | LOSS: 1.161492121757531e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 61/71 | LOSS: 1.1624549202796545e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 62/71 | LOSS: 1.1596667314329858e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 63/71 | LOSS: 1.1613338969596043e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 64/71 | LOSS: 1.1615883802453307e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 65/71 | LOSS: 1.1630362664480785e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 66/71 | LOSS: 1.161407501275613e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 67/71 | LOSS: 1.1573212492187262e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 68/71 | LOSS: 1.1525775704100944e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 69/71 | LOSS: 1.1519219567292436e-05\n",
      "TRAIN: EPOCH 97/100 | BATCH 70/71 | LOSS: 1.1549815701047766e-05\n",
      "VAL: EPOCH 97/100 | BATCH 0/8 | LOSS: 1.5035030628496315e-05\n",
      "VAL: EPOCH 97/100 | BATCH 1/8 | LOSS: 1.3372598004934844e-05\n",
      "VAL: EPOCH 97/100 | BATCH 2/8 | LOSS: 1.4255008257653875e-05\n",
      "VAL: EPOCH 97/100 | BATCH 3/8 | LOSS: 1.5130156043596799e-05\n",
      "VAL: EPOCH 97/100 | BATCH 4/8 | LOSS: 1.483574160374701e-05\n",
      "VAL: EPOCH 97/100 | BATCH 5/8 | LOSS: 1.4417253017503148e-05\n",
      "VAL: EPOCH 97/100 | BATCH 6/8 | LOSS: 1.4526205891992764e-05\n",
      "VAL: EPOCH 97/100 | BATCH 7/8 | LOSS: 1.4154482755657227e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 0/71 | LOSS: 1.6729514754842967e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 1/71 | LOSS: 1.4389562238648068e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 2/71 | LOSS: 1.2977175477620525e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 3/71 | LOSS: 1.4096592622081516e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 4/71 | LOSS: 1.4143698717816732e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 5/71 | LOSS: 1.385607356496621e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 6/71 | LOSS: 1.4568342001959017e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 7/71 | LOSS: 1.4076707657295628e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 8/71 | LOSS: 1.3813081144083602e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 9/71 | LOSS: 1.4021675087860786e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 10/71 | LOSS: 1.3859038683056662e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 11/71 | LOSS: 1.3741003992132997e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 12/71 | LOSS: 1.3988926254499416e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 13/71 | LOSS: 1.3851954528425786e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 14/71 | LOSS: 1.3658503121405374e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 15/71 | LOSS: 1.3850565494522016e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 16/71 | LOSS: 1.3890145322982468e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 17/71 | LOSS: 1.3825276406957224e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 18/71 | LOSS: 1.4029146625130364e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 19/71 | LOSS: 1.4335508831209154e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 20/71 | LOSS: 1.4366271855015795e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 21/71 | LOSS: 1.475529701234667e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 22/71 | LOSS: 1.4695639473084947e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 23/71 | LOSS: 1.4634876492891635e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 24/71 | LOSS: 1.4559644660039339e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 25/71 | LOSS: 1.4354421287302663e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 26/71 | LOSS: 1.4305688771990524e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 27/71 | LOSS: 1.4380013356328294e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 28/71 | LOSS: 1.4234290511953516e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 29/71 | LOSS: 1.4204905649724727e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 30/71 | LOSS: 1.4084443870571352e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 31/71 | LOSS: 1.4041653116692032e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 32/71 | LOSS: 1.4015936055026637e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 33/71 | LOSS: 1.4003747952194639e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 34/71 | LOSS: 1.3876763841835781e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 35/71 | LOSS: 1.3819440533148332e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 36/71 | LOSS: 1.379858386408651e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 37/71 | LOSS: 1.3650466132132447e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 38/71 | LOSS: 1.3714438164942503e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 39/71 | LOSS: 1.359612790565734e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 40/71 | LOSS: 1.3494840880789192e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 41/71 | LOSS: 1.3430800793555267e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 42/71 | LOSS: 1.3367819096750937e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 43/71 | LOSS: 1.3310362956789588e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 44/71 | LOSS: 1.3317669537274115e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 45/71 | LOSS: 1.3226370850569094e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 46/71 | LOSS: 1.3235434128913998e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 47/71 | LOSS: 1.3145697456214597e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 48/71 | LOSS: 1.3150292665047847e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 49/71 | LOSS: 1.3063771821180125e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 50/71 | LOSS: 1.299903347078761e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 51/71 | LOSS: 1.2989865563912854e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 52/71 | LOSS: 1.2967913961510243e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 53/71 | LOSS: 1.2929989342983052e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 54/71 | LOSS: 1.2850335736981255e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 55/71 | LOSS: 1.2838440592791553e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 56/71 | LOSS: 1.2746917251972359e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 57/71 | LOSS: 1.2702789796282465e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 58/71 | LOSS: 1.2634114297062466e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 59/71 | LOSS: 1.2626576108232257e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 60/71 | LOSS: 1.2615819376479208e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 61/71 | LOSS: 1.2582439731270443e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 62/71 | LOSS: 1.2586933201144723e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 63/71 | LOSS: 1.2550705093872239e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 64/71 | LOSS: 1.2502465814087862e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 65/71 | LOSS: 1.2523538220138466e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 66/71 | LOSS: 1.2518557801442186e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 67/71 | LOSS: 1.2463420997686198e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 68/71 | LOSS: 1.2426048129597895e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 69/71 | LOSS: 1.2371878840765152e-05\n",
      "TRAIN: EPOCH 98/100 | BATCH 70/71 | LOSS: 1.232616575859064e-05\n",
      "VAL: EPOCH 98/100 | BATCH 0/8 | LOSS: 1.1975771485595033e-05\n",
      "VAL: EPOCH 98/100 | BATCH 1/8 | LOSS: 9.98668701868155e-06\n",
      "VAL: EPOCH 98/100 | BATCH 2/8 | LOSS: 1.0581313290458638e-05\n",
      "VAL: EPOCH 98/100 | BATCH 3/8 | LOSS: 1.1307914519420592e-05\n",
      "VAL: EPOCH 98/100 | BATCH 4/8 | LOSS: 1.1008860019501298e-05\n",
      "VAL: EPOCH 98/100 | BATCH 5/8 | LOSS: 1.0546547703900918e-05\n",
      "VAL: EPOCH 98/100 | BATCH 6/8 | LOSS: 1.063144113036937e-05\n",
      "VAL: EPOCH 98/100 | BATCH 7/8 | LOSS: 1.0265814808008145e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 0/71 | LOSS: 1.0198414202022832e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 1/71 | LOSS: 9.631140528654214e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 2/71 | LOSS: 9.634300113248173e-06\n",
      "TRAIN: EPOCH 99/100 | BATCH 3/71 | LOSS: 1.0590348892947077e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 4/71 | LOSS: 1.0765901424747427e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 5/71 | LOSS: 1.0534737612033496e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 6/71 | LOSS: 1.0276587220557434e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 7/71 | LOSS: 1.0169393021897122e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 8/71 | LOSS: 1.0250358071870222e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 9/71 | LOSS: 1.055156581060146e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 10/71 | LOSS: 1.0485552427946294e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 11/71 | LOSS: 1.0315158230393232e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 12/71 | LOSS: 1.0371490693964566e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 13/71 | LOSS: 1.0405259737516254e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 14/71 | LOSS: 1.0247051250189543e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 15/71 | LOSS: 1.0521259810047923e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 16/71 | LOSS: 1.0550358317285667e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 17/71 | LOSS: 1.0499767439695359e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 18/71 | LOSS: 1.0480043113746337e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 19/71 | LOSS: 1.0522167167437147e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 20/71 | LOSS: 1.0558454916809707e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 21/71 | LOSS: 1.0603733368374048e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 22/71 | LOSS: 1.0604457162591405e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 23/71 | LOSS: 1.0524938981385882e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 24/71 | LOSS: 1.059292470017681e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 25/71 | LOSS: 1.0571626577215144e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 26/71 | LOSS: 1.0631753260289164e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 27/71 | LOSS: 1.0634604710243625e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 28/71 | LOSS: 1.0632259702482581e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 29/71 | LOSS: 1.0597536705366414e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 30/71 | LOSS: 1.0587830377429078e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 31/71 | LOSS: 1.0653397254145602e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 32/71 | LOSS: 1.0635145896010341e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 33/71 | LOSS: 1.0705321341554414e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 34/71 | LOSS: 1.075314243540301e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 35/71 | LOSS: 1.0766383790420756e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 36/71 | LOSS: 1.0739457384506995e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 37/71 | LOSS: 1.0705847958697154e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 38/71 | LOSS: 1.0708485579249473e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 39/71 | LOSS: 1.0707199908210897e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 40/71 | LOSS: 1.0686767426505401e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 41/71 | LOSS: 1.0711282660243645e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 42/71 | LOSS: 1.0673308235371139e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 43/71 | LOSS: 1.0693640226897763e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 44/71 | LOSS: 1.0693534447151857e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 45/71 | LOSS: 1.06600995739269e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 46/71 | LOSS: 1.0636629938628485e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 47/71 | LOSS: 1.0603029314400677e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 48/71 | LOSS: 1.0604926724134166e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 49/71 | LOSS: 1.0584031406324356e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 50/71 | LOSS: 1.0628787522899288e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 51/71 | LOSS: 1.0677350089211428e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 52/71 | LOSS: 1.0752754757770575e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 53/71 | LOSS: 1.0810277975300172e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 54/71 | LOSS: 1.0763648540226065e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 55/71 | LOSS: 1.0787491289322912e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 56/71 | LOSS: 1.0842831423424446e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 57/71 | LOSS: 1.083895988907396e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 58/71 | LOSS: 1.0901139846236358e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 59/71 | LOSS: 1.0908693654225015e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 60/71 | LOSS: 1.0877434507448448e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 61/71 | LOSS: 1.0848394257693379e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 62/71 | LOSS: 1.0929694312041632e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 63/71 | LOSS: 1.0991076948130285e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 64/71 | LOSS: 1.1088873949260093e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 65/71 | LOSS: 1.1096237633892363e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 66/71 | LOSS: 1.1117690166375085e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 67/71 | LOSS: 1.1244147759391448e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 68/71 | LOSS: 1.1248696113319095e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 69/71 | LOSS: 1.1243389612250862e-05\n",
      "TRAIN: EPOCH 99/100 | BATCH 70/71 | LOSS: 1.1285573044672123e-05\n",
      "VAL: EPOCH 99/100 | BATCH 0/8 | LOSS: 1.4493052731268108e-05\n",
      "VAL: EPOCH 99/100 | BATCH 1/8 | LOSS: 1.2806662653019885e-05\n",
      "VAL: EPOCH 99/100 | BATCH 2/8 | LOSS: 1.3533048938067319e-05\n",
      "VAL: EPOCH 99/100 | BATCH 3/8 | LOSS: 1.4131997886579484e-05\n",
      "VAL: EPOCH 99/100 | BATCH 4/8 | LOSS: 1.3793742073175964e-05\n",
      "VAL: EPOCH 99/100 | BATCH 5/8 | LOSS: 1.330515912438083e-05\n",
      "VAL: EPOCH 99/100 | BATCH 6/8 | LOSS: 1.3338415296207781e-05\n",
      "VAL: EPOCH 99/100 | BATCH 7/8 | LOSS: 1.3024900454183808e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 0/71 | LOSS: 1.2086429705959745e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 1/71 | LOSS: 1.2971507203474175e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 2/71 | LOSS: 1.4570140289530778e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 3/71 | LOSS: 1.460268003938836e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 4/71 | LOSS: 1.531642301415559e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 5/71 | LOSS: 1.6226763364102226e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 6/71 | LOSS: 1.5660783771766417e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 7/71 | LOSS: 1.5205180488919723e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 8/71 | LOSS: 1.530811702347516e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 9/71 | LOSS: 1.5584410721203313e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 10/71 | LOSS: 1.5444170044660993e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 11/71 | LOSS: 1.5795011222508037e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 12/71 | LOSS: 1.5719016188925776e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 13/71 | LOSS: 1.5656432229401878e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 14/71 | LOSS: 1.556523432858133e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 15/71 | LOSS: 1.5365905483122333e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 16/71 | LOSS: 1.5040391832264826e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 17/71 | LOSS: 1.4993793987135481e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 18/71 | LOSS: 1.4889747370417402e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 19/71 | LOSS: 1.4725384016855968e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 20/71 | LOSS: 1.4678580295627139e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 21/71 | LOSS: 1.4464598114665767e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 22/71 | LOSS: 1.4417456638867684e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 23/71 | LOSS: 1.4264281617215602e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 24/71 | LOSS: 1.4114905825408641e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 25/71 | LOSS: 1.401539716775565e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 26/71 | LOSS: 1.4003210068309542e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 27/71 | LOSS: 1.3846300557166355e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 28/71 | LOSS: 1.37104170162588e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 29/71 | LOSS: 1.3667839478633444e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 30/71 | LOSS: 1.3591688266681177e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 31/71 | LOSS: 1.3466026928199426e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 32/71 | LOSS: 1.3419470330215214e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 33/71 | LOSS: 1.3368800329558058e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 34/71 | LOSS: 1.3314946071269723e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 35/71 | LOSS: 1.3172110533964264e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 36/71 | LOSS: 1.3129223761269886e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 37/71 | LOSS: 1.3091954434984424e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 38/71 | LOSS: 1.3014616156280494e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 39/71 | LOSS: 1.290766249439912e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 40/71 | LOSS: 1.2806490880526637e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 41/71 | LOSS: 1.276081095403199e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 42/71 | LOSS: 1.2730715773846727e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 43/71 | LOSS: 1.2744654978550072e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 44/71 | LOSS: 1.2744851462937529e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 45/71 | LOSS: 1.273514758197606e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 46/71 | LOSS: 1.2678792293801497e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 47/71 | LOSS: 1.2645470330123013e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 48/71 | LOSS: 1.2597600289154798e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 49/71 | LOSS: 1.2537898692244199e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 50/71 | LOSS: 1.2528659413767267e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 51/71 | LOSS: 1.2505438217676083e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 52/71 | LOSS: 1.2497333286001725e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 53/71 | LOSS: 1.2416832827579196e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 54/71 | LOSS: 1.2450784065136263e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 55/71 | LOSS: 1.2408923859246507e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 56/71 | LOSS: 1.2392036105187512e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 57/71 | LOSS: 1.2367598487642305e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 58/71 | LOSS: 1.2303648842227353e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 59/71 | LOSS: 1.2360713253656285e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 60/71 | LOSS: 1.2359253172481196e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 61/71 | LOSS: 1.234566852578991e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 62/71 | LOSS: 1.2358025237012447e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 63/71 | LOSS: 1.2332920960034244e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 64/71 | LOSS: 1.2283473104337015e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 65/71 | LOSS: 1.2293764319276141e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 66/71 | LOSS: 1.225486702518972e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 67/71 | LOSS: 1.2234631693739657e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 68/71 | LOSS: 1.2228353889308957e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 69/71 | LOSS: 1.2216572434096763e-05\n",
      "TRAIN: EPOCH 100/100 | BATCH 70/71 | LOSS: 1.2266507005285387e-05\n",
      "VAL: EPOCH 100/100 | BATCH 0/8 | LOSS: 1.573454210301861e-05\n",
      "VAL: EPOCH 100/100 | BATCH 1/8 | LOSS: 1.3159686204744503e-05\n",
      "VAL: EPOCH 100/100 | BATCH 2/8 | LOSS: 1.2893585032240177e-05\n",
      "VAL: EPOCH 100/100 | BATCH 3/8 | LOSS: 1.3770923487754771e-05\n",
      "VAL: EPOCH 100/100 | BATCH 4/8 | LOSS: 1.3671649867319502e-05\n",
      "VAL: EPOCH 100/100 | BATCH 5/8 | LOSS: 1.2973197954124771e-05\n",
      "VAL: EPOCH 100/100 | BATCH 6/8 | LOSS: 1.3145521344475647e-05\n",
      "VAL: EPOCH 100/100 | BATCH 7/8 | LOSS: 1.2668744375332608e-05\n"
     ]
    }
   ],
   "source": [
    "trained_state = trainer(\n",
    "    state, train_loader, val_loader, l2_loss_fn, \n",
    "    num_epochs=100, exp_str=hp.as_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"model\", trained_state, hp, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the last checkpoint\n",
    "sdf_fn = get_mlp_by_path(\"./model\")\n",
    "sdf_fn(jnp.zeros(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
